"""
üß† –°–ø–µ—Ü–∏–∞–ª—å–Ω—ã–π Explainer –¥–ª—è XANFIS –º–æ–¥–µ–ª–µ–π
–ò—Å–ø–æ–ª—å–∑—É–µ—Ç –≤—Å—Ç—Ä–æ–µ–Ω–Ω—ã–µ –ø—Ä–∞–≤–∏–ª–∞ –≤–º–µ—Å—Ç–æ –∞–ø–ø—Ä–æ–∫—Å–∏–º–∞—Ü–∏–∏
"""

import numpy as np
import pandas as pd
from typing import Dict, List, Any, Optional


class XANFISExplainer:
    """Explainer –∫–æ—Ç–æ—Ä—ã–π –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –Ω–µ—á–µ—Ç–∫–∏–µ –ø—Ä–∞–≤–∏–ª–∞ XANFIS"""

    def __init__(self, model, feature_names=None):
        self.model = model
        self.feature_names = feature_names or [f"feature_{i}" for i in range(10)]

        # –ò–∑–≤–ª–µ–∫–∞–µ–º –ø—Ä–∞–≤–∏–ª–∞ –∏ –≤–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏
        self.rules = self._extract_rules()
        self.feature_importance = self._get_feature_importance()

    def _extract_rules(self):
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø—Ä–∞–≤–∏–ª –∏–∑ XANFIS –º–æ–¥–µ–ª–∏"""
        try:
            if hasattr(self.model, 'get_fuzzy_rules'):
                return self.model.get_fuzzy_rules()
            elif hasattr(self.model, 'model') and hasattr(self.model.model, 'get_fuzzy_rules'):
                return self.model.model.get_fuzzy_rules()
            else:
                print("‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å –ø—Ä–∞–≤–∏–ª–∞ XANFIS")
                return []
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –ø—Ä–∞–≤–∏–ª: {e}")
            return []

    def _get_feature_importance(self):
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤"""
        try:
            if hasattr(self.model, 'get_feature_importance'):
                return self.model.get_feature_importance()
            elif hasattr(self.model, 'model') and hasattr(self.model.model, 'get_feature_importance'):
                return self.model.model.get_feature_importance()
            else:
                # –°–æ–∑–¥–∞–µ–º –±–∞–∑–æ–≤—É—é –≤–∞–∂–Ω–æ—Å—Ç—å –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–∞–≤–∏–ª
                n_features = len(self.feature_names)
                importance = np.ones(n_features) / n_features

                # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –≤–∞–∂–Ω–æ—Å—Ç—å –¥–ª—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –≤ –ø—Ä–∞–≤–∏–ª–∞—Ö
                for rule in self.rules:
                    features_used = rule.get('features_used', [])
                    for feat_idx in features_used:
                        if feat_idx < n_features:
                            importance[feat_idx] *= 1.5

                # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
                return importance / np.sum(importance)

        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –≤–∞–∂–Ω–æ—Å—Ç–∏: {e}")
            return np.ones(len(self.feature_names)) / len(self.feature_names)

    def shap_values(self, X, **kwargs):
        """–ò–º–∏—Ç–∞—Ü–∏—è SHAP values –∏—Å–ø–æ–ª—å–∑—É—è –ø—Ä–∞–≤–∏–ª–∞ XANFIS"""

        try:
            n_samples, n_features = X.shape

            # –ë–∞–∑–æ–≤—ã–µ SHAP values –Ω–∞ –æ—Å–Ω–æ–≤–µ –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
            base_shap = np.zeros((n_samples, n_features))

            for i in range(n_samples):
                sample = X[i]

                # –î–ª—è –∫–∞–∂–¥–æ–≥–æ –ø—Ä–∏–∑–Ω–∞–∫–∞ –≤—ã—á–∏—Å–ª—è–µ–º –≤–∫–ª–∞–¥ –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–∞–≤–∏–ª
                for j in range(n_features):
                    feature_value = sample[j]
                    feature_importance = self.feature_importance[j]

                    # –ù–∞—Ö–æ–¥–∏–º –∞–∫—Ç–∏–≤–Ω—ã–µ –ø—Ä–∞–≤–∏–ª–∞ –¥–ª—è —ç—Ç–æ–≥–æ –æ–±—Ä–∞–∑—Ü–∞
                    active_rules = self._get_active_rules(sample)

                    # –í—ã—á–∏—Å–ª—è–µ–º SHAP value –∫–∞–∫ –∫–æ–º–±–∏–Ω–∞—Ü–∏—é –≤–∞–∂–Ω–æ—Å—Ç–∏ –∏ –∞–∫—Ç–∏–≤–∞—Ü–∏–∏ –ø—Ä–∞–≤–∏–ª
                    rule_contribution = 0.0
                    for rule in active_rules:
                        if j in rule.get('features_used', []):
                            rule_weight = rule.get('weight', 1.0)
                            rule_confidence = rule.get('confidence', 0.7)
                            rule_contribution += rule_weight * rule_confidence

                    # –ö–æ–º–±–∏–Ω–∏—Ä—É–µ–º –≤–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–∞ –∏ –≤–∫–ª–∞–¥ –ø—Ä–∞–≤–∏–ª
                    base_shap[i, j] = feature_importance * (1.0 + rule_contribution) * feature_value

            # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è SHAP values
            for i in range(n_samples):
                total_contribution = np.sum(np.abs(base_shap[i]))
                if total_contribution > 0:
                    base_shap[i] = base_shap[i] / total_contribution

            return base_shap

        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ —Ä–∞—Å—á–µ—Ç–∞ SHAP values: {e}")
            # Fallback - –ø—Ä–æ—Å—Ç—ã–µ SHAP values
            return np.random.uniform(-0.1, 0.1, (X.shape[0], X.shape[1]))

    def _get_active_rules(self, sample):
        """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∞–∫—Ç–∏–≤–Ω—ã—Ö –ø—Ä–∞–≤–∏–ª –¥–ª—è –æ–±—Ä–∞–∑—Ü–∞"""
        active_rules = []

        for rule in self.rules:
            # –ü—Ä–æ—Å—Ç–∞—è —ç–≤—Ä–∏—Å—Ç–∏–∫–∞ –∞–∫—Ç–∏–≤–∞—Ü–∏–∏ –ø—Ä–∞–≤–∏–ª–∞
            features_used = rule.get('features_used', [])

            if len(features_used) > 0:
                # –ü—Ä–∞–≤–∏–ª–æ –∞–∫—Ç–∏–≤–Ω–æ –µ—Å–ª–∏ –∑–Ω–∞—á–µ–Ω–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –≤ —Ä–∞–∑—É–º–Ω—ã—Ö –ø—Ä–µ–¥–µ–ª–∞—Ö
                rule_activation = True
                for feat_idx in features_used:
                    if feat_idx < len(sample):
                        # –ü—Ä–æ—Å—Ç–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –∞–∫—Ç–∏–≤–∞—Ü–∏–∏ (–º–æ–∂–Ω–æ —É—Å–ª–æ–∂–Ω–∏—Ç—å)
                        if abs(sample[feat_idx]) > 3.0:  # z-score > 3
                            rule_activation = False
                            break

                if rule_activation:
                    active_rules.append(rule)

        return active_rules if active_rules else self.rules[:1]  # –ú–∏–Ω–∏–º—É–º –æ–¥–Ω–æ –ø—Ä–∞–≤–∏–ª–æ

    def expected_value(self, X=None):
        """–ë–∞–∑–æ–≤–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –¥–ª—è SHAP"""
        return 0.0

    def get_explanation_quality(self):
        """–û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –æ–±—ä—è—Å–Ω–µ–Ω–∏—è"""

        if not self.rules:
            return {
                'rule_coverage': 0.0,
                'feature_coverage': 0.0,
                'rule_confidence': 0.0,
                'explanation_score': 0.1  # –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –±–∞–ª–ª
            }

        # –ü–æ–∫—Ä—ã—Ç–∏–µ –ø—Ä–∞–≤–∏–ª
        rule_coverage = min(1.0, len(self.rules) / 10.0)  # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –ø–æ 10 –ø—Ä–∞–≤–∏–ª–∞–º

        # –ü–æ–∫—Ä—ã—Ç–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        all_features_used = set()
        for rule in self.rules:
            all_features_used.update(rule.get('features_used', []))

        feature_coverage = len(all_features_used) / len(self.feature_names)

        # –°—Ä–µ–¥–Ω—è—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –ø—Ä–∞–≤–∏–ª
        confidences = [rule.get('confidence', 0.7) for rule in self.rules]
        rule_confidence = np.mean(confidences) if confidences else 0.7

        # –û–±—â–∏–π –±–∞–ª–ª –æ–±—ä—è—Å–Ω–∏–º–æ—Å—Ç–∏
        explanation_score = (
            0.4 * rule_coverage +      # 40% - –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∞–≤–∏–ª
            0.3 * feature_coverage +   # 30% - –ø–æ–∫—Ä—ã—Ç–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
            0.3 * rule_confidence      # 30% - —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –ø—Ä–∞–≤–∏–ª
        )

        return {
            'rule_coverage': float(rule_coverage),
            'feature_coverage': float(feature_coverage),
            'rule_confidence': float(rule_confidence),
            'explanation_score': float(explanation_score),
            'rules_count': len(self.rules),
            'features_used': len(all_features_used)
        }


def create_xanfis_explainer(model, feature_names=None):
    """–§–∞–±—Ä–∏—á–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è XANFIS explainer"""
    return XANFISExplainer(model, feature_names)
