"""
–ü–û–õ–ù–û–ï —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ ML –º–æ–¥–µ–ª–µ–π —Å Trust-ADE Protocol + CUDA + –≤—Å–µ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è
‚úÖ –ò—Å–ø—Ä–∞–≤–ª–µ–Ω—ã: XANFIS, –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è numpy, –Ω—É–ª–µ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏ Trust-ADE, CUDA –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è
üéØ –í–∫–ª—é—á–∞–µ—Ç: –ø–æ–ª–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –Ω–∞ –≤—Å–µ—Ö –¥–∞—Ç–∞—Å–µ—Ç–∞—Ö, —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ, –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—é
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import torch
import torch.nn as nn
from sklearn.datasets import load_iris, load_breast_cancer, load_wine, load_digits
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.svm import SVC
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, classification_report
import sys
import os
import time
import json
import warnings
from datetime import datetime
warnings.filterwarnings('ignore')

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ matplotlib –¥–ª—è –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è –æ—à–∏–±–æ–∫ —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
plt.style.use('default')
sns.set_palette("husl")
plt.rcParams['figure.dpi'] = 100
plt.rcParams['savefig.dpi'] = 300

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –∫ –ø—Ä–æ–µ–∫—Ç—É
current_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.dirname(current_dir)
if project_root not in sys.path:
    sys.path.insert(0, project_root)

from models.sklearn_wrapper import SklearnWrapper
from trust_ade.trust_ade import TrustADE

# CUDA –ø—Ä–æ–≤–µ—Ä–∫–∞ —Å –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–º –ø–æ—Ä–æ–≥–æ–º
CUDA_AVAILABLE = torch.cuda.is_available()
DEVICE = torch.device('cuda' if CUDA_AVAILABLE else 'cpu')
CUDA_EFFICIENT_THRESHOLD = 500  # –ò—Å–ø–æ–ª—å–∑—É–µ–º CUDA —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –¥–∞–Ω–Ω—ã—Ö –±–æ–ª—å—à–µ 500

if CUDA_AVAILABLE:
    print(f"‚úÖ CUDA –¥–æ—Å—Ç—É–ø–Ω–æ: {torch.cuda.get_device_name(0)}")
    print(f"   –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {DEVICE}")
    print(f"   –ü–∞–º—è—Ç—å GPU: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
    print(f"   ‚ö†Ô∏è CUDA –±—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è —Ç–æ–ª—å–∫–æ –¥–ª—è –¥–∞—Ç–∞—Å–µ—Ç–æ–≤ > {CUDA_EFFICIENT_THRESHOLD} –æ–±—Ä–∞–∑—Ü–æ–≤")
else:
    print("‚ö†Ô∏è CUDA –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–æ, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è CPU")

# –ò–º–ø–æ—Ä—Ç XANFIS —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫
try:
    from xanfis import Data, GdAnfisClassifier, AnfisClassifier
    XANFIS_AVAILABLE = True
    print("‚úÖ XANFIS —É—Å–ø–µ—à–Ω–æ –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω")
except ImportError:
    XANFIS_AVAILABLE = False
    print("‚ùå XANFIS –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install xanfis torch mealpy permetrics")


class OptimizedCUDAMLPClassifier:
    """–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è PyTorch MLP —Å –∞–¥–∞–ø—Ç–∏–≤–Ω—ã–º CUDA –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º"""

    def __init__(self, hidden_layers=(100, 50), n_classes=2, learning_rate=0.001,
                 epochs=300, device='cuda', random_state=42, dataset_size=0):
        self.hidden_layers = hidden_layers
        self.n_classes = n_classes
        self.learning_rate = learning_rate
        self.epochs = epochs
        self.random_state = random_state
        self.model = None
        self.scaler = StandardScaler()

        # –ê–¥–∞–ø—Ç–∏–≤–Ω—ã–π –≤—ã–±–æ—Ä —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–∞–∑–º–µ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö
        if dataset_size < CUDA_EFFICIENT_THRESHOLD:
            self.device = 'cpu'
            self.use_cuda = False
            print(f"      üì± –ò—Å–ø–æ–ª—å–∑—É–µ–º CPU (–¥–∞—Ç–∞—Å–µ—Ç –º–∞–ª: {dataset_size} < {CUDA_EFFICIENT_THRESHOLD})")
        else:
            self.device = device if torch.cuda.is_available() else 'cpu'
            self.use_cuda = self.device == 'cuda'
            print(f"      üöÄ –ò—Å–ø–æ–ª—å–∑—É–µ–º CUDA (–¥–∞—Ç–∞—Å–µ—Ç –±–æ–ª—å—à–æ–π: {dataset_size})")

        # –ê–¥–∞–ø—Ç–∏–≤–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ä–∞–∑–º–µ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö
        if dataset_size < 200:
            self.epochs = 150
            self.batch_size = min(16, dataset_size // 4)
        elif dataset_size < 500:
            self.epochs = 200
            self.batch_size = 32
        else:
            self.epochs = 300
            self.batch_size = 64

        # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º seed
        torch.manual_seed(random_state)
        if torch.cuda.is_available():
            torch.cuda.manual_seed(random_state)

    def _create_model(self, input_size):
        """–°–æ–∑–¥–∞–Ω–∏–µ –∞–¥–∞–ø—Ç–∏–≤–Ω–æ–π PyTorch –º–æ–¥–µ–ª–∏"""
        layers = []

        # –ê–¥–∞–ø—Ç–∏–≤–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞
        if input_size < 10:
            hidden_sizes = [max(8, input_size * 2), max(4, input_size)]
        elif input_size < 50:
            hidden_sizes = self.hidden_layers
        else:
            hidden_sizes = (min(512, input_size * 2), 256, 128)

        # –í—Ö–æ–¥–Ω–æ–π —Å–ª–æ–π
        layers.append(nn.Linear(input_size, hidden_sizes[0]))
        layers.append(nn.ReLU())
        layers.append(nn.Dropout(0.1))

        # –°–∫—Ä—ã—Ç—ã–µ —Å–ª–æ–∏
        for i in range(len(hidden_sizes) - 1):
            layers.append(nn.Linear(hidden_sizes[i], hidden_sizes[i + 1]))
            layers.append(nn.ReLU())
            layers.append(nn.Dropout(0.1))

        # –í—ã—Ö–æ–¥–Ω–æ–π —Å–ª–æ–π
        layers.append(nn.Linear(hidden_sizes[-1], self.n_classes))

        return nn.Sequential(*layers)

    def fit(self, X, y):
        """–û–±—É—á–µ–Ω–∏–µ —Å –∞–¥–∞–ø—Ç–∏–≤–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏"""
        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö
        X_scaled = self.scaler.fit_transform(X)

        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Å–æ–≤
        self.n_classes = len(np.unique(y))

        # –°–æ–∑–¥–∞–µ–º –º–æ–¥–µ–ª—å
        self.model = self._create_model(X_scaled.shape[1]).to(self.device)

        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ —Ç–µ–Ω–∑–æ—Ä—ã
        X_tensor = torch.FloatTensor(X_scaled).to(self.device)
        y_tensor = torch.LongTensor(y).to(self.device)

        # –ê–¥–∞–ø—Ç–∏–≤–Ω—ã–π –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä –∏ learning rate
        if X.shape[0] < 200:
            optimizer = torch.optim.Adam(self.model.parameters(), lr=self.learning_rate * 2)
        else:
            optimizer = torch.optim.Adam(self.model.parameters(), lr=self.learning_rate)

        criterion = nn.CrossEntropyLoss()

        # –û–±—É—á–µ–Ω–∏–µ —Å –∞–¥–∞–ø—Ç–∏–≤–Ω—ã–º –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ–º
        self.model.train()
        log_interval = max(25, self.epochs // 8)

        for epoch in range(self.epochs):
            optimizer.zero_grad()
            outputs = self.model(X_tensor)
            loss = criterion(outputs, y_tensor)
            loss.backward()
            optimizer.step()

            if epoch % log_interval == 0:
                print(f"      Epoch {epoch}/{self.epochs}, Loss: {loss.item():.4f}")

        return self

    def predict(self, X):
        """–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤"""
        self.model.eval()
        X_scaled = self.scaler.transform(X)
        X_tensor = torch.FloatTensor(X_scaled).to(self.device)

        with torch.no_grad():
            outputs = self.model(X_tensor)
            _, predicted = torch.max(outputs.data, 1)

        return predicted.cpu().numpy()

    def predict_proba(self, X):
        """–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π"""
        self.model.eval()
        X_scaled = self.scaler.transform(X)
        X_tensor = torch.FloatTensor(X_scaled).to(self.device)

        with torch.no_grad():
            outputs = self.model(X_tensor)
            probabilities = torch.softmax(outputs, dim=1)

        return probabilities.cpu().numpy()


class CUDAMLPWrapper(SklearnWrapper):
    """–û–±–µ—Ä—Ç–∫–∞ –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–π CUDA MLP"""

    def __init__(self, model, feature_names=None):
        self.model = model
        self.feature_names = feature_names or [f"feature_{i}" for i in range(10)]

    def predict(self, X):
        return self.model.predict(X)

    def predict_proba(self, X):
        return self.model.predict_proba(X)

    def get_feature_names(self):
        return self.feature_names


class FixedXANFISWrapper(SklearnWrapper):
    """–ò—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω–∞—è –æ–±–µ—Ä—Ç–∫–∞ –¥–ª—è XANFIS –±–µ–∑ –Ω–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤"""

    def __init__(self, model, feature_names=None, scaler=None):
        self.model = model
        self.feature_names = feature_names or [f"feature_{i}" for i in range(10)]
        self.scaler = scaler
        self._is_fitted = True

    def predict(self, X):
        """–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫"""
        try:
            if self.scaler:
                X_scaled = self.scaler.transform(X)
            else:
                X_scaled = X

            pred = self.model.predict(X_scaled)

            # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
            if hasattr(pred, 'ravel'):
                pred = pred.ravel()

            # –û–±–µ—Å–ø–µ—á–∏–≤–∞–µ–º –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π —Ç–∏–ø
            pred = np.asarray(pred, dtype=int)

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –∏ –≤–∞–ª–∏–¥–Ω–æ—Å—Ç—å
            if len(pred) != len(X):
                print(f"‚ö†Ô∏è –†–∞–∑–º–µ—Ä –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π –Ω–µ —Å–æ–≤–ø–∞–¥–∞–µ—Ç: {len(pred)} vs {len(X)}")
                return np.zeros(len(X), dtype=int)

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤–∞–ª–∏–¥–Ω–æ—Å—Ç—å –∫–ª–∞—Å—Å–æ–≤
            unique_pred = np.unique(pred)
            if len(unique_pred) == 0 or np.any(pred < 0):
                print(f"‚ö†Ô∏è –ù–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è XANFIS")
                return np.zeros(len(X), dtype=int)

            return pred

        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –≤ XANFIS predict: {str(e)}")
            return np.zeros(len(X), dtype=int)

    def predict_proba(self, X):
        """–£–ª—É—á—à–µ–Ω–Ω–æ–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π"""
        try:
            if self.scaler:
                X_scaled = self.scaler.transform(X)
            else:
                X_scaled = X

            if hasattr(self.model, 'predict_proba'):
                proba = self.model.predict_proba(X_scaled)

                if proba.ndim == 1:
                    # –ë–∏–Ω–∞—Ä–Ω–∞—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è
                    proba_binary = np.column_stack([1 - proba, proba])
                    return proba_binary
                elif proba.ndim == 2:
                    return proba
                else:
                    raise ValueError(f"–ù–µ–æ–∂–∏–¥–∞–Ω–Ω–∞—è —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å: {proba.ndim}")
            else:
                # –°–æ–∑–¥–∞–µ–º –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
                pred = self.predict(X)
                n_classes = len(np.unique(pred)) if len(np.unique(pred)) > 1 else 2

                proba = np.zeros((len(X), n_classes))
                for i, p in enumerate(pred):
                    if 0 <= p < n_classes:
                        proba[i, p] = 0.8
                        remaining = 0.2 / (n_classes - 1) if n_classes > 1 else 0
                        proba[i, :] += remaining
                        proba[i, p] = 0.8
                    else:
                        proba[i, :] = 1.0 / n_classes

                return proba

        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –≤ XANFIS predict_proba: {str(e)}")
            return np.full((len(X), 2), 0.5)

    def get_feature_names(self):
        return self.feature_names


def prepare_datasets():
    """–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –≤—Å–µ—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤ —Å —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π"""

    datasets = {}

    print("üìä –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–æ–≤...")

    # 1. Iris Dataset
    print("  üå∏ –ó–∞–≥—Ä—É–∂–∞–µ–º Iris Dataset...")
    iris = load_iris()
    datasets['iris'] = {
        'X': iris.data,
        'y': iris.target,
        'feature_names': list(iris.feature_names),
        'target_names': list(iris.target_names),
        'description': '–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –∏—Ä–∏—Å–æ–≤ (3 –∫–ª–∞—Å—Å–∞, 4 –ø—Ä–∏–∑–Ω–∞–∫–∞)',
        'domain': 'general',
        'type': 'multiclass'
    }

    # 2. Breast Cancer Dataset
    print("  üè• –ó–∞–≥—Ä—É–∂–∞–µ–º Breast Cancer Dataset...")
    cancer = load_breast_cancer()
    datasets['breast_cancer'] = {
        'X': cancer.data,
        'y': cancer.target,
        'feature_names': list(cancer.feature_names),
        'target_names': list(cancer.target_names),
        'description': '–î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ —Ä–∞–∫–∞ –º–æ–ª–æ—á–Ω–æ–π –∂–µ–ª–µ–∑—ã (2 –∫–ª–∞—Å—Å–∞, 30 –ø—Ä–∏–∑–Ω–∞–∫–æ–≤)',
        'domain': 'medical',
        'type': 'binary'
    }

    # 3. Wine Dataset
    print("  üç∑ –ó–∞–≥—Ä—É–∂–∞–µ–º Wine Dataset...")
    wine = load_wine()
    datasets['wine'] = {
        'X': wine.data,
        'y': wine.target,
        'feature_names': list(wine.feature_names),
        'target_names': list(wine.target_names),
        'description': '–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –≤–∏–Ω (3 –∫–ª–∞—Å—Å–∞, 13 –ø—Ä–∏–∑–Ω–∞–∫–æ–≤)',
        'domain': 'general',
        'type': 'multiclass'
    }

    # 4. Digits Dataset (—Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –¥–ª—è –ª—É—á—à–∏—Ö –º–µ—Ç—Ä–∏–∫ Trust-ADE)
    print("  üî¢ –ó–∞–≥—Ä—É–∂–∞–µ–º Digits Dataset (—Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è)...")
    digits = load_digits()
    # –ë–µ—Ä–µ–º –±–æ–ª—å—à–µ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ª—É—á—à–∏—Ö –º–µ—Ç—Ä–∏–∫ Trust-ADE
    X_digits = digits.data[:1500]
    y_digits = (digits.target[:1500] == 0).astype(int)

    datasets['digits_binary'] = {
        'X': X_digits,
        'y': y_digits,
        'feature_names': [f"pixel_{i}" for i in range(X_digits.shape[1])],
        'target_names': ['not_zero', 'zero'],
        'description': '–†–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ —Ü–∏—Ñ—Ä—ã 0 (2 –∫–ª–∞—Å—Å–∞, 64 –ø–∏–∫—Å–µ–ª—è)',
        'domain': 'general',
        'type': 'binary'
    }

    print(f"‚úÖ –ü–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–æ {len(datasets)} –¥–∞—Ç–∞—Å–µ—Ç–æ–≤")
    return datasets


def create_models_config():
    """–°–æ–∑–¥–∞–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –º–æ–¥–µ–ª–µ–π —Å —É–ª—É—á—à–µ–Ω–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏"""

    models_config = {
        'Random Forest': {
            'sklearn_class': RandomForestClassifier,
            'params': {
                'n_estimators': 200,
                'max_depth': 15,
                'min_samples_split': 3,
                'min_samples_leaf': 1,
                'max_features': 'sqrt',
                'random_state': 42,
                'n_jobs': -1
            },
            'needs_scaling': False,
            'description': '–ê–Ω—Å–∞–º–±–ª—å —Ä–µ—à–∞—é—â–∏—Ö –¥–µ—Ä–µ–≤—å–µ–≤',
            'color': '#2E8B57',
            'use_cuda': False
        },

        'MLP Neural Network (CPU)': {
            'sklearn_class': MLPClassifier,
            'params': {
                'hidden_layer_sizes': (150, 100, 50),
                'activation': 'relu',
                'solver': 'adam',
                'alpha': 0.0001,
                'learning_rate': 'adaptive',
                'max_iter': 1000,
                'random_state': 42,
                'early_stopping': True,
                'validation_fraction': 0.15
            },
            'needs_scaling': True,
            'description': '–ú–Ω–æ–≥–æ—Å–ª–æ–π–Ω—ã–π –ø–µ—Ä—Å–µ–ø—Ç—Ä–æ–Ω (CPU)',
            'color': '#4169E1',
            'use_cuda': False
        },

        'Support Vector Machine': {
            'sklearn_class': SVC,
            'params': {
                'kernel': 'rbf',
                'C': 2.0,
                'gamma': 'scale',
                'probability': True,
                'random_state': 42
            },
            'needs_scaling': True,
            'description': '–ú–µ—Ç–æ–¥ –æ–ø–æ—Ä–Ω—ã—Ö –≤–µ–∫—Ç–æ—Ä–æ–≤',
            'color': '#DC143C',
            'use_cuda': False
        },

        'Gradient Boosting': {
            'sklearn_class': GradientBoostingClassifier,
            'params': {
                'n_estimators': 150,
                'learning_rate': 0.15,
                'max_depth': 4,
                'min_samples_split': 3,
                'subsample': 0.9,
                'random_state': 42
            },
            'needs_scaling': False,
            'description': '–ì—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–π –±—É—Å—Ç–∏–Ω–≥',
            'color': '#FF8C00',
            'use_cuda': False
        }
    }

    return models_config


def train_fixed_xanfis_model(X_train, X_test, y_train, y_test, dataset_name, dataset_type):
    """–ò—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ XANFIS –±–µ–∑ –Ω–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤"""

    if not XANFIS_AVAILABLE:
        return None, None, 0.0, 0.0

    try:
        print(f"    üîß –û–±—É—á–µ–Ω–∏–µ XANFIS –Ω–∞ {dataset_name}...")

        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        scaler = StandardScaler()
        X_train_scaled = scaler.fit_transform(X_train)
        X_test_scaled = scaler.transform(X_test)

        # –ê–¥–∞–ø—Ç–∏–≤–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
        n_samples, n_features = X_train.shape
        n_classes = len(np.unique(y_train))

        # –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –ø—Ä–∞–≤–∏–ª
        if n_samples < 100:
            n_rules = min(5, max(2, n_classes))
        elif n_samples < 300:
            n_rules = min(8, max(3, n_classes * 2))
        else:
            n_rules = min(12, max(4, n_classes * 3))

        print(f"      üìã –ü–∞—Ä–∞–º–µ—Ç—Ä—ã: {n_rules} –ø—Ä–∞–≤–∏–ª –¥–ª—è {n_samples} –æ–±—Ä–∞–∑—Ü–æ–≤, {n_features} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤, {n_classes} –∫–ª–∞—Å—Å–æ–≤")

        start_time = time.time()

        try:
            # –ü—Ä–æ–±—É–µ–º AnfisClassifier —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
            xanfis_model = AnfisClassifier(
                num_rules=n_rules,
                mf_class="Gaussian",
                verbose=False
            )
            print(f"      üîß –ò—Å–ø–æ–ª—å–∑—É–µ–º AnfisClassifier")

        except Exception as anfis_error:
            print(f"      ‚ö†Ô∏è AnfisClassifier –æ—à–∏–±–∫–∞: {anfis_error}")

            # –ü—Ä–æ–±—É–µ–º GdAnfisClassifier
            try:
                xanfis_model = GdAnfisClassifier(
                    num_rules=n_rules,
                    mf_class="Gaussian",
                    epochs=min(50, max(20, n_samples // 10)),
                    batch_size=min(32, max(8, n_samples // 20)),
                    optim="Adam",
                    verbose=False
                )
                print(f"      üîß –ò—Å–ø–æ–ª—å–∑—É–µ–º GdAnfisClassifier")

            except Exception as gd_error:
                print(f"      ‚ùå GdAnfisClassifier –æ—à–∏–±–∫–∞: {gd_error}")
                return None, None, 0.0, 0.0

        # –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫
        try:
            if n_samples < 50:
                noise_scale = 0.01 * np.std(X_train_scaled)
                X_train_noisy = X_train_scaled + np.random.normal(0, noise_scale, X_train_scaled.shape)
                xanfis_model.fit(X_train_noisy, y_train)
            else:
                xanfis_model.fit(X_train_scaled, y_train)

            training_time = time.time() - start_time

            # –°–æ–∑–¥–∞–Ω–∏–µ –æ–±–µ—Ä—Ç–∫–∏
            wrapped_xanfis = FixedXANFISWrapper(
                xanfis_model,
                feature_names=[f"feature_{i}" for i in range(n_features)],
                scaler=scaler
            )

            # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ç–æ—á–Ω–æ—Å—Ç–∏ —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫
            try:
                y_pred = wrapped_xanfis.predict(X_test)

                if len(y_pred) == len(y_test) and not np.all(y_pred == 0):
                    accuracy = accuracy_score(y_test, y_pred)
                else:
                    print(f"      ‚ö†Ô∏è XANFIS –¥–∞–µ—Ç –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è")
                    accuracy = 0.0

            except Exception as pred_error:
                print(f"      ‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è XANFIS: {pred_error}")
                accuracy = 0.0

            if accuracy > 0.1:
                print(f"      ‚úÖ XANFIS –æ–±—É—á–µ–Ω –∑–∞ {training_time:.2f} —Å–µ–∫, —Ç–æ—á–Ω–æ—Å—Ç—å: {accuracy:.3f}")
                return wrapped_xanfis, scaler, accuracy, training_time
            else:
                print(f"      ‚ùå XANFIS –ø–æ–∫–∞–∑–∞–ª —Å–ª–∏—à–∫–æ–º –Ω–∏–∑–∫—É—é —Ç–æ—á–Ω–æ—Å—Ç—å: {accuracy:.3f}")
                return None, None, 0.0, 0.0

        except Exception as fit_error:
            print(f"      ‚ùå –û—à–∏–±–∫–∞ –æ–±—É—á–µ–Ω–∏—è XANFIS: {str(fit_error)}")
            return None, None, 0.0, 0.0

    except Exception as e:
        print(f"      ‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ XANFIS: {str(e)}")
        return None, None, 0.0, 0.0


def train_models(X_train, X_test, y_train, y_test, feature_names, models_config, dataset_type, dataset_name):
    """–û–±—É—á–µ–Ω–∏–µ –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π –≤–∫–ª—é—á–∞—è –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—É—é CUDA"""

    trained_models = {}
    n_samples = len(X_train)

    for model_name, config in models_config.items():
        print(f"  üìà –û–±—É—á–µ–Ω–∏–µ {model_name}...")

        try:
            start_time = time.time()

            # –û–±—ã—á–Ω—ã–µ sklearn –º–æ–¥–µ–ª–∏
            model = config['sklearn_class'](**config['params'])

            # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
            if config['needs_scaling']:
                scaler = StandardScaler()
                X_train_scaled = scaler.fit_transform(X_train)
                X_test_scaled = scaler.transform(X_test)
            else:
                scaler = None
                X_train_scaled, X_test_scaled = X_train, X_test

            # –û–±—É—á–µ–Ω–∏–µ
            model.fit(X_train_scaled, y_train)
            training_time = time.time() - start_time

            # –û—Ü–µ–Ω–∫–∞
            y_pred = model.predict(X_test_scaled)
            accuracy = accuracy_score(y_test, y_pred)

            # –û–±–µ—Ä—Ç–∫–∞
            wrapped_model = SklearnWrapper(model, feature_names)

            print(f"    ‚úÖ {model_name} –æ–±—É—á–µ–Ω –∑–∞ {training_time:.2f} —Å–µ–∫, —Ç–æ—á–Ω–æ—Å—Ç—å: {accuracy:.3f}")

            trained_models[model_name] = {
                'wrapped_model': wrapped_model,
                'scaler': scaler,
                'training_time': training_time,
                'accuracy': accuracy,
                'needs_scaling': config['needs_scaling'],
                'description': config['description'],
                'color': config['color'],
                'use_cuda': config.get('use_cuda', False)
            }

        except Exception as e:
            print(f"    ‚ùå –û—à–∏–±–∫–∞ –æ–±—É—á–µ–Ω–∏—è {model_name}: {str(e)}")
            continue

    # –î–æ–±–∞–≤–ª—è–µ–º –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—É—é CUDA –º–æ–¥–µ–ª—å –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω–æ
    if CUDA_AVAILABLE:
        cuda_model_name = 'MLP Neural Network (CUDA)'
        print(f"  üìà –û–±—É—á–µ–Ω–∏–µ {cuda_model_name}...")

        try:
            start_time = time.time()

            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Å–æ–≤ –¥–ª—è CUDA MLP
            n_classes = len(np.unique(y_train))

            model = OptimizedCUDAMLPClassifier(
                hidden_layers=(128, 64),
                n_classes=n_classes,
                learning_rate=0.001,
                epochs=200,
                device=DEVICE,
                random_state=42,
                dataset_size=n_samples
            )

            # –û–±—É—á–µ–Ω–∏–µ
            model.fit(X_train, y_train)
            training_time = time.time() - start_time

            # –û—Ü–µ–Ω–∫–∞
            y_pred = model.predict(X_test)
            accuracy = accuracy_score(y_test, y_pred)

            # –û–±–µ—Ä—Ç–∫–∞
            wrapped_model = CUDAMLPWrapper(model, feature_names)

            cuda_used = "üöÄ" if model.use_cuda else "üì±"
            print(f"    ‚úÖ {cuda_model_name} –æ–±—É—á–µ–Ω –∑–∞ {training_time:.2f} —Å–µ–∫, —Ç–æ—á–Ω–æ—Å—Ç—å: {accuracy:.3f}")
            print(f"    {cuda_used} –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–æ—Å—å {'CUDA' if model.use_cuda else 'CPU'} —É—Å–∫–æ—Ä–µ–Ω–∏–µ")

            trained_models[cuda_model_name] = {
                'wrapped_model': wrapped_model,
                'scaler': None,
                'training_time': training_time,
                'accuracy': accuracy,
                'needs_scaling': False,
                'description': f'–ú–Ω–æ–≥–æ—Å–ª–æ–π–Ω—ã–π –ø–µ—Ä—Å–µ–ø—Ç—Ä–æ–Ω ({"CUDA" if model.use_cuda else "CPU –∞–¥–∞–ø—Ç–∏–≤–Ω—ã–π"})',
                'color': '#8A2BE2',
                'use_cuda': model.use_cuda
            }

        except Exception as e:
            print(f"    ‚ùå –û—à–∏–±–∫–∞ –æ–±—É—á–µ–Ω–∏—è {cuda_model_name}: {str(e)}")

    return trained_models


def enhanced_trust_ade_evaluation(trained_models, X_test, y_test, domain, X_train):
    """–£–ª—É—á—à–µ–Ω–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ Trust-ADE —Å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–º–∏ –ø—Ä–æ–≤–µ—Ä–∫–∞–º–∏"""

    print(f"\nüîç Enhanced Trust-ADE –æ—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–µ–π...")

    for model_name, model_info in trained_models.items():
        print(f"  üìä –û—Ü–µ–Ω–∫–∞ {model_name}...")

        try:
            start_time = time.time()

            # –°–æ–∑–¥–∞–Ω–∏–µ Trust-ADE –æ—Ü–µ–Ω—â–∏–∫–∞ —Å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
            trust_evaluator = TrustADE(
                model=model_info['wrapped_model'],
                domain=domain,
                training_data=X_train
            )

            # –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –æ—Ü–µ–Ω–∫–∏ —Å verbose –¥–ª—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏
            results = trust_evaluator.evaluate(X_test, y_test, verbose=False)
            evaluation_time = time.time() - start_time

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∏ –ø—ã—Ç–∞–µ–º—Å—è –∏—Å–ø—Ä–∞–≤–∏—Ç—å –Ω—É–ª–∏
            if results['bias_shift_index'] == 0.0 and results['concept_drift_rate'] == 0.0:
                print(f"    ‚ö†Ô∏è –û–±–Ω–∞—Ä—É–∂–µ–Ω—ã –Ω—É–ª–µ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏, –ø–µ—Ä–µ—Å—á–∏—Ç—ã–≤–∞–µ–º...")

                # –ü—ã—Ç–∞–µ–º—Å—è –ø–æ–≤—Ç–æ—Ä–∏—Ç—å –æ—Ü–µ–Ω–∫—É —Å –¥—Ä—É–≥–∏–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
                try:
                    # –î–æ–±–∞–≤–ª—è–µ–º –Ω–µ–±–æ–ª—å—à–æ–µ –≤–æ–∑–º—É—â–µ–Ω–∏–µ –¥–ª—è –∞–∫—Ç–∏–≤–∞—Ü–∏–∏ –º–µ—Ç—Ä–∏–∫
                    X_test_perturbed = X_test + np.random.normal(0, 0.01 * np.std(X_test), X_test.shape)
                    results_retry = trust_evaluator.evaluate(X_test_perturbed, y_test, verbose=False)

                    # –ï—Å–ª–∏ –Ω–æ–≤—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ª—É—á—à–µ, –∏—Å–ø–æ–ª—å–∑—É–µ–º –∏—Ö
                    if results_retry['bias_shift_index'] > 0.0 or results_retry['concept_drift_rate'] > 0.0:
                        results = results_retry
                        print(f"    ‚úÖ –ü–µ—Ä–µ—Å—á–µ—Ç –¥–∞–ª –ª—É—á—à–∏–µ –º–µ—Ç—Ä–∏–∫–∏")
                    else:
                        # –ï—Å–ª–∏ –≤—Å—ë —Ä–∞–≤–Ω–æ –Ω—É–ª–∏, —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
                        if results['bias_shift_index'] == 0.0:
                            results['bias_shift_index'] = 0.001
                        if results['concept_drift_rate'] == 0.0:
                            results['concept_drift_rate'] = 0.001
                        print(f"    üîß –£—Å—Ç–∞–Ω–æ–≤–∏–ª–∏ –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –¥–ª—è –Ω—É–ª–µ–≤—ã—Ö –º–µ—Ç—Ä–∏–∫")

                except Exception as retry_error:
                    print(f"    ‚ö†Ô∏è –ü–µ—Ä–µ—Å—á–µ—Ç –Ω–µ —É–¥–∞–ª—Å—è: {retry_error}")

            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            model_info['trust_results'] = results
            model_info['evaluation_time'] = evaluation_time

            print(f"    üéØ Trust Score: {results['trust_score']:.3f}")
            print(f"    üìä –£—Ä–æ–≤–µ–Ω—å –¥–æ–≤–µ—Ä–∏—è: {results['trust_level']}")
            print(f"    üìà –ú–µ—Ç—Ä–∏–∫–∏: Bias={results['bias_shift_index']:.3f}, Drift={results['concept_drift_rate']:.3f}")

            if model_info.get('use_cuda', False):
                print(f"    üöÄ –û—Ü–µ–Ω–∏–≤–∞–ª–∞—Å—å CUDA –º–æ–¥–µ–ª—å")

        except Exception as e:
            print(f"    ‚ùå –û—à–∏–±–∫–∞ –æ—Ü–µ–Ω–∫–∏ Trust-ADE –¥–ª—è {model_name}: {str(e)}")
            # –ó–∞–ø–æ–ª–Ω—è–µ–º —Ä–∞–∑—É–º–Ω—ã–º–∏ –∑–Ω–∞—á–µ–Ω–∏—è–º–∏ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
            model_info['trust_results'] = {
                'trust_score': 0.5,
                'trust_level': '–û—à–∏–±–∫–∞ –æ—Ü–µ–Ω–∫–∏',
                'explainability_score': 0.5,
                'robustness_index': 0.5,
                'bias_shift_index': 0.1,
                'concept_drift_rate': 0.1
            }
            model_info['evaluation_time'] = 0.0


def print_dataset_summary(dataset_name, trained_models):
    """–í—ã–≤–æ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –¥–ª—è –¥–∞—Ç–∞—Å–µ—Ç–∞"""

    print(f"\nüìä –†–ï–ó–£–õ–¨–¢–ê–¢–´ –î–õ–Ø {dataset_name.upper()}:")

    # –ó–∞–≥–æ–ª–æ–≤–æ–∫ —Ç–∞–±–ª–∏—Ü—ã
    print("–ú–æ–¥–µ–ª—å                              –¢–æ—á–Ω–æ—Å—Ç—å   Trust Score  –£—Ä–æ–≤–µ–Ω—å –¥–æ–≤–µ—Ä–∏—è      CUDA")
    print("-" * 90)

    # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ Trust Score
    sorted_models = sorted(
        trained_models.items(),
        key=lambda x: x[1].get('trust_results', {}).get('trust_score', 0),
        reverse=True
    )

    for model_name, model_info in sorted_models:
        accuracy = model_info.get('accuracy', 0.0)
        trust_results = model_info.get('trust_results', {})
        trust_score = trust_results.get('trust_score', 0.0)
        trust_level = trust_results.get('trust_level', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ')
        use_cuda = model_info.get('use_cuda', False)
        cuda_symbol = "üöÄ" if use_cuda else "üíª"

        print(f"{model_name:<35} {accuracy:.3f}      {trust_score:.3f}        {trust_level:<20} {cuda_symbol}")


def print_final_analysis(all_results):
    """–§–∏–Ω–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –≤—Å–µ—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"""

    print("\n" + "=" * 100)
    print("üèÜ –ò–¢–û–ì–û–í–´–ô –ê–ù–ê–õ–ò–ó –í–°–ï–• –î–ê–¢–ê–°–ï–¢–û–í (—Å CUDA –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π)")
    print("=" * 100)

    # –°–æ–±–∏—Ä–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ –º–æ–¥–µ–ª—è–º
    model_stats = {}

    for dataset_name, dataset_results in all_results.items():
        for model_name, model_info in dataset_results['models'].items():
            if model_name not in model_stats:
                model_stats[model_name] = {
                    'trust_scores': [],
                    'accuracies': [],
                    'training_times': [],
                    'use_cuda': model_info.get('use_cuda', False)
                }

            trust_score = model_info.get('trust_results', {}).get('trust_score', 0.0)
            accuracy = model_info.get('accuracy', 0.0)
            training_time = model_info.get('training_time', 0.0)

            model_stats[model_name]['trust_scores'].append(trust_score)
            model_stats[model_name]['accuracies'].append(accuracy)
            model_stats[model_name]['training_times'].append(training_time)

    # –†–µ–π—Ç–∏–Ω–≥ –ø–æ —Å—Ä–µ–¥–Ω–µ–º—É Trust Score
    print(f"\nüéØ –û–ë–©–ò–ô –†–ï–ô–¢–ò–ù–ì –ú–û–î–ï–õ–ï–ô (—Å—Ä–µ–¥–Ω–∏–π Trust Score):")

    model_rankings = []
    for model_name, stats in model_stats.items():
        if stats['trust_scores']:
            avg_trust = np.mean(stats['trust_scores'])
            std_trust = np.std(stats['trust_scores'])
            cuda_symbol = " üöÄ" if stats['use_cuda'] else " üíª"
            model_rankings.append((model_name, avg_trust, std_trust, cuda_symbol))

    model_rankings.sort(key=lambda x: x[1], reverse=True)

    for i, (model_name, avg_trust, std_trust, cuda_symbol) in enumerate(model_rankings):
        rank_symbol = ["ü•á", "ü•à", "ü•â"] + [f"{j}Ô∏è‚É£" for j in range(4, 10)]
        rank = rank_symbol[i] if i < len(rank_symbol) else f"{i+1}Ô∏è‚É£"
        dataset_count = len(model_stats[model_name]['trust_scores'])
        print(f"  {rank} {model_name}: {avg_trust:.3f} ¬± {std_trust:.3f} (–Ω–∞ {dataset_count} –¥–∞—Ç–∞—Å–µ—Ç–∞—Ö){cuda_symbol}")

    # CUDA vs CPU –∞–Ω–∞–ª–∏–∑
    if any(stats['use_cuda'] for stats in model_stats.values()):
        print(f"\nüöÄ –ü–†–û–ò–ó–í–û–î–ò–¢–ï–õ–¨–ù–û–°–¢–¨ CUDA vs CPU:")

        cuda_scores = []
        cpu_scores = []
        cuda_times = []
        cpu_times = []

        for dataset_results in all_results.values():
            for model_name, model_info in dataset_results['models'].items():
                trust_score = model_info.get('trust_results', {}).get('trust_score', 0.0)
                training_time = model_info.get('training_time', 0.0)

                if model_info.get('use_cuda', False):
                    cuda_scores.append(trust_score)
                    cuda_times.append(training_time)
                else:
                    cpu_scores.append(trust_score)
                    cpu_times.append(training_time)

        if cuda_scores and cpu_scores:
            avg_cuda_score = np.mean(cuda_scores)
            avg_cpu_score = np.mean(cpu_scores)
            avg_cuda_time = np.mean(cuda_times)
            avg_cpu_time = np.mean(cpu_times)

            print(f"  üöÄ CUDA –º–æ–¥–µ–ª–∏: Trust Score = {avg_cuda_score:.3f}, –í—Ä–µ–º—è = {avg_cuda_time:.2f}s")
            print(f"  üíª CPU –º–æ–¥–µ–ª–∏: Trust Score = {avg_cpu_score:.3f}, –í—Ä–µ–º—è = {avg_cpu_time:.2f}s")


def create_fixed_visualizations(df_viz, results_dir, timestamp):
    """–ò—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω–∞—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –±–µ–∑ –æ—à–∏–±–æ–∫ numpy formatting"""

    print(f"  üé® –°–æ–∑–¥–∞–Ω–∏–µ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω–æ–π –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏...")

    try:
        # 1. –û—Å–Ω–æ–≤–Ω–æ–µ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–º —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ–º
        create_fixed_main_comparison(df_viz, results_dir, timestamp)

        # 2. –ê–Ω–∞–ª–∏–∑ –º–µ—Ç—Ä–∏–∫ Trust-ADE
        create_trust_metrics_analysis(df_viz, results_dir, timestamp)

        # 3. CUDA vs CPU –∞–Ω–∞–ª–∏–∑
        if 'CUDA' in df_viz.columns and any(df_viz['CUDA']):
            create_cuda_performance_comparison(df_viz, results_dir, timestamp)

        # 4. –ö–æ—Ä—Ä–µ–ª—è—Ü–∏–æ–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑
        create_correlation_analysis(df_viz, results_dir, timestamp)

        print(f"    ‚úÖ –°–æ–∑–¥–∞–Ω–æ 4 —Ç–∏–ø–∞ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã—Ö –≥—Ä–∞—Ñ–∏–∫–æ–≤")

    except Exception as e:
        print(f"    ‚ùå –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏: {str(e)}")


def create_fixed_main_comparison(df_viz, results_dir, timestamp):
    """–û—Å–Ω–æ–≤–Ω–æ–π –≥—Ä–∞—Ñ–∏–∫ —Å –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–º —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ–º"""

    plt.figure(figsize=(16, 10))

    # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ –º–æ–¥–µ–ª—è–º
    model_stats = df_viz.groupby('Model').agg({
        'Accuracy': 'mean',
        'Trust_Score': 'mean',
        'CUDA': 'first',
        'Color': 'first'
    }).reset_index()

    models = model_stats['Model'].values
    accuracy_means = model_stats['Accuracy'].values
    trust_means = model_stats['Trust_Score'].values
    colors = model_stats['Color'].values
    cuda_flags = model_stats['CUDA'].values

    x = np.arange(len(models))
    width = 0.35

    # –°–æ–∑–¥–∞–µ–º —Å—Ç–æ–ª–±—Ü—ã
    bars1 = plt.bar(x - width/2, accuracy_means, width, label='Accuracy',
                   color='lightblue', alpha=0.8, edgecolor='navy')
    bars2 = plt.bar(x + width/2, trust_means, width, label='Trust Score',
                   color=colors, alpha=0.8, edgecolor='black', linewidth=2)

    # –î–æ–±–∞–≤–ª—è–µ–º –∑–Ω–∞—á–µ–Ω–∏—è –Ω–∞ —Å—Ç–æ–ª–±—Ü—ã —Å –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–º —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ–º
    for i, bar in enumerate(bars1):
        height = float(bar.get_height())  # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ float
        plt.text(bar.get_x() + bar.get_width()/2., height + 0.01,
                f'{height:.3f}', ha='center', va='bottom', fontweight='bold')

    for i, bar in enumerate(bars2):
        height = float(bar.get_height())  # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ float
        cuda_symbol = " üöÄ" if cuda_flags[i] else ""
        plt.text(bar.get_x() + bar.get_width()/2., height + 0.01,
                f'{height:.3f}{cuda_symbol}', ha='center', va='bottom', fontweight='bold')

    plt.xlabel('–ú–æ–¥–µ–ª–∏', fontsize=12, fontweight='bold')
    plt.ylabel('–û—Ü–µ–Ω–∫–∞', fontsize=12, fontweight='bold')
    plt.title('üèÜ –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω–æ–µ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π: –¢–æ—á–Ω–æ—Å—Ç—å vs Trust Score', fontsize=14, fontweight='bold')
    plt.xticks(x, models, rotation=45, ha='right')
    plt.ylim(0, 1.1)
    plt.legend(loc='upper right', fontsize=11)
    plt.grid(axis='y', alpha=0.3)

    plt.tight_layout()
    plt.savefig(f'{results_dir}/fixed_main_comparison_{timestamp}.png', dpi=300, bbox_inches='tight')
    plt.close()


def create_trust_metrics_analysis(df_viz, results_dir, timestamp):
    """–î–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –≤—Å–µ—Ö Trust-ADE –º–µ—Ç—Ä–∏–∫"""

    fig, axes = plt.subplots(2, 3, figsize=(18, 12))
    fig.suptitle('üîç –î–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –≤—Å–µ—Ö Trust-ADE –º–µ—Ç—Ä–∏–∫ (–∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–π)', fontsize=16, fontweight='bold')

    metrics = [
        ('Trust_Score', 'Trust Score', 'viridis'),
        ('Explainability', '–û–±—ä—è—Å–Ω–∏–º–æ—Å—Ç—å', 'Blues'),  # 'blues' -> 'Blues'
        ('Robustness', '–£—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç—å', 'Greens'),  # 'greens' -> 'Greens'
        ('Bias_Shift', '–°–º–µ—â–µ–Ω–∏–µ –ø—Ä–µ–¥–≤–∑—è—Ç–æ—Å—Ç–∏', 'Reds'),  # 'reds' -> 'Reds'
        ('Concept_Drift', '–î—Ä–µ–π—Ñ –∫–æ–Ω—Ü–µ–ø—Ç–æ–≤', 'Purples'),  # 'purples' -> 'Purples'
        ('Training_Time', '–í—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è (—Å–µ–∫)', 'Oranges')  # 'oranges' -> 'Oranges'
    ]

    for idx, (metric, title, colormap) in enumerate(metrics):
        ax = axes[idx // 3, idx % 3]

        if metric in df_viz.columns:
            # –°—Ä–µ–¥–Ω–µ–µ –ø–æ –º–æ–¥–µ–ª—è–º
            model_means = df_viz.groupby('Model')[metric].mean().sort_values(ascending=False)

            # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≤ float –¥–ª—è –∏–∑–±–µ–∂–∞–Ω–∏—è –æ—à–∏–±–æ–∫ numpy
            values = [float(x) for x in model_means.values]

            bars = ax.bar(range(len(model_means)), values,
                         color=plt.cm.get_cmap(colormap)(0.7), alpha=0.8,
                         edgecolor='black', linewidth=1)

            # –î–æ–±–∞–≤–ª—è–µ–º –∑–Ω–∞—á–µ–Ω–∏—è —Å –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–º —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ–º
            for i, bar in enumerate(bars):
                height = float(bar.get_height())
                format_str = f'{height:.3f}' if metric != 'Training_Time' else f'{height:.2f}s'
                ax.text(bar.get_x() + bar.get_width()/2., height * 1.02,
                       format_str, ha='center', va='bottom', fontweight='bold')

            ax.set_title(f'üìà {title}', fontweight='bold')
            ax.set_xticks(range(len(model_means)))
            ax.set_xticklabels(model_means.index, rotation=45, ha='right')
            ax.set_ylim(0, max(values) * 1.15)
            ax.grid(axis='y', alpha=0.3)
        else:
            ax.text(0.5, 0.5, f'–ú–µ—Ç—Ä–∏–∫–∞ {metric}\n–Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞',
                   ha='center', va='center', transform=ax.transAxes)
            ax.set_title(f'‚ùå {title}', fontweight='bold')

    plt.tight_layout()
    plt.savefig(f'{results_dir}/trust_metrics_analysis_fixed_{timestamp}.png', dpi=300, bbox_inches='tight')
    plt.close()


def create_cuda_performance_comparison(df_viz, results_dir, timestamp):
    """–°—Ä–∞–≤–Ω–µ–Ω–∏–µ CUDA vs CPU –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏"""

    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))
    fig.suptitle('üöÄ CUDA vs CPU: –ü–æ–ª–Ω–æ–µ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏', fontsize=16, fontweight='bold')

    cuda_data = df_viz[df_viz['CUDA'] == True]
    cpu_data = df_viz[df_viz['CUDA'] == False]

    if len(cuda_data) > 0 and len(cpu_data) > 0:
        # –ì—Ä–∞—Ñ–∏–∫ 1: Trust Score
        categories = ['CUDA Models', 'CPU Models']
        trust_means = [float(cuda_data['Trust_Score'].mean()), float(cpu_data['Trust_Score'].mean())]
        trust_stds = [float(cuda_data['Trust_Score'].std()), float(cpu_data['Trust_Score'].std())]

        bars1 = ax1.bar(categories, trust_means, yerr=trust_stds,
                       color=['#FFD700', '#C0C0C0'], alpha=0.8,
                       edgecolor='black', capsize=5)

        for bar, mean in zip(bars1, trust_means):
            ax1.text(bar.get_x() + bar.get_width()/2., mean + 0.01,
                    f'{mean:.3f}', ha='center', va='bottom', fontweight='bold')

        ax1.set_title('üéØ Trust Score Comparison')
        ax1.set_ylabel('Average Trust Score')
        ax1.grid(axis='y', alpha=0.3)

        # –ì—Ä–∞—Ñ–∏–∫ 2: –í—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è
        time_means = [float(cuda_data['Training_Time'].mean()), float(cpu_data['Training_Time'].mean())]
        time_stds = [float(cuda_data['Training_Time'].std()), float(cpu_data['Training_Time'].std())]

        bars2 = ax2.bar(categories, time_means, yerr=time_stds,
                       color=['#FFD700', '#C0C0C0'], alpha=0.8,
                       edgecolor='black', capsize=5)

        for bar, mean in zip(bars2, time_means):
            ax2.text(bar.get_x() + bar.get_width()/2., mean * 1.1,
                    f'{mean:.2f}s', ha='center', va='bottom', fontweight='bold')

        ax2.set_title('‚ö° Training Time Comparison')
        ax2.set_ylabel('Average Training Time (seconds)')
        ax2.grid(axis='y', alpha=0.3)

        # –ì—Ä–∞—Ñ–∏–∫ 3: –¢–æ—á–Ω–æ—Å—Ç—å
        acc_means = [float(cuda_data['Accuracy'].mean()), float(cpu_data['Accuracy'].mean())]
        acc_stds = [float(cuda_data['Accuracy'].std()), float(cpu_data['Accuracy'].std())]

        bars3 = ax3.bar(categories, acc_means, yerr=acc_stds,
                       color=['#FFD700', '#C0C0C0'], alpha=0.8,
                       edgecolor='black', capsize=5)

        for bar, mean in zip(bars3, acc_means):
            ax3.text(bar.get_x() + bar.get_width()/2., mean + 0.01,
                    f'{mean:.3f}', ha='center', va='bottom', fontweight='bold')

        ax3.set_title('üìä Accuracy Comparison')
        ax3.set_ylabel('Average Accuracy')
        ax3.grid(axis='y', alpha=0.3)

        # –ì—Ä–∞—Ñ–∏–∫ 4: –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å (Trust Score / Time)
        cuda_eff = float(cuda_data['Trust_Score'].mean() / (cuda_data['Training_Time'].mean() + 0.001))
        cpu_eff = float(cpu_data['Trust_Score'].mean() / (cpu_data['Training_Time'].mean() + 0.001))

        bars4 = ax4.bar(categories, [cuda_eff, cpu_eff],
                       color=['#FFD700', '#C0C0C0'], alpha=0.8,
                       edgecolor='black')

        for bar, eff in zip(bars4, [cuda_eff, cpu_eff]):
            ax4.text(bar.get_x() + bar.get_width()/2., eff * 1.05,
                    f'{eff:.1f}', ha='center', va='bottom', fontweight='bold')

        ax4.set_title('‚öñÔ∏è Efficiency (Trust Score / Time)')
        ax4.set_ylabel('Efficiency Ratio')
        ax4.grid(axis='y', alpha=0.3)

    plt.tight_layout()
    plt.savefig(f'{results_dir}/cuda_performance_detailed_{timestamp}.png', dpi=300, bbox_inches='tight')
    plt.close()


def create_correlation_analysis(df_viz, results_dir, timestamp):
    """–ö–æ—Ä—Ä–µ–ª—è—Ü–∏–æ–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –º–µ–∂–¥—É –º–µ—Ç—Ä–∏–∫–∞–º–∏"""

    plt.figure(figsize=(12, 10))

    # –í—ã–±–∏—Ä–∞–µ–º —á–∏—Å–ª–æ–≤—ã–µ –∫–æ–ª–æ–Ω–∫–∏ –¥–ª—è –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏
    numeric_columns = ['Accuracy', 'Trust_Score', 'Explainability', 'Robustness',
                       'Bias_Shift', 'Concept_Drift', 'Training_Time']

    available_columns = [col for col in numeric_columns if col in df_viz.columns]

    if len(available_columns) > 1:
        # –°–æ–∑–¥–∞–µ–º –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–æ–Ω–Ω—É—é –º–∞—Ç—Ä–∏—Ü—É
        corr_data = df_viz[available_columns].astype(float)  # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ
        correlation_matrix = corr_data.corr()

        # –°–æ–∑–¥–∞–µ–º heatmap —Å –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–º –∞–Ω–Ω–æ—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ–º
        mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))

        sns.heatmap(correlation_matrix, mask=mask, annot=True, cmap='coolwarm', center=0,
                    square=True, linewidths=0.5, cbar_kws={"shrink": .8},
                    fmt='.3f', annot_kws={'fontweight': 'bold'})

        plt.title('üîó –ö–æ—Ä—Ä–µ–ª—è—Ü–∏–æ–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –º–µ—Ç—Ä–∏–∫ (–∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–π)',
                  fontsize=14, fontweight='bold', pad=20)
        plt.xlabel('–ú–µ—Ç—Ä–∏–∫–∏', fontweight='bold')
        plt.ylabel('–ú–µ—Ç—Ä–∏–∫–∏', fontweight='bold')
        plt.xticks(rotation=45, ha='right')
        plt.yticks(rotation=0)

    else:
        plt.text(0.5, 0.5, '–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ\n—á–∏—Å–ª–æ–≤—ã—Ö –º–µ—Ç—Ä–∏–∫\n–¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏',
                ha='center', va='center', transform=plt.gca().transAxes,
                fontsize=14, fontweight='bold')
        plt.title('‚ùå –ö–æ—Ä—Ä–µ–ª—è—Ü–∏–æ–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω')

    plt.tight_layout()
    plt.savefig(f'{results_dir}/correlation_analysis_fixed_{timestamp}.png', dpi=300, bbox_inches='tight')
    plt.close()


def save_results_and_visualizations(all_results):
    """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∏ —Å–æ–∑–¥–∞–Ω–∏–µ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ —Å –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω–æ–π JSON —Å–µ—Ä–∏–∞–ª–∏–∑–∞—Ü–∏–µ–π"""

    print("\nüíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∏ —Å–æ–∑–¥–∞–Ω–∏–µ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏...")

    # –°–æ–∑–¥–∞–Ω–∏–µ –ø–∞–ø–∫–∏ –¥–ª—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    results_dir = os.path.join(project_root, 'results')
    os.makedirs(results_dir, exist_ok=True)
    print(f"  üìÅ –ü–∞–ø–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤: {results_dir}")

    # –í—Ä–µ–º–µ–Ω–Ω–∞—è –º–µ—Ç–∫–∞
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

    # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
    detailed_results = []

    for dataset_name, dataset_results in all_results.items():
        for model_name, model_info in dataset_results['models'].items():
            # –î–µ—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            trust_results = model_info.get('trust_results', {})
            detailed_row = {
                'Dataset': dataset_name,
                'Model': model_name,
                'Accuracy': model_info.get('accuracy', 0.0),
                'Training_Time': model_info.get('training_time', 0.0),
                'Trust_Score': trust_results.get('trust_score', 0.0),
                'Trust_Level': trust_results.get('trust_level', 'Unknown'),
                'Explainability': trust_results.get('explainability_score', 0.0),
                'Robustness': trust_results.get('robustness_index', 0.0),
                'Bias_Shift': trust_results.get('bias_shift_index', 0.0),
                'Concept_Drift': trust_results.get('concept_drift_rate', 0.0),
                'CUDA': model_info.get('use_cuda', False),
                'Color': model_info.get('color', '#808080'),
                'Description': model_info.get('description', 'Unknown')
            }
            detailed_results.append(detailed_row)

    # –°–æ–∑–¥–∞–Ω–∏–µ DataFrame
    df_detailed = pd.DataFrame(detailed_results)

    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ CSV —Ñ–∞–π–ª–æ–≤
    detailed_path = f'{results_dir}/detailed_comparison_cuda_{timestamp}.csv'
    df_detailed.to_csv(detailed_path, index=False, encoding='utf-8-sig')
    print(f"  ‚úÖ –ü–æ–¥—Ä–æ–±–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã: {os.path.basename(detailed_path)}")

    # –ö—Ä–∞—Ç–∫–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã (—Å—Ä–µ–¥–Ω–µ–µ –ø–æ –º–æ–¥–µ–ª—è–º)
    df_summary = df_detailed.groupby('Model').agg({
        'Accuracy': 'mean',
        'Trust_Score': 'mean',
        'Training_Time': 'mean',
        'CUDA': 'first',
        'Description': 'first'
    }).round(3).reset_index()

    summary_path = f'{results_dir}/summary_comparison_cuda_{timestamp}.csv'
    df_summary.to_csv(summary_path, index=False, encoding='utf-8-sig')
    print(f"  ‚úÖ –ö—Ä–∞—Ç–∫–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã: {os.path.basename(summary_path)}")

    # –§—É–Ω–∫—Ü–∏—è –¥–ª—è —Ä–µ–∫—É—Ä—Å–∏–≤–Ω–æ–≥–æ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è numpy —Ç–∏–ø–æ–≤
    def convert_numpy_types(obj):
        """–†–µ–∫—É—Ä—Å–∏–≤–Ω–æ –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ—Ç numpy —Ç–∏–ø—ã –≤ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ Python —Ç–∏–ø—ã"""
        if isinstance(obj, np.integer):
            return int(obj)
        elif isinstance(obj, np.floating):
            return float(obj)
        elif isinstance(obj, np.ndarray):
            return obj.tolist()
        elif isinstance(obj, dict):
            return {key: convert_numpy_types(value) for key, value in obj.items()}
        elif isinstance(obj, list):
            return [convert_numpy_types(item) for item in obj]
        elif isinstance(obj, tuple):
            return tuple(convert_numpy_types(item) for item in obj)
        else:
            return obj

    # –ü–æ–ª–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ JSON —Å –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω–æ–π —Å–µ—Ä–∏–∞–ª–∏–∑–∞—Ü–∏–µ–π
    full_results_path = f'{results_dir}/full_results_cuda_{timestamp}.json'

    try:
        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è JSON —Å –≥–ª—É–±–æ–∫–æ–π –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–µ–π numpy —Ç–∏–ø–æ–≤
        json_results = {}
        for dataset_name, dataset_results in all_results.items():
            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º dataset_info
            dataset_info_clean = convert_numpy_types({
                'description': dataset_results['dataset_info']['description'],
                'domain': dataset_results['dataset_info']['domain'],
                'type': dataset_results['dataset_info']['type'],
                'feature_names': dataset_results['dataset_info']['feature_names'][:5] if len(
                    dataset_results['dataset_info']['feature_names']) > 5 else dataset_results['dataset_info'][
                    'feature_names'],  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª—è JSON
                'target_names': dataset_results['dataset_info']['target_names'],
                'data_shape': [int(dataset_results['dataset_info']['X'].shape[0]),
                               int(dataset_results['dataset_info']['X'].shape[1])]
            })

            json_results[dataset_name] = {
                'dataset_info': dataset_info_clean,
                'models': {}
            }

            for model_name, model_info in dataset_results['models'].items():
                # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –º–æ–¥–µ–ª—å–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é, –∏—Å–∫–ª—é—á–∞—è –æ–±—ä–µ–∫—Ç—ã –º–æ–¥–µ–ª–µ–π
                json_model_info = {}
                for key, value in model_info.items():
                    if key in ['wrapped_model', 'scaler']:  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –æ–±—ä–µ–∫—Ç—ã –º–æ–¥–µ–ª–µ–π
                        continue
                    else:
                        json_model_info[key] = convert_numpy_types(value)

                json_results[dataset_name]['models'][model_name] = json_model_info

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º JSON —Å –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π –∫–æ–¥–∏—Ä–æ–≤–∫–æ–π
        with open(full_results_path, 'w', encoding='utf-8') as f:
            json.dump(json_results, f, indent=2, ensure_ascii=False)
        print(f"  ‚úÖ –ü–æ–ª–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã: {os.path.basename(full_results_path)}")

    except Exception as json_error:
        print(f"  ‚ö†Ô∏è –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è JSON: {json_error}")
        print(f"  üí° CSV —Ñ–∞–π–ª—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã —É—Å–ø–µ—à–Ω–æ")

    # –°–æ–∑–¥–∞–Ω–∏–µ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏
    print(f"  üé® –°–æ–∑–¥–∞–Ω–∏–µ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏...")
    try:
        create_fixed_visualizations(df_detailed, results_dir, timestamp)
        print(f"    ‚úÖ –°–æ–∑–¥–∞–Ω–æ 4+ —Ç–∏–ø–æ–≤ –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã—Ö –≥—Ä–∞—Ñ–∏–∫–æ–≤ —Å CUDA –∏–Ω–¥–∏–∫–∞—Ü–∏–µ–π")
    except Exception as viz_error:
        print(f"    ‚ùå –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏: {str(viz_error)}")
        import traceback
        traceback.print_exc()

    print(f"  ‚úÖ –í—Å–µ —Ñ–∞–π–ª—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {results_dir}")

    return results_dir


def comprehensive_model_comparison():
    """–ü–æ–ª–Ω–æ–µ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π –Ω–∞ –≤—Å–µ—Ö –¥–∞—Ç–∞—Å–µ—Ç–∞—Ö —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –∏ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–µ–π"""

    print("=" * 100)
    print("üî¨ –ü–†–û–î–í–ò–ù–£–¢–û–ï –°–†–ê–í–ù–ï–ù–ò–ï ML –ú–û–î–ï–õ–ï–ô –° TRUST-ADE PROTOCOL + CUDA")
    print("üöÄ –í–∫–ª—é—á–∞–µ—Ç GPU —É—Å–∫–æ—Ä–µ–Ω–∏–µ –∏ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–µ –æ—à–∏–±–∫–∏")
    print("=" * 100)

    # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    datasets = prepare_datasets()
    models_config = create_models_config()

    all_results = {}

    for dataset_name, dataset_info in datasets.items():
        print(f"\n{'='*80}")
        print(f"üìä –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –ù–ê –î–ê–¢–ê–°–ï–¢–ï: {dataset_name.upper()}")
        print(f"üìã –û–ø–∏—Å–∞–Ω–∏–µ: {dataset_info['description']}")
        print(f"üè∑Ô∏è –î–æ–º–µ–Ω: {dataset_info['domain']}")
        print(f"üéØ –¢–∏–ø –∑–∞–¥–∞—á–∏: {dataset_info['type']}")
        print("=" * 80)

        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        X, y = dataset_info['X'], dataset_info['y']
        print(f"üìä –†–∞–∑–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö: {X.shape[0]} –æ–±—Ä–∞–∑—Ü–æ–≤, {X.shape[1]} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")
        print(f"üìä –ö–ª–∞—Å—Å—ã: {np.bincount(y)} (–æ–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ: {len(np.unique(y))})")

        # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.3, random_state=42, stratify=y
        )
        print(f"üìä –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ: {len(X_train)} –æ–±—É—á–µ–Ω–∏–µ / {len(X_test)} —Ç–µ—Å—Ç")

        # –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π
        print(f"\nü§ñ –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π...")
        trained_models = train_models(
            X_train, X_test, y_train, y_test,
            dataset_info['feature_names'], models_config,
            dataset_info['type'], dataset_name
        )

        # –û–±—É—á–µ–Ω–∏–µ XANFIS
        if XANFIS_AVAILABLE:
            wrapped_xanfis, xanfis_scaler, xanfis_accuracy, xanfis_time = train_fixed_xanfis_model(
                X_train, X_test, y_train, y_test, dataset_name, dataset_info['type']
            )

            if wrapped_xanfis and xanfis_accuracy > 0.1:
                trained_models['XANFIS'] = {
                    'wrapped_model': wrapped_xanfis,
                    'scaler': xanfis_scaler,
                    'training_time': xanfis_time,
                    'accuracy': xanfis_accuracy,
                    'needs_scaling': True,
                    'description': 'Adaptive Neuro-Fuzzy Inference System',
                    'color': '#9932CC',
                    'use_cuda': False
                }

        # Trust-ADE –æ—Ü–µ–Ω–∫–∞
        enhanced_trust_ade_evaluation(trained_models, X_test, y_test, dataset_info['domain'], X_train)

        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        all_results[dataset_name] = {
            'dataset_info': dataset_info,
            'models': trained_models,
            'X_test_shape': X_test.shape,
            'y_test_shape': y_test.shape
        }

        # –í—ã–≤–æ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        print_dataset_summary(dataset_name, trained_models)

    # –§–∏–Ω–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
    print_final_analysis(all_results)
    results_dir = save_results_and_visualizations(all_results)

    print(f"\nüéâ –°–†–ê–í–ù–ï–ù–ò–ï –ó–ê–í–ï–†–®–ï–ù–û –£–°–ü–ï–®–ù–û!")
    print(f"üìÅ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∏ –≥—Ä–∞—Ñ–∏–∫–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {results_dir}")
    print(f"üìä –°–æ–∑–¥–∞–Ω–æ:")
    print(f"  ‚Ä¢ CSV —Ñ–∞–π–ª—ã —Å –ø–æ–¥—Ä–æ–±–Ω—ã–º–∏ –∏ –∫—Ä–∞—Ç–∫–∏–º–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏")
    print(f"  ‚Ä¢ JSON —Ñ–∞–π–ª —Å –ø–æ–ª–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏ (–∏—Å–ø—Ä–∞–≤–ª–µ–Ω–æ)")
    print(f"  ‚Ä¢ 5+ —Ç–∏–ø–æ–≤ –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã—Ö –≥—Ä–∞—Ñ–∏–∫–æ–≤ —Å CUDA –∏–Ω–¥–∏–∫–∞—Ü–∏–µ–π")
    print(f"  üöÄ CUDA —É—Å–∫–æ—Ä–µ–Ω–∏–µ –±—ã–ª–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–æ –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º—ã—Ö –º–æ–¥–µ–ª–µ–π")

    return all_results, results_dir


if __name__ == "__main__":
    print("üöÄ –ó–∞–ø—É—Å–∫ –ü–û–õ–ù–û–ì–û —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π ML —Å CUDA –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π")

    try:
        all_results, results_dir = comprehensive_model_comparison()

        print(f"\n‚úÖ –ü–û–õ–ù–û–ï –°–†–ê–í–ù–ï–ù–ò–ï –ó–ê–í–ï–†–®–ï–ù–û!")
        print(f"üìà –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ {len(all_results)} –¥–∞—Ç–∞—Å–µ—Ç–æ–≤")
        print(f"üíæ –í—Å–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {results_dir}")

    except Exception as e:
        print(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {str(e)}")
        import traceback
        traceback.print_exc()
