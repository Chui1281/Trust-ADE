"""
–ê–Ω–∞–ª–∏–∑ –∏ –≤—ã–≤–æ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
"""
import numpy as np


def print_dataset_summary(dataset_name, trained_models):
    """–í—ã–≤–æ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –¥–ª—è –¥–∞—Ç–∞—Å–µ—Ç–∞"""

    print(f"\nüìä –†–ï–ó–£–õ–¨–¢–ê–¢–´ –î–õ–Ø {dataset_name.upper()}:")

    # –ó–∞–≥–æ–ª–æ–≤–æ–∫ —Ç–∞–±–ª–∏—Ü—ã
    print("–ú–æ–¥–µ–ª—å                              –¢–æ—á–Ω–æ—Å—Ç—å   Trust Score  –£—Ä–æ–≤–µ–Ω—å –¥–æ–≤–µ—Ä–∏—è      CUDA")
    print("-" * 90)

    # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ Trust Score
    sorted_models = sorted(
        trained_models.items(),
        key=lambda x: x[1].get('trust_results', {}).get('trust_score', 0),
        reverse=True
    )

    for model_name, model_info in sorted_models:
        accuracy = model_info.get('accuracy', 0.0)
        trust_results = model_info.get('trust_results', {})
        trust_score = trust_results.get('trust_score', 0.0)
        trust_level = trust_results.get('trust_level', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ')
        use_cuda = model_info.get('use_cuda', False)
        cuda_symbol = "üöÄ" if use_cuda else "üíª"

        print(f"{model_name:<35} {accuracy:.3f}      {trust_score:.3f}        {trust_level:<20} {cuda_symbol}")


def print_final_analysis(all_results):
    """–§–∏–Ω–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –≤—Å–µ—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"""

    print("\n" + "=" * 100)
    print("üèÜ –ò–¢–û–ì–û–í–´–ô –ê–ù–ê–õ–ò–ó –í–°–ï–• –î–ê–¢–ê–°–ï–¢–û–í (—Å CUDA –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π)")
    print("=" * 100)

    # –°–æ–±–∏—Ä–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ –º–æ–¥–µ–ª—è–º
    model_stats = {}

    for dataset_name, dataset_results in all_results.items():
        for model_name, model_info in dataset_results['models'].items():
            if model_name not in model_stats:
                model_stats[model_name] = {
                    'trust_scores': [],
                    'accuracies': [],
                    'training_times': [],
                    'use_cuda': model_info.get('use_cuda', False)
                }

            trust_score = model_info.get('trust_results', {}).get('trust_score', 0.0)
            accuracy = model_info.get('accuracy', 0.0)
            training_time = model_info.get('training_time', 0.0)

            model_stats[model_name]['trust_scores'].append(trust_score)
            model_stats[model_name]['accuracies'].append(accuracy)
            model_stats[model_name]['training_times'].append(training_time)

    # –†–µ–π—Ç–∏–Ω–≥ –ø–æ —Å—Ä–µ–¥–Ω–µ–º—É Trust Score
    print(f"\nüéØ –û–ë–©–ò–ô –†–ï–ô–¢–ò–ù–ì –ú–û–î–ï–õ–ï–ô (—Å—Ä–µ–¥–Ω–∏–π Trust Score):")

    model_rankings = []
    for model_name, stats in model_stats.items():
        if stats['trust_scores']:
            avg_trust = np.mean(stats['trust_scores'])
            std_trust = np.std(stats['trust_scores'])
            cuda_symbol = " üöÄ" if stats['use_cuda'] else " üíª"
            model_rankings.append((model_name, avg_trust, std_trust, cuda_symbol))

    model_rankings.sort(key=lambda x: x[1], reverse=True)

    for i, (model_name, avg_trust, std_trust, cuda_symbol) in enumerate(model_rankings):
        rank_symbol = ["ü•á", "ü•à", "ü•â"] + [f"{j}Ô∏è‚É£" for j in range(4, 10)]
        rank = rank_symbol[i] if i < len(rank_symbol) else f"{i + 1}Ô∏è‚É£"
        dataset_count = len(model_stats[model_name]['trust_scores'])
        print(f"  {rank} {model_name}: {avg_trust:.3f} ¬± {std_trust:.3f} (–Ω–∞ {dataset_count} –¥–∞—Ç–∞—Å–µ—Ç–∞—Ö){cuda_symbol}")

