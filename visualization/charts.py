"""
–†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –º–æ–¥—É–ª—å –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ –¥–ª—è Trust-ADE Protocol
–í–∫–ª—é—á–∞–µ—Ç –æ—Å–Ω–æ–≤–Ω—ã–µ –∏ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Ç–∏–ø—ã –≥—Ä–∞—Ñ–∏–∫–æ–≤ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –¥–æ–≤–µ—Ä–∏—è –∫ ML –º–æ–¥–µ–ª—è–º
"""

import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
import pandas as pd
from matplotlib.patches import Patch
import warnings
from math import pi
import os

warnings.filterwarnings('ignore')

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ —Å—Ç–∏–ª—è matplotlib
plt.style.use('default')
sns.set_palette("husl")
plt.rcParams['figure.dpi'] = 100
plt.rcParams['savefig.dpi'] = 300
plt.rcParams['font.size'] = 10
plt.rcParams['axes.titlesize'] = 12
plt.rcParams['axes.labelsize'] = 10
plt.rcParams['xtick.labelsize'] = 9
plt.rcParams['ytick.labelsize'] = 9
plt.rcParams['legend.fontsize'] = 9

# –¶–≤–µ—Ç–æ–≤–∞—è –ø–∞–ª–∏—Ç—Ä–∞ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —Ç–∏–ø–æ–≤ –º–æ–¥–µ–ª–µ–π
MODEL_COLORS = {
    'Support Vector Machine': '#DC143C',
    'Random Forest': '#2E8B57', 
    'Gradient Boosting': '#FF8C00',
    'MLP Neural Network (CPU)': '#4169E1',
    'MLP Neural Network (CUDA)': '#8A2BE2',
    'XANFIS': '#9932CC',
    'Default': '#808080'
}

def create_extended_visualizations(df_viz, results_dir, timestamp):
    """–°–æ–∑–¥–∞–Ω–∏–µ —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–≥–æ –Ω–∞–±–æ—Ä–∞ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–π"""
    
    print(f"  üé® –°–æ–∑–¥–∞–Ω–∏–µ —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–π –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ Trust-ADE...")
    
    try:
        # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –µ—Å–ª–∏ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
        os.makedirs(results_dir, exist_ok=True)
        
        # 1. –û—Å–Ω–æ–≤–Ω—ã–µ –≥—Ä–∞—Ñ–∏–∫–∏ (–∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–µ –≤–µ—Ä—Å–∏–∏)
        create_fixed_main_comparison(df_viz, results_dir, timestamp)
        create_trust_metrics_analysis(df_viz, results_dir, timestamp)
        
        # 2. CUDA vs CPU –∞–Ω–∞–ª–∏–∑ (–µ—Å–ª–∏ –µ—Å—Ç—å CUDA –¥–∞–Ω–Ω—ã–µ)
        if 'CUDA' in df_viz.columns and any(df_viz['CUDA']):
            create_cuda_performance_comparison(df_viz, results_dir, timestamp)
        
        # 3. –ö–æ—Ä—Ä–µ–ª—è—Ü–∏–æ–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑
        create_correlation_analysis(df_viz, results_dir, timestamp)
        
        # 4. –ù–û–í–´–ï –î–û–ü–û–õ–ù–ò–¢–ï–õ–¨–ù–´–ï –ì–†–ê–§–ò–ö–ò
        create_radar_chart_comparison(df_viz, results_dir, timestamp)
        create_performance_matrix(df_viz, results_dir, timestamp)
        create_dataset_comparison(df_viz, results_dir, timestamp)
        create_metric_distributions(df_viz, results_dir, timestamp)
        create_trust_score_breakdown(df_viz, results_dir, timestamp)
        create_efficiency_analysis(df_viz, results_dir, timestamp)
        create_model_ranking_chart(df_viz, results_dir, timestamp)
        create_detailed_heatmap(df_viz, results_dir, timestamp)
        
        print(f"    ‚úÖ –°–æ–∑–¥–∞–Ω–æ 12 —Ç–∏–ø–æ–≤ –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã—Ö –≥—Ä–∞—Ñ–∏–∫–æ–≤")
        
    except Exception as e:
        print(f"    ‚ùå –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏: {str(e)}")
        import traceback
        traceback.print_exc()


def create_fixed_main_comparison(df_viz, results_dir, timestamp):
    """–û—Å–Ω–æ–≤–Ω–æ–π –≥—Ä–∞—Ñ–∏–∫ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π —Å –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–º —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ–º"""
    
    try:
        plt.figure(figsize=(16, 10))
        
        # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ –º–æ–¥–µ–ª—è–º
        model_stats = df_viz.groupby('Model').agg({
            'Accuracy': 'mean',
            'Trust_Score': 'mean',
            'CUDA': 'first',
            'Color': 'first'
        }).reset_index()
        
        models = model_stats['Model'].values
        accuracy_means = model_stats['Accuracy'].values.astype(float)
        trust_means = model_stats['Trust_Score'].values.astype(float)
        colors = [MODEL_COLORS.get(model, MODEL_COLORS['Default']) for model in models]
        cuda_flags = model_stats['CUDA'].values if 'CUDA' in model_stats.columns else [False] * len(models)
        
        x = np.arange(len(models))
        width = 0.35
        
        # –°–æ–∑–¥–∞–µ–º —Å—Ç–æ–ª–±—Ü—ã
        bars1 = plt.bar(x - width / 2, accuracy_means, width, label='Accuracy',
                        color='lightblue', alpha=0.8, edgecolor='navy')
        bars2 = plt.bar(x + width / 2, trust_means, width, label='Trust Score',
                        color=colors, alpha=0.8, edgecolor='black', linewidth=2)
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∑–Ω–∞—á–µ–Ω–∏—è –Ω–∞ —Å—Ç–æ–ª–±—Ü—ã
        for i, bar in enumerate(bars1):
            height = float(bar.get_height())
            plt.text(bar.get_x() + bar.get_width() / 2., height + 0.01,
                     f'{height:.3f}', ha='center', va='bottom', fontweight='bold')
        
        for i, bar in enumerate(bars2):
            height = float(bar.get_height())
            cuda_symbol = " üöÄ" if cuda_flags[i] else ""
            plt.text(bar.get_x() + bar.get_width() / 2., height + 0.01,
                     f'{height:.3f}{cuda_symbol}', ha='center', va='bottom', fontweight='bold')

        plt.xlabel('Models', fontsize=12, fontweight='bold')
        plt.ylabel('Score', fontsize=12, fontweight='bold')
        plt.title('üèÜ Trust-ADE: Model Accuracy and Trust Score Comparison', fontsize=14, fontweight='bold')
        plt.xticks(x, models, rotation=45, ha='right')
        plt.ylim(0, 1.1)
        plt.legend(loc='upper right', fontsize=11)
        plt.grid(axis='y', alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(f'{results_dir}/main_comparison_{timestamp}.png', dpi=300, bbox_inches='tight')
        plt.close()
        
    except Exception as e:
        print(f"    ‚ùå –û—à–∏–±–∫–∞ –≤ create_fixed_main_comparison: {e}")
        plt.close()


def create_trust_metrics_analysis(df_viz, results_dir, timestamp):
    """–î–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –≤—Å–µ—Ö Trust-ADE –º–µ—Ç—Ä–∏–∫"""
    
    try:
        fig, axes = plt.subplots(2, 3, figsize=(18, 12))
        fig.suptitle('üîç Detailed Trust-ADE Metrics Analysis', fontsize=16, fontweight='bold')
        metrics = [
            ('Trust_Score', 'Trust Score', 'viridis'),
            ('Explainability', 'Explainability', 'Blues'),
            ('Robustness', 'Robustness', 'Greens'),
            ('Bias_Shift', 'Bias Shift', 'Reds'),
            ('Concept_Drift', 'Concept Drift', 'Purples'),
            ('Training_Time', 'Training Time (s)', 'Oranges')
        ]

        for idx, (metric, title, colormap) in enumerate(metrics):
            ax = axes[idx // 3, idx % 3]
            
            if metric in df_viz.columns:
                model_means = df_viz.groupby('Model')[metric].mean().sort_values(ascending=False)
                values = [float(x) for x in model_means.values]
                
                if values:
                    bars = ax.bar(range(len(model_means)), values,
                                  color=plt.cm.get_cmap(colormap)(0.7), alpha=0.8,
                                  edgecolor='black', linewidth=1)
                    
                    for i, bar in enumerate(bars):
                        height = float(bar.get_height())
                        format_str = f'{height:.3f}' if metric != 'Training_Time' else f'{height:.2f}s'
                        ax.text(bar.get_x() + bar.get_width() / 2., height * 1.02,
                                format_str, ha='center', va='bottom', fontweight='bold', fontsize=8)
                    
                    ax.set_title(f'üìà {title}', fontweight='bold')
                    ax.set_xticks(range(len(model_means)))
                    ax.set_xticklabels(model_means.index, rotation=45, ha='right')
                    ax.set_ylim(0, max(values) * 1.15 if max(values) > 0 else 1)
                    ax.grid(axis='y', alpha=0.3)
                else:
                    ax.text(0.5, 0.5, f'–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö\n–¥–ª—è –º–µ—Ç—Ä–∏–∫–∏\n{metric}',
                            ha='center', va='center', transform=ax.transAxes)
            else:
                ax.text(0.5, 0.5, f'–ú–µ—Ç—Ä–∏–∫–∞\n{metric}\n–Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞',
                        ha='center', va='center', transform=ax.transAxes)
                ax.set_title(f'‚ùå {title}', fontweight='bold')
        
        plt.tight_layout()
        plt.savefig(f'{results_dir}/trust_metrics_analysis_{timestamp}.png', dpi=300, bbox_inches='tight')
        plt.close()
        
    except Exception as e:
        print(f"    ‚ùå –û—à–∏–±–∫–∞ –≤ create_trust_metrics_analysis: {e}")
        plt.close()


def create_cuda_performance_comparison(df_viz, results_dir, timestamp):
    """–°—Ä–∞–≤–Ω–µ–Ω–∏–µ CUDA vs CPU –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏"""
    
    try:
        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))
        fig.suptitle('üöÄ CUDA vs CPU: Performance Comparison', fontsize=16, fontweight='bold')
        
        cuda_data = df_viz[df_viz['CUDA'] == True]
        cpu_data = df_viz[df_viz['CUDA'] == False]
        
        if len(cuda_data) > 0 and len(cpu_data) > 0:
            categories = ['CUDA Models', 'CPU Models']
            
            # –ì—Ä–∞—Ñ–∏–∫ 1: Trust Score
            trust_means = [float(cuda_data['Trust_Score'].mean()), float(cpu_data['Trust_Score'].mean())]
            trust_stds = [float(cuda_data['Trust_Score'].std()), float(cpu_data['Trust_Score'].std())]
            trust_stds = [std if not np.isnan(std) else 0 for std in trust_stds]
            
            bars1 = ax1.bar(categories, trust_means, yerr=trust_stds,
                            color=['#FFD700', '#C0C0C0'], alpha=0.8,
                            edgecolor='black', capsize=5)
            
            for bar, mean in zip(bars1, trust_means):
                ax1.text(bar.get_x() + bar.get_width() / 2., mean + 0.01,
                         f'{mean:.3f}', ha='center', va='bottom', fontweight='bold')
            
            ax1.set_title('üéØ Trust Score Comparison')
            ax1.set_ylabel('Average Trust Score')
            ax1.grid(axis='y', alpha=0.3)
            
            # –ì—Ä–∞—Ñ–∏–∫ 2: –í—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è
            time_means = [float(cuda_data['Training_Time'].mean()), float(cpu_data['Training_Time'].mean())]
            time_stds = [float(cuda_data['Training_Time'].std()), float(cpu_data['Training_Time'].std())]
            time_stds = [std if not np.isnan(std) else 0 for std in time_stds]
            
            bars2 = ax2.bar(categories, time_means, yerr=time_stds,
                            color=['#FFD700', '#C0C0C0'], alpha=0.8,
                            edgecolor='black', capsize=5)
            
            for bar, mean in zip(bars2, time_means):
                ax2.text(bar.get_x() + bar.get_width() / 2., mean * 1.1,
                         f'{mean:.2f}s', ha='center', va='bottom', fontweight='bold')
            
            ax2.set_title('‚ö° Training Time Comparison')
            ax2.set_ylabel('Average Training Time (seconds)')
            ax2.grid(axis='y', alpha=0.3)
            
            # –ì—Ä–∞—Ñ–∏–∫ 3: –¢–æ—á–Ω–æ—Å—Ç—å
            acc_means = [float(cuda_data['Accuracy'].mean()), float(cpu_data['Accuracy'].mean())]
            acc_stds = [float(cuda_data['Accuracy'].std()), float(cpu_data['Accuracy'].std())]
            acc_stds = [std if not np.isnan(std) else 0 for std in acc_stds]
            
            bars3 = ax3.bar(categories, acc_means, yerr=acc_stds,
                            color=['#FFD700', '#C0C0C0'], alpha=0.8,
                            edgecolor='black', capsize=5)
            
            for bar, mean in zip(bars3, acc_means):
                ax3.text(bar.get_x() + bar.get_width() / 2., mean + 0.01,
                         f'{mean:.3f}', ha='center', va='bottom', fontweight='bold')
            
            ax3.set_title('üìä Accuracy Comparison')
            ax3.set_ylabel('Average Accuracy')
            ax3.grid(axis='y', alpha=0.3)
            
            # –ì—Ä–∞—Ñ–∏–∫ 4: –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å
            cuda_eff = float(cuda_data['Trust_Score'].mean() / max(cuda_data['Training_Time'].mean(), 0.001))
            cpu_eff = float(cpu_data['Trust_Score'].mean() / max(cpu_data['Training_Time'].mean(), 0.001))
            
            bars4 = ax4.bar(categories, [cuda_eff, cpu_eff],
                            color=['#FFD700', '#C0C0C0'], alpha=0.8, edgecolor='black')
            
            for bar, eff in zip(bars4, [cuda_eff, cpu_eff]):
                ax4.text(bar.get_x() + bar.get_width() / 2., eff * 1.05,
                         f'{eff:.1f}', ha='center', va='bottom', fontweight='bold')
            
            ax4.set_title('‚öñÔ∏è Efficiency (Trust Score / Time)')
            ax4.set_ylabel('Efficiency Ratio')
            ax4.grid(axis='y', alpha=0.3)
        
        else:
            for ax in [ax1, ax2, ax3, ax4]:
                ax.text(0.5, 0.5, '–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö\n–¥–ª—è CUDA vs CPU\n—Å—Ä–∞–≤–Ω–µ–Ω–∏—è',
                        ha='center', va='center', transform=ax.transAxes,
                        fontsize=12, fontweight='bold')
        
        plt.tight_layout()
        plt.savefig(f'{results_dir}/cuda_performance_{timestamp}.png', dpi=300, bbox_inches='tight')
        plt.close()
        
    except Exception as e:
        print(f"    ‚ùå –û—à–∏–±–∫–∞ –≤ create_cuda_performance_comparison: {e}")
        plt.close()


def create_correlation_analysis(df_viz, results_dir, timestamp):
    """–ö–æ—Ä—Ä–µ–ª—è—Ü–∏–æ–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –º–µ–∂–¥—É –º–µ—Ç—Ä–∏–∫–∞–º–∏"""
    
    try:
        plt.figure(figsize=(12, 10))
        
        numeric_columns = ['Accuracy', 'Trust_Score', 'Explainability', 'Robustness',
                           'Bias_Shift', 'Concept_Drift', 'Training_Time']
        
        available_columns = [col for col in numeric_columns if col in df_viz.columns]
        
        if len(available_columns) > 1:
            corr_data = df_viz[available_columns].astype(float)
            correlation_matrix = corr_data.corr()
            
            if not correlation_matrix.empty:
                mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))
                
                sns.heatmap(correlation_matrix, mask=mask, annot=True, cmap='coolwarm', center=0,
                            square=True, linewidths=0.5, cbar_kws={"shrink": .8},
                            fmt='.3f', annot_kws={'fontweight': 'bold'})
                
                plt.title('üîó Correlation Analysis of Trust-ADE Metrics',
                          fontsize=14, fontweight='bold', pad=20)
                plt.xlabel('Metrics', fontweight='bold')
                plt.ylabel('Metrics', fontweight='bold')
                plt.xticks(rotation=45, ha='right')
                plt.yticks(rotation=0)
            else:
                plt.text(0.5, 0.5, '–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è\n–∫–æ—Ä—Ä–µ–ª—è—Ü–∏–æ–Ω–Ω–æ–≥–æ\n–∞–Ω–∞–ª–∏–∑–∞',
                         ha='center', va='center', transform=plt.gca().transAxes,
                         fontsize=14, fontweight='bold')
        else:
            plt.text(0.5, 0.5, '–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ\n–º–µ—Ç—Ä–∏–∫ –¥–ª—è\n–∞–Ω–∞–ª–∏–∑–∞ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏',
                     ha='center', va='center', transform=plt.gca().transAxes,
                     fontsize=14, fontweight='bold')
        
        plt.tight_layout()
        plt.savefig(f'{results_dir}/correlation_analysis_{timestamp}.png', dpi=300, bbox_inches='tight')
        plt.close()
        
    except Exception as e:
        print(f"    ‚ùå –û—à–∏–±–∫–∞ –≤ create_correlation_analysis: {e}")
        plt.close()


def create_radar_chart_comparison(df_viz, results_dir, timestamp):
    """–†–∞–¥–∞—Ä–Ω–∞—è –¥–∏–∞–≥—Ä–∞–º–º–∞ –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π –ø–æ –≤—Å–µ–º –º–µ—Ç—Ä–∏–∫–∞–º"""
    
    try:
        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Ä–∞–¥–∞—Ä–Ω–æ–π –¥–∏–∞–≥—Ä–∞–º–º—ã
        radar_metrics = ['Trust_Score', 'Accuracy', 'Explainability', 'Robustness']
        available_metrics = [m for m in radar_metrics if m in df_viz.columns]
        
        if len(available_metrics) < 3:
            return
        
        model_stats = df_viz.groupby('Model')[available_metrics].mean()
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ —Ä–∞–¥–∞—Ä–Ω–æ–π –¥–∏–∞–≥—Ä–∞–º–º—ã
        angles = [n / float(len(available_metrics)) * 2 * pi for n in range(len(available_metrics))]
        angles += angles[:1]  # –ó–∞–º—ã–∫–∞–µ–º –∫—Ä—É–≥
        
        fig, ax = plt.subplots(figsize=(12, 10), subplot_kw=dict(projection='polar'))
        
        colors = plt.cm.Set3(np.linspace(0, 1, len(model_stats)))
        
        for idx, (model, values) in enumerate(model_stats.iterrows()):
            values_list = values.tolist()
            values_list += values_list[:1]  # –ó–∞–º—ã–∫–∞–µ–º –∫—Ä—É–≥
            
            ax.plot(angles, values_list, 'o-', linewidth=2, label=model, color=colors[idx])
            ax.fill(angles, values_list, alpha=0.25, color=colors[idx])
        
        ax.set_xticks(angles[:-1])
        ax.set_xticklabels(available_metrics)
        ax.set_ylim(0, 1)
        ax.set_title('üï∏Ô∏è Radar Comparison of Models by Trust-ADE Metrics',
                     size=14, fontweight='bold', pad=20)
        ax.legend(loc='upper right', bbox_to_anchor=(1.3, 1.0))
        ax.grid(True)
        
        plt.tight_layout()
        plt.savefig(f'{results_dir}/radar_comparison_{timestamp}.png', dpi=300, bbox_inches='tight')
        plt.close()
        
    except Exception as e:
        print(f"    ‚ùå –û—à–∏–±–∫–∞ –≤ create_radar_chart_comparison: {e}")
        plt.close()


def create_performance_matrix(df_viz, results_dir, timestamp):
    """–ú–∞—Ç—Ä–∏—Ü–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏: Trust Score vs Accuracy vs Training Time"""
    
    try:
        plt.figure(figsize=(12, 8))
        
        # –°–æ–∑–¥–∞–µ–º scatter plot
        scatter = plt.scatter(df_viz['Accuracy'], df_viz['Trust_Score'], 
                             s=df_viz['Training_Time'] * 1000,  # –†–∞–∑–º–µ—Ä —Ç–æ—á–∫–∏ = –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è
                             c=[MODEL_COLORS.get(model, MODEL_COLORS['Default']) for model in df_viz['Model']],
                             alpha=0.7, edgecolors='black', linewidth=1)
        
        # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ–¥–ø–∏—Å–∏ –º–æ–¥–µ–ª–µ–π
        for idx, row in df_viz.iterrows():
            plt.annotate(row['Model'], (row['Accuracy'], row['Trust_Score']),
                        xytext=(5, 5), textcoords='offset points', fontsize=8,
                        bbox=dict(boxstyle='round,pad=0.3', facecolor='white', alpha=0.7))
        
        plt.xlabel('Accuracy', fontsize=12, fontweight='bold')
        plt.ylabel('Trust Score', fontsize=12, fontweight='bold')
        plt.title('üéØ Model Performance Matrix\n(point size = training time)',
                  fontsize=14, fontweight='bold')
        plt.grid(True, alpha=0.3)
        
        # –°–æ–∑–¥–∞–µ–º –ª–µ–≥–µ–Ω–¥—É –¥–ª—è —Ä–∞–∑–º–µ—Ä–∞ —Ç–æ—á–µ–∫
        sizes = [0.1, 0.3, 0.5]
        size_labels = ['–ë—ã—Å—Ç—Ä–æ', '–°—Ä–µ–¥–Ω–µ', '–ú–µ–¥–ª–µ–Ω–Ω–æ']
        size_legend = [plt.scatter([], [], s=s*1000, c='gray', alpha=0.7) for s in sizes]
        plt.legend(size_legend, size_labels, title='–í—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è', 
                  loc='upper left', title_fontsize=10)
        
        plt.tight_layout()
        plt.savefig(f'{results_dir}/performance_matrix_{timestamp}.png', dpi=300, bbox_inches='tight')
        plt.close()
        
    except Exception as e:
        print(f"    ‚ùå –û—à–∏–±–∫–∞ –≤ create_performance_matrix: {e}")
        plt.close()


def create_dataset_comparison(df_viz, results_dir, timestamp):
    """–°—Ä–∞–≤–Ω–µ–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–µ–π –ø–æ –¥–∞—Ç–∞—Å–µ—Ç–∞–º"""
    
    try:
        if 'Dataset' not in df_viz.columns:
            return
        
        datasets = df_viz['Dataset'].unique()
        if len(datasets) < 2:
            return
        
        fig, axes = plt.subplots(2, 2, figsize=(16, 12))
        fig.suptitle('üìä Model Comparison by Dataset', fontsize=16, fontweight='bold')
        
        metrics = ['Trust_Score', 'Accuracy', 'Training_Time', 'Explainability']
        metric_titles = ['Trust Score', 'Accuracy', 'Training Time (s)', 'Explainability']
        
        for idx, (metric, title) in enumerate(zip(metrics, metric_titles)):
            if metric not in df_viz.columns:
                continue
                
            ax = axes[idx // 2, idx % 2]
            
            pivot_data = df_viz.pivot_table(values=metric, index='Model', columns='Dataset', aggfunc='mean')
            
            im = ax.imshow(pivot_data.values, aspect='auto', cmap='viridis', interpolation='nearest')
            
            # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –æ—Å–µ–π
            ax.set_xticks(range(len(pivot_data.columns)))
            ax.set_xticklabels(pivot_data.columns, rotation=45, ha='right')
            ax.set_yticks(range(len(pivot_data.index)))
            ax.set_yticklabels(pivot_data.index)
            
            # –î–æ–±–∞–≤–ª—è–µ–º –∑–Ω–∞—á–µ–Ω–∏—è –≤ —è—á–µ–π–∫–∏
            for i in range(len(pivot_data.index)):
                for j in range(len(pivot_data.columns)):
                    value = pivot_data.iloc[i, j]
                    if not np.isnan(value):
                        ax.text(j, i, f'{value:.3f}', ha='center', va='center',
                               color='white' if value < pivot_data.values.mean() else 'black',
                               fontweight='bold', fontsize=8)
            
            ax.set_title(f'üìà {title}', fontweight='bold')
            
            # –¶–≤–µ—Ç–æ–≤–∞—è —à–∫–∞–ª–∞
            cbar = plt.colorbar(im, ax=ax, shrink=0.8)
            cbar.ax.tick_params(labelsize=8)
        
        plt.tight_layout()
        plt.savefig(f'{results_dir}/dataset_comparison_{timestamp}.png', dpi=300, bbox_inches='tight')
        plt.close()
        
    except Exception as e:
        print(f"    ‚ùå –û—à–∏–±–∫–∞ –≤ create_dataset_comparison: {e}")
        plt.close()


def create_metric_distributions(df_viz, results_dir, timestamp):
    """–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è Trust-ADE –º–µ—Ç—Ä–∏–∫"""
    
    try:
        metrics = ['Trust_Score', 'Accuracy', 'Explainability', 'Robustness']
        available_metrics = [m for m in metrics if m in df_viz.columns]
        
        if len(available_metrics) < 2:
            return
        
        fig, axes = plt.subplots(2, 2, figsize=(16, 12))
        fig.suptitle('üìä Trust-ADE Metric Distributions', fontsize=16, fontweight='bold')
        
        for idx, metric in enumerate(available_metrics[:4]):
            ax = axes[idx // 2, idx % 2]
            
            # –ì–∏—Å—Ç–æ–≥—Ä–∞–º–º–∞ —Å kde
            ax.hist(df_viz[metric], bins=20, alpha=0.7, edgecolor='black', 
                   color=plt.cm.viridis(0.7), density=True)
            
            # KDE –∫—Ä–∏–≤–∞—è
            from scipy import stats
            kde = stats.gaussian_kde(df_viz[metric].dropna())
            x_range = np.linspace(df_viz[metric].min(), df_viz[metric].max(), 100)
            ax.plot(x_range, kde(x_range), 'r-', linewidth=2, label='KDE')
            
            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
            mean_val = df_viz[metric].mean()
            std_val = df_viz[metric].std()
            ax.axvline(mean_val, color='red', linestyle='--', linewidth=2, label=f'Mean: {mean_val:.3f}')
            ax.axvline(mean_val + std_val, color='orange', linestyle='--', alpha=0.7, 
                      label=f'Mean + Std: {mean_val + std_val:.3f}')
            ax.axvline(mean_val - std_val, color='orange', linestyle='--', alpha=0.7,
                      label=f'Mean - Std: {mean_val - std_val:.3f}')
            
            ax.set_title(f'üìà Distribution {metric}', fontweight='bold')
            ax.set_xlabel(metric)
            ax.set_ylabel('Density')
            ax.legend(fontsize=8)
            ax.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(f'{results_dir}/metric_distributions_{timestamp}.png', dpi=300, bbox_inches='tight')
        plt.close()
        
    except Exception as e:
        print(f"    ‚ùå –û—à–∏–±–∫–∞ –≤ create_metric_distributions: {e}")
        plt.close()


def create_trust_score_breakdown(df_viz, results_dir, timestamp):
    """–î–µ—Ç–∞–ª—å–Ω–∞—è —Ä–∞–∑–±–∏–≤–∫–∞ Trust Score –ø–æ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞–º"""
    
    try:
        trust_components = ['Explainability', 'Robustness']
        available_components = [c for c in trust_components if c in df_viz.columns]
        
        if len(available_components) < 2:
            return
        
        model_stats = df_viz.groupby('Model')[available_components + ['Trust_Score']].mean()
        
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 8))
        fig.suptitle('üîç Trust Score Detailed Breakdown', fontsize=16, fontweight='bold')
        
        # Stacked bar chart –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
        bottom = np.zeros(len(model_stats))
        colors = plt.cm.Set3(np.linspace(0, 1, len(available_components)))
        
        for idx, component in enumerate(available_components):
            ax1.bar(model_stats.index, model_stats[component], bottom=bottom, 
                   label=component, color=colors[idx], alpha=0.8)
            bottom += model_stats[component]
        
        ax1.set_title('üìä Trust Score Components', fontweight='bold')
        ax1.set_ylabel('Component Value')
        ax1.legend()
        ax1.tick_params(axis='x', rotation=45)
        ax1.grid(True, alpha=0.3)
        
        # Scatter plot: –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã vs –∏—Ç–æ–≥–æ–≤—ã–π Trust Score
        if len(available_components) >= 2:
            scatter = ax2.scatter(model_stats[available_components[0]], 
                                 model_stats[available_components[1]],
                                 s=model_stats['Trust_Score'] * 500,
                                 c=model_stats['Trust_Score'], 
                                 cmap='viridis', alpha=0.7, edgecolors='black')
            
            # –ü–æ–¥–ø–∏—Å–∏ –º–æ–¥–µ–ª–µ–π
            for idx, model in enumerate(model_stats.index):
                ax2.annotate(model, 
                           (model_stats.iloc[idx][available_components[0]], 
                            model_stats.iloc[idx][available_components[1]]),
                           xytext=(5, 5), textcoords='offset points', fontsize=8,
                           bbox=dict(boxstyle='round,pad=0.3', facecolor='white', alpha=0.7))
            
            ax2.set_xlabel(available_components[0])
            ax2.set_ylabel(available_components[1])
            ax2.set_title('üéØ Components vs Trust Score\n(point size = Trust Score)', fontweight='bold')
            ax2.grid(True, alpha=0.3)
            
            # –¶–≤–µ—Ç–æ–≤–∞—è —à–∫–∞–ª–∞
            cbar = plt.colorbar(scatter, ax=ax2)
            cbar.set_label('Trust Score')
        
        plt.tight_layout()
        plt.savefig(f'{results_dir}/trust_score_breakdown_{timestamp}.png', dpi=300, bbox_inches='tight')
        plt.close()
        
    except Exception as e:
        print(f"    ‚ùå –û—à–∏–±–∫–∞ –≤ create_trust_score_breakdown: {e}")
        plt.close()


def create_efficiency_analysis(df_viz, results_dir, timestamp):
    """–ê–Ω–∞–ª–∏–∑ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–µ–π"""
    
    try:
        # –í—ã—á–∏—Å–ª—è–µ–º —Ä–∞–∑–ª–∏—á–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏
        df_viz = df_viz.copy()
        df_viz['Trust_Efficiency'] = df_viz['Trust_Score'] / (df_viz['Training_Time'] + 0.001)
        df_viz['Accuracy_Efficiency'] = df_viz['Accuracy'] / (df_viz['Training_Time'] + 0.001)
        df_viz['Overall_Efficiency'] = (df_viz['Trust_Score'] + df_viz['Accuracy']) / 2 / (df_viz['Training_Time'] + 0.001)
        
        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))
        fig.suptitle('‚ö° Model Efficiency Analysis', fontsize=16, fontweight='bold')
        
        # –ì—Ä–∞—Ñ–∏–∫ 1: Trust Efficiency
        model_eff = df_viz.groupby('Model')['Trust_Efficiency'].mean().sort_values(ascending=False)
        bars1 = ax1.bar(range(len(model_eff)), model_eff.values, 
                       color=plt.cm.viridis(0.7), alpha=0.8, edgecolor='black')
        ax1.set_title('üéØ Trust Efficiency (Trust Score / Time)', fontweight='bold')
        ax1.set_xticks(range(len(model_eff)))
        ax1.set_xticklabels(model_eff.index, rotation=45, ha='right')
        ax1.set_ylabel('Trust Score / Second')
        ax1.grid(True, alpha=0.3)
        
        for bar, val in zip(bars1, model_eff.values):
            ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(model_eff) * 0.01,
                    f'{val:.2f}', ha='center', va='bottom', fontweight='bold')
        
        # –ì—Ä–∞—Ñ–∏–∫ 2: Accuracy Efficiency  
        acc_eff = df_viz.groupby('Model')['Accuracy_Efficiency'].mean().sort_values(ascending=False)
        bars2 = ax2.bar(range(len(acc_eff)), acc_eff.values,
                       color=plt.cm.plasma(0.7), alpha=0.8, edgecolor='black')
        ax2.set_title('üìä Accuracy Efficiency (Accuracy / Time)', fontweight='bold')
        ax2.set_xticks(range(len(acc_eff)))
        ax2.set_xticklabels(acc_eff.index, rotation=45, ha='right')
        ax2.set_ylabel('Accuracy / Second')
        ax2.grid(True, alpha=0.3)
        
        for bar, val in zip(bars2, acc_eff.values):
            ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(acc_eff) * 0.01,
                    f'{val:.2f}', ha='center', va='bottom', fontweight='bold')
        
        # –ì—Ä–∞—Ñ–∏–∫ 3: Overall Efficiency
        overall_eff = df_viz.groupby('Model')['Overall_Efficiency'].mean().sort_values(ascending=False)
        bars3 = ax3.bar(range(len(overall_eff)), overall_eff.values,
                       color=plt.cm.cividis(0.7), alpha=0.8, edgecolor='black')
        ax3.set_title('‚öñÔ∏è Overall Efficiency', fontweight='bold')
        ax3.set_xticks(range(len(overall_eff)))
        ax3.set_xticklabels(overall_eff.index, rotation=45, ha='right')
        ax3.set_ylabel('Overall Score / Second')
        ax3.grid(True, alpha=0.3)
        
        for bar, val in zip(bars3, overall_eff.values):
            ax3.text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(overall_eff) * 0.01,
                    f'{val:.2f}', ha='center', va='bottom', fontweight='bold')
        
        # –ì—Ä–∞—Ñ–∏–∫ 4: Scatter - Quality vs Speed
        quality_score = (df_viz['Trust_Score'] + df_viz['Accuracy']) / 2
        ax4.scatter(df_viz['Training_Time'], quality_score, 
                   s=100, alpha=0.7, edgecolors='black')
        
        for idx, row in df_viz.iterrows():
            ax4.annotate(row['Model'], (row['Training_Time'], quality_score.iloc[idx]),
                        xytext=(5, 5), textcoords='offset points', fontsize=8,
                        bbox=dict(boxstyle='round,pad=0.3', facecolor='white', alpha=0.7))
        
        ax4.set_xlabel('Training Time (seconds)')
        ax4.set_ylabel('Quality Score (Trust + Accuracy)/2')
        ax4.set_title('üéØ Quality vs Speed Trade-off', fontweight='bold')
        ax4.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(f'{results_dir}/efficiency_analysis_{timestamp}.png', dpi=300, bbox_inches='tight')
        plt.close()
        
    except Exception as e:
        print(f"    ‚ùå –û—à–∏–±–∫–∞ –≤ create_efficiency_analysis: {e}")
        plt.close()


def create_model_ranking_chart(df_viz, results_dir, timestamp):
    """–†–µ–π—Ç–∏–Ω–≥ –º–æ–¥–µ–ª–µ–π –ø–æ —Ä–∞–∑–ª–∏—á–Ω—ã–º –∫—Ä–∏—Ç–µ—Ä–∏—è–º"""
    
    try:
        # –°–æ–∑–¥–∞–µ–º —Ä–µ–π—Ç–∏–Ω–≥–∏ –ø–æ —Ä–∞–∑–Ω—ã–º –º–µ—Ç—Ä–∏–∫–∞–º
        metrics_for_ranking = ['Trust_Score', 'Accuracy', 'Explainability', 'Robustness']
        available_metrics = [m for m in metrics_for_ranking if m in df_viz.columns]
        
        if len(available_metrics) < 2:
            return
        
        model_stats = df_viz.groupby('Model')[available_metrics].mean()
        
        # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º —Ä–∞–Ω–≥–∏ (1 = –ª—É—á—à–∏–π)
        rankings = model_stats.rank(ascending=False, method='min')
        
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 8))
        fig.suptitle('üèÜ Model Ranking by Trust-ADE protocol', fontsize=16, fontweight='bold')
        
        # –¢–µ–ø–ª–æ–≤–∞—è –∫–∞—Ä—Ç–∞ —Ä–∞–Ω–≥–æ–≤
        im = ax1.imshow(rankings.T, aspect='auto', cmap='RdYlGn_r', interpolation='nearest')
        
        ax1.set_xticks(range(len(rankings.index)))
        ax1.set_xticklabels(rankings.index, rotation=45, ha='right')
        ax1.set_yticks(range(len(available_metrics)))
        ax1.set_yticklabels(available_metrics)
        
        # –î–æ–±–∞–≤–ª—è–µ–º —Ä–∞–Ω–≥–∏ –≤ —è—á–µ–π–∫–∏
        for i in range(len(available_metrics)):
            for j in range(len(rankings.index)):
                rank = int(rankings.iloc[j, i])
                ax1.text(j, i, f'{rank}', ha='center', va='center',
                        color='white' if rank > len(rankings.index)/2 else 'black',
                        fontweight='bold', fontsize=12)
        
        ax1.set_title('üìä Rank Matrix\n(1 = best)', fontweight='bold')
        
        # –¶–≤–µ—Ç–æ–≤–∞—è —à–∫–∞–ª–∞
        cbar1 = plt.colorbar(im, ax=ax1, shrink=0.8)
        cbar1.set_label('Rank')
        
        # –û–±—â–∏–π —Ä–µ–π—Ç–∏–Ω–≥ (—Å—Ä–µ–¥–Ω–∏–π —Ä–∞–Ω–≥)
        overall_ranking = rankings.mean(axis=1).sort_values()
        
        colors = ['#FFD700', '#C0C0C0', '#CD7F32']  # –ó–æ–ª–æ—Ç–æ, —Å–µ—Ä–µ–±—Ä–æ, –±—Ä–æ–Ω–∑–∞
        colors.extend(plt.cm.Set3(np.linspace(0, 1, max(0, len(overall_ranking) - 3))))
        
        bars = ax2.barh(range(len(overall_ranking)), overall_ranking.values,
                       color=colors[:len(overall_ranking)], alpha=0.8, edgecolor='black')
        
        ax2.set_yticks(range(len(overall_ranking)))
        ax2.set_yticklabels(overall_ranking.index)
        ax2.set_xlabel('Average Rank')
        ax2.set_title('ü•á Overall Model Ranking\n(lower = better)', fontweight='bold')
        ax2.grid(True, alpha=0.3, axis='x')
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∑–Ω–∞—á–µ–Ω–∏—è —Ä–∞–Ω–≥–æ–≤
        for bar, rank in zip(bars, overall_ranking.values):
            ax2.text(rank + max(overall_ranking) * 0.01, bar.get_y() + bar.get_height()/2,
                    f'{rank:.1f}', ha='left', va='center', fontweight='bold')
        
        # –î–æ–±–∞–≤–ª—è–µ–º –º–µ–¥–∞–ª–∏ –¥–ª—è —Ç–æ–ø-3
        medals = ['ü•á', 'ü•à', 'ü•â']
        for i, (model, rank) in enumerate(overall_ranking.head(3).items()):
            ax2.text(0.02, i, medals[i], transform=ax2.transData, fontsize=16)
        
        plt.tight_layout()
        plt.savefig(f'{results_dir}/model_ranking_{timestamp}.png', dpi=300, bbox_inches='tight')
        plt.close()
        
    except Exception as e:
        print(f"    ‚ùå –û—à–∏–±–∫–∞ –≤ create_model_ranking_chart: {e}")
        plt.close()


def create_detailed_heatmap(df_viz, results_dir, timestamp):
    """–î–µ—Ç–∞–ª—å–Ω–∞—è —Ç–µ–ø–ª–æ–≤–∞—è –∫–∞—Ä—Ç–∞ –≤—Å–µ—Ö –º–µ—Ç—Ä–∏–∫ –ø–æ –º–æ–¥–µ–ª—è–º"""
    
    try:
        # –í—ã–±–∏—Ä–∞–µ–º –≤—Å–µ —á–∏—Å–ª–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏
        numeric_columns = ['Trust_Score', 'Accuracy', 'Explainability', 'Robustness',
                          'Bias_Shift', 'Concept_Drift', 'Training_Time']
        
        available_columns = [col for col in numeric_columns if col in df_viz.columns]
        
        if len(available_columns) < 3:
            return
        
        # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ –º–æ–¥–µ–ª—è–º –∏ –Ω–æ—Ä–º–∞–ª–∏–∑—É–µ–º –¥–∞–Ω–Ω—ã–µ
        model_stats = df_viz.groupby('Model')[available_columns].mean()
        
        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –¥–ª—è –ª—É—á—à–µ–π –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏
        normalized_stats = model_stats.copy()
        for col in available_columns:
            if col not in ['Bias_Shift', 'Concept_Drift']:  # –î–ª—è —ç—Ç–∏—Ö –º–µ—Ç—Ä–∏–∫ –º–µ–Ω—å—à–µ = –ª—É—á—à–µ
                normalized_stats[col] = (model_stats[col] - model_stats[col].min()) / (model_stats[col].max() - model_stats[col].min())
            else:
                normalized_stats[col] = 1 - (model_stats[col] - model_stats[col].min()) / (model_stats[col].max() - model_stats[col].min())
        
        plt.figure(figsize=(14, 10))
        
        # –°–æ–∑–¥–∞–µ–º —Ç–µ–ø–ª–æ–≤—É—é –∫–∞—Ä—Ç—É
        im = plt.imshow(normalized_stats.values, aspect='auto', cmap='RdYlGn', interpolation='nearest')
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –æ—Å–µ–π
        plt.xticks(range(len(available_columns)), available_columns, rotation=45, ha='right')
        plt.yticks(range(len(model_stats.index)), model_stats.index)
        
        # –î–æ–±–∞–≤–ª—è–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –≤ —è—á–µ–π–∫–∏
        for i in range(len(model_stats.index)):
            for j in range(len(available_columns)):
                original_value = model_stats.iloc[i, j]
                color = 'white' if normalized_stats.iloc[i, j] < 0.5 else 'black'
                
                if available_columns[j] == 'Training_Time':
                    text = f'{original_value:.2f}s'
                else:
                    text = f'{original_value:.3f}'
                    
                plt.text(j, i, text, ha='center', va='center',
                        color=color, fontweight='bold', fontsize=9)
        
        plt.title('üå°Ô∏è Detailed Heatmap of Trust-ADE Metrics\n(green = better, red = worse)',
                 fontsize=14, fontweight='bold', pad=20)
        plt.xlabel('Metrics', fontweight='bold')
        plt.ylabel('Models', fontweight='bold')
        
        # –¶–≤–µ—Ç–æ–≤–∞—è —à–∫–∞–ª–∞
        cbar = plt.colorbar(im, shrink=0.8)
        cbar.set_label('Normalized Value (0-1)')
        
        plt.tight_layout()
        plt.savefig(f'{results_dir}/detailed_heatmap_{timestamp}.png', dpi=300, bbox_inches='tight')
        plt.close()
        
    except Exception as e:
        print(f"    ‚ùå –û—à–∏–±–∫–∞ –≤ create_detailed_heatmap: {e}")
        plt.close()


# –û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –≤—Å–µ—Ö –≥—Ä–∞—Ñ–∏–∫–æ–≤
def create_fixed_visualizations(df_viz, results_dir, timestamp):
    """–û–±–µ—Ä—Ç–∫–∞ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–π –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ (—Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º –∫–æ–¥–æ–º)"""
    create_extended_visualizations(df_viz, results_dir, timestamp)

