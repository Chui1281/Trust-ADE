"""
CUDA-–æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏
"""
import torch
import torch.nn as nn
import numpy as np
from sklearn.preprocessing import StandardScaler
from config.settings import DEVICE, CUDA_EFFICIENT_THRESHOLD


class OptimizedCUDAMLPClassifier:
    """–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è PyTorch MLP —Å –∞–¥–∞–ø—Ç–∏–≤–Ω—ã–º CUDA –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º"""

    def __init__(self, hidden_layers=(100, 50), n_classes=2, learning_rate=0.001,
                 epochs=300, device='cuda', random_state=42, dataset_size=0):
        self.hidden_layers = hidden_layers
        self.n_classes = n_classes
        self.learning_rate = learning_rate
        self.epochs = epochs
        self.random_state = random_state
        self.model = None
        self.scaler = StandardScaler()

        # –ê–¥–∞–ø—Ç–∏–≤–Ω—ã–π –≤—ã–±–æ—Ä —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–∞–∑–º–µ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö
        if dataset_size < CUDA_EFFICIENT_THRESHOLD:
            self.device = 'cpu'
            self.use_cuda = False
            print(f"      üì± –ò—Å–ø–æ–ª—å–∑—É–µ–º CPU (–¥–∞—Ç–∞—Å–µ—Ç –º–∞–ª: {dataset_size} < {CUDA_EFFICIENT_THRESHOLD})")
        else:
            self.device = device if torch.cuda.is_available() else 'cpu'
            self.use_cuda = self.device == 'cuda'
            print(f"      üöÄ –ò—Å–ø–æ–ª—å–∑—É–µ–º CUDA (–¥–∞—Ç–∞—Å–µ—Ç –±–æ–ª—å—à–æ–π: {dataset_size})")

        # –ê–¥–∞–ø—Ç–∏–≤–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ä–∞–∑–º–µ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö
        if dataset_size < 200:
            self.epochs = 150
            self.batch_size = min(16, dataset_size // 4)
        elif dataset_size < 500:
            self.epochs = 200
            self.batch_size = 32
        else:
            self.epochs = 300
            self.batch_size = 64

        # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º seed
        torch.manual_seed(random_state)
        if torch.cuda.is_available():
            torch.cuda.manual_seed(random_state)

    def _create_model(self, input_size):
        """–°–æ–∑–¥–∞–Ω–∏–µ –∞–¥–∞–ø—Ç–∏–≤–Ω–æ–π PyTorch –º–æ–¥–µ–ª–∏"""
        layers = []

        # –ê–¥–∞–ø—Ç–∏–≤–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞
        if input_size < 10:
            hidden_sizes = [max(8, input_size * 2), max(4, input_size)]
        elif input_size < 50:
            hidden_sizes = self.hidden_layers
        else:
            hidden_sizes = (min(512, input_size * 2), 256, 128)

        # –í—Ö–æ–¥–Ω–æ–π —Å–ª–æ–π
        layers.append(nn.Linear(input_size, hidden_sizes[0]))
        layers.append(nn.ReLU())
        layers.append(nn.Dropout(0.1))

        # –°–∫—Ä—ã—Ç—ã–µ —Å–ª–æ–∏
        for i in range(len(hidden_sizes) - 1):
            layers.append(nn.Linear(hidden_sizes[i], hidden_sizes[i + 1]))
            layers.append(nn.ReLU())
            layers.append(nn.Dropout(0.1))

        # –í—ã—Ö–æ–¥–Ω–æ–π —Å–ª–æ–π
        layers.append(nn.Linear(hidden_sizes[-1], self.n_classes))

        return nn.Sequential(*layers)

    def fit(self, X, y):
        """–û–±—É—á–µ–Ω–∏–µ —Å –∞–¥–∞–ø—Ç–∏–≤–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏"""
        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö
        X_scaled = self.scaler.fit_transform(X)

        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Å–æ–≤
        self.n_classes = len(np.unique(y))

        # –°–æ–∑–¥–∞–µ–º –º–æ–¥–µ–ª—å
        self.model = self._create_model(X_scaled.shape[1]).to(self.device)

        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ —Ç–µ–Ω–∑–æ—Ä—ã
        X_tensor = torch.FloatTensor(X_scaled).to(self.device)
        y_tensor = torch.LongTensor(y).to(self.device)

        # –ê–¥–∞–ø—Ç–∏–≤–Ω—ã–π –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä –∏ learning rate
        if X.shape[0] < 200:
            optimizer = torch.optim.Adam(self.model.parameters(), lr=self.learning_rate * 2)
        else:
            optimizer = torch.optim.Adam(self.model.parameters(), lr=self.learning_rate)

        criterion = nn.CrossEntropyLoss()

        # –û–±—É—á–µ–Ω–∏–µ —Å –∞–¥–∞–ø—Ç–∏–≤–Ω—ã–º –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ–º
        self.model.train()
        log_interval = max(25, self.epochs // 8)

        for epoch in range(self.epochs):
            optimizer.zero_grad()
            outputs = self.model(X_tensor)
            loss = criterion(outputs, y_tensor)
            loss.backward()
            optimizer.step()

            if epoch % log_interval == 0:
                print(f"      Epoch {epoch}/{self.epochs}, Loss: {loss.item():.4f}")

        return self

    def predict(self, X):
        """–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤"""
        self.model.eval()
        X_scaled = self.scaler.transform(X)
        X_tensor = torch.FloatTensor(X_scaled).to(self.device)

        with torch.no_grad():
            outputs = self.model(X_tensor)
            _, predicted = torch.max(outputs.data, 1)

        return predicted.cpu().numpy()

    def predict_proba(self, X):
        """–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π"""
        self.model.eval()
        X_scaled = self.scaler.transform(X)
        X_tensor = torch.FloatTensor(X_scaled).to(self.device)

        with torch.no_grad():
            outputs = self.model(X_tensor)
            probabilities = torch.softmax(outputs, dim=1)

        return probabilities.cpu().numpy()

