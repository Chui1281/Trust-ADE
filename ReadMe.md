# ğŸš€ Trust-ADE: Dynamic Trust Assessment Protocol for AI Systems

**Trust-ADE (Trust Assessment through Dynamic Explainability)** is a comprehensive protocol for quantitative trust assessment of artificial intelligence systems, based on the scientific research "From Correlations to Causality: XAI 2.0 Approach and Trust-ADE Protocol for Trustworthy AI".

## ğŸ“Š Trust-ADE Protocol

### ğŸ¯ Core Formula

The protocol aggregates three key dimensions into a unified trust metric:

```
Trust_ADE = w_E Ã— ES + w_R Ã— (RI Ã— e^(-Î³ Ã— CDR)) + w_F Ã— (1 - BSI)
```

**Where:**

- **ES** - Explainability Score
- **RI** - Robustness Index
- **CDR** - Concept-Drift Rate
- **BSI** - Bias Shift Index
- **w_E, w_R, w_F** - domain-specific weights
- **Î³** - concept drift sensitivity parameter


### ğŸ” Protocol Components

#### 1. Explainability Score (ES)

Evaluates explanation quality through four dimensions:

```
ES = w_c Ã— F_c + w_s Ã— C_s + w_i Ã— S_i + w_h Ã— U_h
```

- **F_c** - Causal Fidelity:

```
F_c = |E_sys âˆ© E_exp|/|E_exp| Ã— Î± + |E_sys âˆ© E_exp|/|E_sys| Ã— (1-Î±)
```

- **C_s** - Semantic Coherence:

```
C_s = 1 - H(E)/H_max
```

- **S_i** - Interpretation Stability:

```
S_i = 1 - (1/N) Ã— Î£ d(E_i, E_i^Îµ)
```

- **U_h** - Human Comprehensibility (expert assessment)


#### 2. Robustness Index (RI)

Integrates robustness to various types of perturbations:

```
RI = w_a Ã— R_a + w_n Ã— R_n + w_e Ã— R_e
```

- **R_a** - Adversarial robustness
- **R_n** - Noise robustness
- **R_e** - Explanation robustness


#### 3. Concept-Drift Rate (CDR)

Measures the rate of conceptual dependency changes:

```
CDR = Î» Ã— KS(P_t, P_t-Î”t) + (1-Î») Ã— JS(P_t, P_t-Î”t)
```

- **KS** - Kolmogorov-Smirnov statistic
- **JS** - Jensen-Shannon divergence


#### 4. Bias Shift Index (BSI)

Tracks bias dynamics:

```
BSI = âˆš(w_dp Ã— DP_Î”Â² + w_eo Ã— EO_Î”Â² + w_cf Ã— CF_Î”Â²)
```

- **DP_Î”** - demographic parity change
- **EO_Î”** - equalized odds change
- **CF_Î”** - calibrated fairness change


## ğŸ—ï¸ Project Architecture

```
trust_ade/
â”œâ”€â”€ ğŸ“ trust_ade/              # Core protocol modules
â”‚   â”œâ”€â”€ trust_ade.py           # Main TrustADE class  
â”‚   â”œâ”€â”€ trust_calculator.py    # Final metric calculation
â”‚   â”œâ”€â”€ explainability_score.py # Explainability assessment module
â”‚   â”œâ”€â”€ robustness_index.py    # Robustness analysis
â”‚   â”œâ”€â”€ bias_shift_index.py    # Bias detection
â”‚   â”œâ”€â”€ concept_drift.py       # Concept drift monitoring
â”‚   â”œâ”€â”€ base_model.py          # Base model interface
â”‚   â””â”€â”€ utils.py              # Helper utilities
|
â”œâ”€â”€ ğŸ“ config/                    # Configuration and settings
â”‚   â””â”€â”€ settings.py               # Global settings, CUDA config
â”‚
â”œâ”€â”€ ğŸ“ models/                 # ML model integrations
â”‚   â”œâ”€â”€ sklearn_wrapper.py     # Scikit-learn wrapper
|   â”œâ”€â”€ wrappers.py              # Base wrappers
â”‚   |â”€â”€ cuda_models.py           # CUDA-optimized models
â”‚   â””â”€â”€ __init__.py
â”‚
â”œâ”€â”€ ğŸ“ explainers/             # Explainability modules
â”‚   â”œâ”€â”€ shap_explainer.py     # SHAP integration
â”‚   â””â”€â”€ __init__.py
â”‚
â”œâ”€â”€ ğŸ“ data/                     # Data handling
â”‚   â””â”€â”€ datasets.py              # Dataset loading and preparation
â”‚
â”œâ”€â”€ ğŸ“ training/                 # Model training
â”‚   â””â”€â”€ trainers.py              # Training for all model types
â”‚
â”œâ”€â”€ ğŸ“ evaluation/               # Evaluation and Trust-ADE
â”‚   â””â”€â”€ trust_evaluator.py      # Trust-ADE assessment protocol
â”‚
â”œâ”€â”€ ğŸ“ visualization/            # Results visualization
â”‚   â””â”€â”€ charts.py                # Chart and report generation
â”‚
â”œâ”€â”€ ğŸ“ utils/                    # Utilities
â”‚   â””â”€â”€ io_utils.py              # Save/load results
â”‚
â”œâ”€â”€ ğŸ“ analysis/                 # Results analysis
â”‚   â””â”€â”€ results.py               # Final analysis and comparison
â”‚
â”œâ”€â”€ ğŸ“ cli/                      # Command line interface
â”‚   â””â”€â”€ dataset_selector.py     # CLI for dataset selection
â”‚
â”œâ”€â”€ ğŸ“„ main.py                   # Main execution script
â”‚
â”œâ”€â”€ ğŸ“ tests/                  # Tests
â”‚   â”œâ”€â”€ test_basic.py         # Basic tests
|   â”œâ”€â”€ demo_trust_ade.py      # Basic demonstration
â”‚   â””â”€â”€ test_installation.py  # Installation verification
â”‚
â””â”€â”€ ğŸ“ results/                # Analysis results
```


## ğŸ“¦ Installation

### System Requirements

```bash
Python >= 3.8
NumPy >= 1.21.0
Pandas >= 1.3.0
Scikit-learn >= 1.0.0
```


### Quick Installation

```bash
git clone https://github.com/your-org/trust-ade.git
cd trust-ade
pip install -r requirements.txt
python setup.py install
```


### Dependencies

```txt
numpy>=1.21.0
pandas>=1.3.0
scikit-learn>=1.0.0
matplotlib>=3.5.0
seaborn>=0.11.0
shap>=0.41.0
scipy>=1.7.0
tqdm>=4.62.0
```


## ğŸš€ Quick Start

### 1. Run on All Datasets

```bash
python main.py
```


### 2. Selective Execution

```bash
# Specific datasets only
python main.py --datasets iris breast_cancer

# Exclude large datasets
python main.py --exclude digits_binary

# Quick testing
python main.py --datasets iris wine --quick

# CUDA models only
python main.py --cuda-only

# Verbose output
python main.py --datasets breast_cancer --verbose
```


### 3. Command Help

```bash
python main.py --help
```


## ğŸ¯ Key Features

### âœ… Supported Models

- **XANFIS** - Adaptive Neuro-Fuzzy Inference System (ğŸ† **Best Trust Score**)
- **MLP Neural Network (CPU)** - Multi-layer perceptron (sklearn)
- **Support Vector Machine** - Support vector method
- **Gradient Boosting** - Gradient boosting ensemble
- **Random Forest** - Decision tree ensemble
- **MLP Neural Network (CUDA)** - Optimized PyTorch model with GPU


### ğŸ“Š Supported Datasets

- **Iris** - Iris classification (3 classes, 4 features)
- **Breast Cancer** - Breast cancer diagnosis (2 classes, 30 features)
- **Wine** - Wine classification (3 classes, 13 features)
- **Digits Binary** - Digit 0 recognition (2 classes, 64 pixels)


### ğŸ”¬ Trust-ADE Metrics

- **Trust Score** - Final trust assessment (0-1)
- **Explainability Score** - Explanation quality
- **Robustness Index** - Perturbation resistance
- **Bias Shift Index** - Bias index
- **Concept Drift Rate** - Concept drift rate


## ğŸ“ˆ Real Performance Results

### Benchmark Results (4 Datasets)

Based on comprehensive testing across iris, breast_cancer, wine, and digits_binary datasets:

```bash
ğŸ¯ OVERALL MODEL RANKING (average Trust Score):
  ğŸ¥‡ XANFIS: 0.842 Â± 0.078 (on 4 datasets) ğŸ’»
  ğŸ¥ˆ MLP Neural Network (CPU): 0.584 Â± 0.046 (on 4 datasets) ğŸ’»
  ğŸ¥‰ Support Vector Machine: 0.562 Â± 0.050 (on 4 datasets) ğŸ’»
  4ï¸âƒ£ Gradient Boosting: 0.532 Â± 0.099 (on 4 datasets) ğŸ’»
  5ï¸âƒ£ Random Forest: 0.517 Â± 0.094 (on 4 datasets) ğŸ’»
  6ï¸âƒ£ MLP Neural Network (CUDA): 0.452 Â± 0.099 (on 4 datasets) ğŸ’»
```


### Key Findings

- **ğŸ† XANFIS** achieves the highest Trust Score (0.842) due to superior explainability through rule-based reasoning
- **ğŸ“Š Explainability vs Accuracy Trade-off**: Higher accuracy doesn't always mean higher trust
- **ğŸ” CUDA Performance**: Interestingly, CUDA acceleration showed lower trust scores, indicating potential optimization areas
- **ğŸ“ˆ Consistency**: XANFIS shows good stability across different domains (Â±0.078 std deviation)


### Generated Files

```
results/
â”œâ”€â”€ detailed_comparison_cuda_20250823_140323.csv    # Detailed results
â”œâ”€â”€ summary_comparison_cuda_20250823_140323.csv     # Brief summary
â”œâ”€â”€ full_results_cuda_20250823_140323.json         # Complete data
â”œâ”€â”€ fixed_main_comparison_20250823_140323.png       # Main chart
â”œâ”€â”€ trust_metrics_analysis_fixed_20250823_140323.png # Metrics analysis
â”œâ”€â”€ cuda_performance_detailed_20250823_140323.png   # CUDA vs CPU
â””â”€â”€ correlation_analysis_fixed_20250823_140323.png  # Correlations
```


## ğŸ“Š Explainability Maturity Scale L0-L6

| Level | Description | Trust-ADE Support | XANFIS Achievement |
| :-- | :-- | :-- | :-- |
| **L0** | Complete opacity | âŒ | âŒ |
| **L1** | Basic post-hoc explanations | âœ… LIME, SHAP | âŒ |
| **L2** | Enhanced post-hoc + validation | âœ… | âŒ |
| **L3** | Partial architectural transparency | âœ… | âŒ |
| **L4** | Global interpretability | âœ… **Trust-ADE L4** | âœ… **XANFIS L4-L5** |
| **L5** | Context-adaptive explanations | âœ… **Trust-ADE L5** | âœ… **XANFIS L4-L5** |
| **L6** | Autonomous self-explaining systems | ğŸš§ In development | ğŸš§ Future work |

## ğŸ› ï¸ Programming API

### Basic Usage

```python
from data.datasets import prepare_datasets, create_models_config
from training.trainers import train_models
from evaluation.trust_evaluator import enhanced_trust_ade_evaluation
from sklearn.model_selection import train_test_split

# Data preparation
datasets = prepare_datasets()
models_config = create_models_config()

# Dataset selection
dataset_name = 'breast_cancer'
dataset_info = datasets[dataset_name]
X, y = dataset_info['X'], dataset_info['y']

# Data splitting
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=42, stratify=y
)

# Model training
trained_models = train_models(
    X_train, X_test, y_train, y_test,
    dataset_info['feature_names'], models_config,
    dataset_info['type'], dataset_name
)

# Trust-ADE evaluation
enhanced_trust_ade_evaluation(
    trained_models, X_test, y_test, 
    dataset_info['domain'], X_train
)

# Results available in trained_models[model_name]['trust_results']
```


### Custom Model

```python
from models.wrappers import SklearnWrapper
from sklearn.ensemble import ExtraTreesClassifier

# Create custom model
custom_model = ExtraTreesClassifier(n_estimators=150, random_state=42)
custom_model.fit(X_train, y_train)

# Wrapper for Trust-ADE
wrapped_model = SklearnWrapper(
    model=custom_model,
    feature_names=[f"feature_{i}" for i in range(X_train.shape[1])]
)

# Add to trained_models for evaluation
trained_models['Custom Extra Trees'] = {
    'wrapped_model': wrapped_model,
    'scaler': None,
    'training_time': 1.23,
    'accuracy': 0.95,
    'needs_scaling': False,
    'description': 'Extra Trees Classifier',
    'color': '#FF6B6B',
    'use_cuda': False
}
```


### Working with CUDA Models

```python
from models.cuda_models import OptimizedCUDAMLPClassifier
from models.wrappers import CUDAMLPWrapper

# Create CUDA model
cuda_model = OptimizedCUDAMLPClassifier(
    hidden_layers=(128, 64, 32),
    n_classes=len(np.unique(y_train)),
    learning_rate=0.001,
    epochs=200,
    dataset_size=len(X_train)
)

# Training
cuda_model.fit(X_train, y_train)

# Wrapper
wrapped_cuda = CUDAMLPWrapper(cuda_model, feature_names)
```


## âš™ï¸ Configuration

### CUDA Setup

```python
# config/settings.py
CUDA_AVAILABLE = torch.cuda.is_available()
DEVICE = torch.device('cuda' if CUDA_AVAILABLE else 'cpu')
CUDA_EFFICIENT_THRESHOLD = 500  # Minimum size for CUDA
```


### Domain Configurations

```python
# Weights for different domains
DOMAIN_CONFIGS = {
    'medical': {'w_E': 0.5, 'w_R': 0.3, 'w_F': 0.2},
    'financial': {'w_E': 0.3, 'w_R': 0.4, 'w_F': 0.3},
    'general': {'w_E': 0.4, 'w_R': 0.3, 'w_F': 0.3}
}
```


## ğŸ“Š Visualization

The system automatically generates 12+ types of professional charts:

1. **Main Comparison** - Trust Score vs Accuracy
2. **Detailed Metrics Analysis** - All Trust-ADE components
3. **CUDA vs CPU Comparison** - Performance and quality analysis
4. **Correlation Analysis** - Relationships between metrics
5. **Domain-specific Analysis** - Per-dataset breakdowns
6. **Model Performance Heatmaps** - Comprehensive metric overview

### Custom Visualization

```python
from visualization.charts import create_fixed_visualizations
import pandas as pd

# Prepare data for visualization
viz_data = []
for dataset_name, results in all_results.items():
    for model_name, model_info in results['models'].items():
        trust_results = model_info.get('trust_results', {})
        viz_data.append({
            'Dataset': dataset_name,
            'Model': model_name,
            'Trust_Score': trust_results.get('trust_score', 0),
            'Accuracy': model_info.get('accuracy', 0),
            'CUDA': model_info.get('use_cuda', False)
        })

df_viz = pd.DataFrame(viz_data)
create_fixed_visualizations(df_viz, 'results', '20250823_custom')
```


## ğŸ§ª Testing

```bash
# Installation check
python tests/test_installation.py

# Basic tests
python tests/test_basic.py

# Test specific module
python -c "from training.trainers import train_models; print('âœ… Trainers OK')"
```


## ğŸ”§ System Extension

### Adding New Dataset

```python
# data/datasets.py
def prepare_datasets():
    datasets = {}
    
    # Your custom dataset
    datasets['custom_dataset'] = {
        'X': your_X_data,
        'y': your_y_data,
        'feature_names': your_feature_names,
        'target_names': your_target_names,
        'description': 'Dataset description',
        'domain': 'your_domain',
        'type': 'binary'  # or 'multiclass'
    }
    
    return datasets
```


### New Model Type

```python
# models/your_model.py
from models.wrappers import SklearnWrapper

class YourModelWrapper(SklearnWrapper):
    def __init__(self, model, feature_names=None):
        super().__init__(model, feature_names)
    
    def predict(self, X):
        return self.model.predict(X)
    
    def predict_proba(self, X):
        return self.model.predict_proba(X)
```


## ğŸš€ Usage Examples

### Medical Diagnosis

```bash
python main.py --datasets breast_cancer --verbose
```


### Financial Analysis

```bash
python main.py --datasets wine --cuda-only
```


### Quick Comparison

```bash
python main.py --datasets iris --quick
```


## ğŸ“„ Execution Logs

Example detailed log:

```
ğŸ”¬ ADVANCED ML MODEL COMPARISON WITH TRUST-ADE PROTOCOL + CUDA
================================================================================
ğŸ“Š TESTING ON DATASET: BREAST_CANCER
ğŸ“‹ Description: Breast cancer diagnosis (2 classes, 30 features)
ğŸ·ï¸ Domain: medical
ğŸ¯ Task type: binary
================================================================================

  ğŸ“ˆ Training XANFIS...
    âœ… XANFIS trained in 31.28 sec, accuracy: 0.550
    ğŸ§  Rules extracted: 19
    ğŸ“Š Explainability: Full rule and feature importance support

ğŸ” Enhanced Trust-ADE model evaluation...
  ğŸ“Š Evaluating XANFIS...
    ğŸ¯ Trust Score: 0.863
    ğŸ“Š Trust Level: High Trust
    ğŸ“ˆ Metrics: Bias=0.253, Drift=0.031
```


## ğŸ”¬ Scientific Foundation

The Trust-ADE protocol is based on research:

- **Causal interpretability** instead of correlations
- **Dynamic monitoring** of explanation quality
- **Integrated assessment** of explainability, robustness and fairness
- **Standards compliance** ISO/IEC 24029 and EU AI Act


### Why XANFIS Achieves Higher Trust Scores

1. **ğŸ§  Rule-Based Explainability**: Unlike black-box models, XANFIS provides explicit IF-THEN rules
2. **ğŸ” Causal Reasoning**: Rules represent causal relationships rather than correlations
3. **ğŸ“Š Feature Coverage**: Complete feature space coverage with interpretable membership functions
4. **âš–ï¸ Explanation Stability**: Rule-based explanations are inherently more stable than post-hoc methods
5. **ğŸ¯ Human Comprehensibility**: Rules can be directly understood by domain experts

### Component Formulas

**Explainability Score:**

```
ES = w_c Ã— F_c + w_s Ã— C_s + w_i Ã— S_i + w_h Ã— U_h

where:
F_c = |E_sys âˆ© E_exp|/|E_exp| Ã— Î± + |E_sys âˆ© E_exp|/|E_sys| Ã— (1-Î±)
C_s = 1 - H(E)/H_max
S_i = 1 - (1/N) Ã— Î£ d(E_i, E_i^Îµ)
```

**Robustness Index:**

```
RI = w_a Ã— R_a + w_n Ã— R_n + w_e Ã— R_e

where:
R_a = 1 - (1/|A|) Ã— Î£ I[f(x + a) â‰  f(x)]
R_n = E[similarity(f(x), f(x + Îµ))]
R_e = E[similarity(E(x), E(x + Îµ))]
```

**Concept-Drift Rate:**

```
CDR = Î» Ã— KS(P_t, P_t-Î”t) + (1-Î») Ã— JS(P_t, P_t-Î”t)
```

**Bias Shift Index:**

```
BSI = âˆš(w_dp Ã— DP_Î”Â² + w_eo Ã— EO_Î”Â² + w_cf Ã— CF_Î”Â²)
```


## ğŸ“„ Citation

If you use Trust-ADE in your research, please cite:

```bibtex
@article{trofimov2025trust_ade,
  title={From Correlations to Causality: XAI 2.0 Approach and Trust-ADE Protocol for Trustworthy AI},
  author={Trofimov, Yu.V. and Averkin, A.N. and Ilyin, A.S. and Lebedev, A.D.},
  journal={Journal of Explainable AI},
  year={2025}
}
```


## ğŸ¤ Contributing

1. Fork the repository
2. Create feature branch (`git checkout -b feature/amazing-feature`)
3. Commit changes (`git commit -m 'Add amazing feature'`)
4. Push to branch (`git push origin feature/amazing-feature`)
5. Create Pull Request

## ğŸ“„ License

MIT License - free use for research and commercial projects.

***

**Trust-ADE** â€” Your reliable tool for creating trustworthy artificial intelligence systems based on a scientifically grounded protocol for dynamic assessment of explainability, robustness and fairness!

