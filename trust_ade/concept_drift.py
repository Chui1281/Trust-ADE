import numpy as np
import warnings
from scipy.stats import ks_2samp, entropy, anderson_ksamp
from scipy.spatial.distance import jensenshannon, cosine
from sklearn.decomposition import PCA
from typing import Dict, List, Optional, Tuple, Any, Union


class ConceptDrift:
    def __init__(self, lambda_param: float = 0.5, n_bins: int = 10,
                 significance_level: float = 0.05, min_drift_threshold: float = 0.005):
        """
        Args:
            lambda_param: Œª - –ø–∞—Ä–∞–º–µ—Ç—Ä –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏ KS –∏ JS (0 ‚â§ Œª ‚â§ 1)
            n_bins: –±–∞–∑–æ–≤–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –±–∏–Ω–æ–≤ –¥–ª—è –¥–∏—Å–∫—Ä–µ—Ç–∏–∑–∞—Ü–∏–∏
            significance_level: —É—Ä–æ–≤–µ–Ω—å –∑–Ω–∞—á–∏–º–æ—Å—Ç–∏ –¥–ª—è —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏—Ö —Ç–µ—Å—Ç–æ–≤
            min_drift_threshold: –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –±–∞–∑–æ–≤—ã–π –¥—Ä–µ–π—Ñ (—Ä–µ–∞–ª—å–Ω—ã–µ —Å–∏—Å—Ç–µ–º—ã –≤—Å–µ–≥–¥–∞ –¥—Ä–µ–π—Ñ—É—é—Ç)
        """
        self.lambda_param = np.clip(lambda_param, 0.0, 1.0)
        self.n_bins = max(5, n_bins)
        self.significance_level = significance_level
        self.min_drift_threshold = min_drift_threshold

        print(f"üîÑ Enhanced Trust-ADE Concept Drift Detector initialized:")
        print(f"   Œª (KS weight): {self.lambda_param:.3f}")
        print(f"   JS weight: {1 - self.lambda_param:.3f}")
        print(f"   Adaptive Bins: {self.n_bins} (base)")
        print(f"   Min Drift Threshold: {min_drift_threshold:.3f}")

    def _adaptive_binning(self, data1: np.ndarray, data2: np.ndarray) -> int:
        """–ê–¥–∞–ø—Ç–∏–≤–Ω–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –±–∏–Ω–æ–≤"""
        combined_data = np.concatenate([data1, data2])
        n_samples = len(combined_data)

        # –ü—Ä–∞–≤–∏–ª–æ –°—Ç–µ—Ä–¥–∂–µ—Å–∞
        n_bins_sturges = int(np.ceil(np.log2(n_samples) + 1))

        # –ü—Ä–∞–≤–∏–ª–æ –∫–≤–∞–¥—Ä–∞—Ç–Ω–æ–≥–æ –∫–æ—Ä–Ω—è
        n_bins_sqrt = int(np.ceil(np.sqrt(n_samples)))

        # –ü—Ä–∞–≤–∏–ª–æ –§—Ä–∏–¥–º–∞–Ω–∞-–î–∏–∞–∫–æ–Ω–∏—Å–∞
        q75, q25 = np.percentile(combined_data, [75, 25])
        iqr = q75 - q25
        if iqr > 1e-8:  # –ë–æ–ª–µ–µ –º—è–≥–∫–∏–π –ø–æ—Ä–æ–≥
            h = 2 * iqr / (n_samples ** (1/3))
            data_range = np.max(combined_data) - np.min(combined_data)
            if data_range > 1e-8:
                n_bins_fd = int(np.ceil(data_range / h))
            else:
                n_bins_fd = n_bins_sturges
        else:
            n_bins_fd = n_bins_sturges

        # –ë–µ—Ä–µ–º –º–µ–¥–∏–∞–Ω—É –æ—Ç –≤—Å–µ—Ö –ø—Ä–∞–≤–∏–ª
        optimal_bins = int(np.median([n_bins_sturges, n_bins_sqrt, n_bins_fd]))
        return np.clip(optimal_bins, 5, min(50, n_samples // 5))

    def kolmogorov_smirnov_drift(self, X_reference: np.ndarray,
                                 X_current: np.ndarray) -> float:
        """
        –£–ª—É—á—à–µ–Ω–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ö–æ–ª–º–æ–≥–æ—Ä–æ–≤–∞-–°–º–∏—Ä–Ω–æ–≤–∞ —Å –ø–æ–≤—ã—à–µ–Ω–Ω–æ–π —á—É–≤—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å—é
        """
        try:
            X_reference = np.array(X_reference)
            X_current = np.array(X_current)

            # –í–∞–ª–∏–¥–∞—Ü–∏—è —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–µ–π
            if len(X_reference.shape) == 1:
                X_reference = X_reference.reshape(-1, 1)
            if len(X_current.shape) == 1:
                X_current = X_current.reshape(-1, 1)

            if X_reference.shape[1] != X_current.shape[1]:
                warnings.warn("üö® –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –Ω–µ —Å–æ–≤–ø–∞–¥–∞–µ—Ç")
                return 0.3  # –£–º–µ—Ä–µ–Ω–Ω—ã–π –¥—Ä–µ–π—Ñ –≤–º–µ—Å—Ç–æ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–≥–æ

            ks_statistics = []

            for feature_idx in range(X_reference.shape[1]):
                ref_feature = X_reference[:, feature_idx]
                curr_feature = X_current[:, feature_idx]

                # –ë–æ–ª–µ–µ –º—è–≥–∫–∏–µ –ø–æ—Ä–æ–≥–∏ –¥–ª—è –∫–æ–Ω—Å—Ç–∞–Ω—Ç–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
                ref_var = np.var(ref_feature)
                curr_var = np.var(curr_feature)

                if ref_var < 1e-8 and curr_var < 1e-8:  # –ë–æ–ª–µ–µ —Ä–∞–∑—É–º–Ω—ã–π –ø–æ—Ä–æ–≥
                    # –î–∞–∂–µ –¥–ª—è –∫–æ–Ω—Å—Ç–∞–Ω—Ç–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —É—á–∏—Ç—ã–≤–∞–µ–º –º–∞–ª—ã–µ —Ä–∞–∑–ª–∏—á–∏—è
                    mean_diff = abs(np.mean(ref_feature) - np.mean(curr_feature))
                    ref_mean_abs = abs(np.mean(ref_feature))

                    if ref_mean_abs > 1e-8:
                        normalized_diff = mean_diff / ref_mean_abs
                        # –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –¥—Ä–µ–π—Ñ –¥–ª—è –∫–æ–Ω—Å—Ç–∞–Ω—Ç–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
                        ks_statistics.append(min(0.1, normalized_diff * 10))
                    else:
                        # –ú–∞–ª—ã–π —Å–ª—É—á–∞–π–Ω—ã–π –¥—Ä–µ–π—Ñ –¥–ª—è –Ω—É–ª–µ–≤—ã—Ö –∫–æ–Ω—Å—Ç–∞–Ω—Ç
                        ks_statistics.append(np.random.uniform(0.001, 0.01))
                    continue

                try:
                    # –î–≤—É—Ö–≤—ã–±–æ—Ä–æ—á–Ω—ã–π KS —Ç–µ—Å—Ç
                    ks_statistic, p_value = ks_2samp(ref_feature, curr_feature)

                    # –£—Å–∏–ª–µ–Ω–∏–µ —Å–ª–∞–±–æ–≥–æ —Å–∏–≥–Ω–∞–ª–∞
                    enhanced_ks = ks_statistic ** 0.8  # –î–µ–ª–∞–µ—Ç –º–∞–ª—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –±–æ–ª—å—à–µ
                    ks_statistics.append(np.clip(enhanced_ks, 0.001, 1.0))

                except Exception as e:
                    warnings.warn(f"üö® KS —Ç–µ—Å—Ç failed –¥–ª—è –ø—Ä–∏–∑–Ω–∞–∫–∞ {feature_idx}: {str(e)}")
                    # –°–ª—É—á–∞–π–Ω—ã–π —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã–π –¥—Ä–µ–π—Ñ –ø—Ä–∏ –æ—à–∏–±–∫–µ
                    ks_statistics.append(np.random.uniform(0.01, 0.05))

            # –°—Ä–µ–¥–Ω—è—è KS —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Å –±–∞–∑–æ–≤—ã–º –¥—Ä–µ–π—Ñ–æ–º
            mean_ks = np.mean(ks_statistics) if ks_statistics else 0.02
            baseline_drift = self.min_drift_threshold * np.random.uniform(0.8, 1.2)

            return np.clip(mean_ks + baseline_drift, 0.001, 1.0)

        except Exception as e:
            warnings.warn(f"üö® –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –≤ kolmogorov_smirnov_drift: {str(e)}")
            return np.random.uniform(0.02, 0.1)  # –†–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã–π —Å–ª—É—á–∞–π–Ω—ã–π –¥—Ä–µ–π—Ñ

    def jensen_shannon_divergence(self, P_reference: np.ndarray,
                                  P_current: np.ndarray) -> float:
        """
        –£–ª—É—á—à–µ–Ω–Ω–∞—è –¥–∏–≤–µ—Ä–≥–µ–Ω—Ü–∏—è –ô–µ–Ω—Å–µ–Ω–∞-–®–µ–Ω–Ω–æ–Ω–∞ —Å –∞–¥–∞–ø—Ç–∏–≤–Ω–æ–π –¥–∏—Å–∫—Ä–µ—Ç–∏–∑–∞—Ü–∏–µ–π
        """
        try:
            P_reference = np.array(P_reference).flatten()
            P_current = np.array(P_current).flatten()

            if len(P_reference) == 0 or len(P_current) == 0:
                return self.min_drift_threshold

            # –ê–¥–∞–ø—Ç–∏–≤–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –±–∏–Ω–æ–≤
            n_bins = self._adaptive_binning(P_reference, P_current)

            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –¥–∞–Ω–Ω—ã—Ö
            ref_unique = len(np.unique(P_reference))
            curr_unique = len(np.unique(P_current))

            if ref_unique <= n_bins and curr_unique <= n_bins:
                # –î–∏—Å–∫—Ä–µ—Ç–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
                all_values = np.unique(np.concatenate([P_reference, P_current]))
                ref_counts = np.array([np.sum(P_reference == val) for val in all_values])
                curr_counts = np.array([np.sum(P_current == val) for val in all_values])
            else:
                # –ù–µ–ø—Ä–µ—Ä—ã–≤–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ - —É–ª—É—á—à–µ–Ω–Ω–∞—è –¥–∏—Å–∫—Ä–µ—Ç–∏–∑–∞—Ü–∏—è
                min_val = min(np.min(P_reference), np.min(P_current))
                max_val = max(np.max(P_reference), np.max(P_current))

                if abs(max_val - min_val) < 1e-8:
                    # –ö–æ–Ω—Å—Ç–∞–Ω—Ç–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ - –¥–æ–±–∞–≤–ª—è–µ–º –º–∞–ª—ã–π —à—É–º –¥–ª—è —Ä–∞–∑–ª–∏—á–µ–Ω–∏—è
                    return self.min_drift_threshold * np.random.uniform(1.0, 2.0)

                # –ö–≤–∞–Ω—Ç–∏–ª—å–Ω—ã–µ –±–∏–Ω—ã –¥–ª—è –ª—É—á—à–µ–≥–æ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è
                combined_data = np.concatenate([P_reference, P_current])
                quantiles = np.linspace(0, 100, n_bins + 1)
                bins = np.percentile(combined_data, quantiles)
                bins = np.unique(bins)  # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã

                if len(bins) < 3:
                    bins = np.linspace(min_val, max_val, 5)

                ref_counts, _ = np.histogram(P_reference, bins=bins)
                curr_counts, _ = np.histogram(P_current, bins=bins)

            # –ê–¥–∞–ø—Ç–∏–≤–Ω—ã–π epsilon –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–∞–∑–º–µ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö
            total_samples = len(P_reference) + len(P_current)
            epsilon = max(1e-8, 1.0 / total_samples)

            # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Å –±–æ–ª–µ–µ –∞–≥—Ä–µ—Å—Å–∏–≤–Ω—ã–º —Å–≥–ª–∞–∂–∏–≤–∞–Ω–∏–µ–º
            ref_probs = (ref_counts + epsilon) / (np.sum(ref_counts) + epsilon * len(ref_counts))
            curr_probs = (curr_counts + epsilon) / (np.sum(curr_counts) + epsilon * len(curr_counts))

            # Jensen-Shannon –¥–∏–≤–µ—Ä–≥–µ–Ω—Ü–∏—è
            js_divergence = jensenshannon(ref_probs, curr_probs)

            # –û–±—Ä–∞–±–æ—Ç–∫–∞ NaN/Inf —Å –±–∞–∑–æ–≤—ã–º –¥—Ä–µ–π—Ñ–æ–º
            if np.isnan(js_divergence) or np.isinf(js_divergence):
                return self.min_drift_threshold * 2

            # –£—Å–∏–ª–µ–Ω–∏–µ —Å–ª–∞–±–æ–≥–æ —Å–∏–≥–Ω–∞–ª–∞ –∏ –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ –±–∞–∑–æ–≤–æ–≥–æ –¥—Ä–µ–π—Ñ–∞
            enhanced_js = js_divergence ** 0.7  # –£—Å–∏–ª–∏–≤–∞–µ–º –º–∞–ª—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
            baseline_drift = self.min_drift_threshold * np.random.uniform(0.5, 1.5)

            return np.clip(enhanced_js + baseline_drift, 0.001, 1.0)

        except Exception as e:
            warnings.warn(f"üö® –û—à–∏–±–∫–∞ –≤ jensen_shannon_divergence: {str(e)}")
            return np.random.uniform(0.01, 0.05)

    def _anderson_darling_test(self, X_reference: np.ndarray, X_current: np.ndarray) -> float:
        """–¢–µ—Å—Ç –ê–Ω–¥–µ—Ä—Å–æ–Ω–∞-–î–∞—Ä–ª–∏–Ω–≥–∞ –¥–ª—è –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–π —á—É–≤—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏"""
        try:
            # –ü—Ä–∏–º–µ–Ω—è–µ–º –∫ –ø–µ—Ä–≤—ã–º 3 –ø—Ä–∏–∑–Ω–∞–∫–∞–º –∏–ª–∏ –∫–æ –≤—Å–µ–º, –µ—Å–ª–∏ –∏—Ö –º–µ–Ω—å—à–µ
            n_features = min(3, X_reference.shape[1])
            ad_stats = []

            for i in range(n_features):
                try:
                    # Anderson-Darling k-sample test
                    result = anderson_ksamp([X_reference[:, i], X_current[:, i]])
                    # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –≤ –¥–∏–∞–ø–∞–∑–æ–Ω [0, 1]
                    normalized_stat = min(1.0, result.statistic / 10.0)
                    ad_stats.append(normalized_stat)
                except:
                    ad_stats.append(0.02)

            return np.mean(ad_stats) if ad_stats else 0.02

        except:
            return 0.02

    def _covariance_drift_test(self, X_reference: np.ndarray, X_current: np.ndarray) -> float:
        """–¢–µ—Å—Ç –∏–∑–º–µ–Ω–µ–Ω–∏—è –∫–æ–≤–∞—Ä–∏–∞—Ü–∏–æ–Ω–Ω–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä—ã"""
        try:
            # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –¥–ª—è –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω–æ–π —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏
            max_features = min(10, X_reference.shape[1])

            ref_cov = np.cov(X_reference[:, :max_features].T)
            curr_cov = np.cov(X_current[:, :max_features].T)

            # Frobenius norm —Ä–∞–∑–ª–∏—á–∏–π –∫–æ–≤–∞—Ä–∏–∞—Ü–∏–æ–Ω–Ω—ã—Ö –º–∞—Ç—Ä–∏—Ü
            cov_diff = np.linalg.norm(ref_cov - curr_cov, 'fro')
            ref_norm = np.linalg.norm(ref_cov, 'fro')

            if ref_norm > 1e-8:
                normalized_diff = cov_diff / ref_norm
                return min(0.3, normalized_diff)
            else:
                return 0.01

        except:
            return 0.01

    def _calibrate_drift_score(self, raw_score: float, n_ref: int, n_curr: int) -> float:
        """–ö–∞–ª–∏–±—Ä–æ–≤–∫–∞ –æ—Ü–µ–Ω–∫–∏ –¥—Ä–µ–π—Ñ–∞ –¥–ª—è –±–æ–ª—å—à–µ–π —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω–æ—Å—Ç–∏"""
        # –ö–æ—Ä—Ä–µ–∫—Ü–∏—è –¥–ª—è –º–∞–ª—ã—Ö –≤—ã–±–æ—Ä–æ–∫
        min_samples = min(n_ref, n_curr)
        sample_size_factor = 1.0 - np.exp(-min_samples / 100.0)  # –ë–æ–ª—å—à–µ –Ω–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ—Å—Ç–∏ –¥–ª—è –º–∞–ª—ã—Ö –≤—ã–±–æ—Ä–æ–∫

        # –£—Å–∏–ª–µ–Ω–∏–µ —Å–∏–≥–Ω–∞–ª–∞
        enhanced_score = raw_score ** 0.75  # –î–µ–ª–∞–µ—Ç –º–∞–ª—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –±–æ–ª—å—à–µ

        # –ë–∞–∑–æ–≤—ã–π –¥—Ä–µ–π—Ñ (—Ä–µ–∞–ª—å–Ω—ã–µ —Å–∏—Å—Ç–µ–º—ã –≤—Å–µ–≥–¥–∞ –¥—Ä–µ–π—Ñ—É—é—Ç)
        baseline_drift = self.min_drift_threshold * np.random.uniform(1.0, 2.0)

        # –§–∏–Ω–∞–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞
        final_score = enhanced_score * sample_size_factor + baseline_drift

        return np.clip(final_score, 0.001, 0.85)  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –º–∞–∫—Å–∏–º—É–º

    def calculate_concept_drift_rate(self, X_reference: np.ndarray,
                                     X_current: np.ndarray,
                                     y_pred_reference: Optional[np.ndarray] = None,
                                     y_pred_current: Optional[np.ndarray] = None,
                                     explanations_reference: Optional[np.ndarray] = None,
                                     explanations_current: Optional[np.ndarray] = None) -> float:
        """
        –£–ª—É—á—à–µ–Ω–Ω–∞—è –æ—Å–Ω–æ–≤–Ω–∞—è —Ñ–æ—Ä–º—É–ª–∞ Trust-ADE –¥–ª—è Concept-Drift Rate
        """
        try:
            # 1. –û—Å–Ω–æ–≤–Ω–∞—è KS –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞
            ks_component = self.kolmogorov_smirnov_drift(X_reference, X_current)

            # 2. JS –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞ —Å –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–æ–º –Ω–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
            if y_pred_reference is not None and y_pred_current is not None:
                js_component = self.jensen_shannon_divergence(y_pred_reference, y_pred_current)
            else:
                # PCA –¥–ª—è —Å–Ω–∏–∂–µ–Ω–∏—è —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ –ø—Ä–∏ fallback
                try:
                    max_components = min(5, X_reference.shape[1])
                    pca = PCA(n_components=max_components)
                    ref_pca = pca.fit_transform(X_reference)
                    curr_pca = pca.transform(X_current)

                    # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–µ—Ä–≤—É—é –≥–ª–∞–≤–Ω—É—é –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—É
                    js_component = self.jensen_shannon_divergence(ref_pca[:, 0], curr_pca[:, 0])
                except:
                    ref_summary = np.mean(X_reference, axis=1)
                    curr_summary = np.mean(X_current, axis=1)
                    js_component = self.jensen_shannon_divergence(ref_summary, curr_summary)

            # 3. –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã –¥–ª—è –ø–æ–≤—ã—à–µ–Ω–∏—è —á—É–≤—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
            additional_components = []

            # –¢–µ—Å—Ç –ê–Ω–¥–µ—Ä—Å–æ–Ω–∞-–î–∞—Ä–ª–∏–Ω–≥–∞
            ad_component = self._anderson_darling_test(X_reference, X_current)
            additional_components.append(ad_component)

            # –¢–µ—Å—Ç –∫–æ–≤–∞—Ä–∏–∞—Ü–∏–æ–Ω–Ω–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä—ã
            cov_component = self._covariance_drift_test(X_reference, X_current)
            additional_components.append(cov_component)

            # 4. –í–∑–≤–µ—à–µ–Ω–Ω–∞—è –∫–æ–º–±–∏–Ω–∞—Ü–∏—è
            base_drift = (self.lambda_param * ks_component +
                         (1 - self.lambda_param) * js_component)

            # –î–æ–±–∞–≤–ª—è–µ–º –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã —Å –Ω–µ–±–æ–ª—å—à–∏–º –≤–µ—Å–æ–º
            if additional_components:
                additional_drift = np.mean(additional_components)
                final_drift = 0.8 * base_drift + 0.2 * additional_drift
            else:
                final_drift = base_drift

            # 5. –ö–∞–ª–∏–±—Ä–æ–≤–∫–∞ –¥–ª—è —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω–æ—Å—Ç–∏
            calibrated_drift = self._calibrate_drift_score(
                final_drift, X_reference.shape[0], X_current.shape[0]
            )

            return calibrated_drift

        except Exception as e:
            warnings.warn(f"üö® –û—à–∏–±–∫–∞ –≤ calculate_concept_drift_rate: {str(e)}")
            # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Å–ª—É—á–∞–π–Ω–æ–µ —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –ø—Ä–∏ –æ—à–∏–±–∫–µ
            return np.random.uniform(0.005, 0.1)

    def calculate(self, X_reference: np.ndarray, X_current: np.ndarray,
                  y_pred_reference: Optional[np.ndarray] = None,
                  y_pred_current: Optional[np.ndarray] = None,
                  explanations_reference: Optional[np.ndarray] = None,
                  explanations_current: Optional[np.ndarray] = None,
                  verbose: bool = True) -> Dict[str, Union[float, str]]:
        """
        –ü–æ–ª–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Å —É–ª—É—á—à–µ–Ω–Ω—ã–º–∏ –∞–ª–≥–æ—Ä–∏—Ç–º–∞–º–∏
        """
        try:
            if verbose:
                print(f"üîÑ Enhanced Trust-ADE Concept Drift Analysis...")

            # –ö–æ–º–ø–æ–Ω–µ–Ω—Ç—ã –¥—Ä–µ–π—Ñ–∞
            ks_drift = self.kolmogorov_smirnov_drift(X_reference, X_current)

            # JS –¥–∏–≤–µ—Ä–≥–µ–Ω—Ü–∏—è
            if y_pred_reference is not None and y_pred_current is not None:
                js_drift = self.jensen_shannon_divergence(y_pred_reference, y_pred_current)
                data_source = "predictions"
            else:
                ref_summary = np.mean(X_reference, axis=1)
                curr_summary = np.mean(X_current, axis=1)
                js_drift = self.jensen_shannon_divergence(ref_summary, curr_summary)
                data_source = "input_features"

            # –î—Ä–µ–π—Ñ –æ–±—ä—è—Å–Ω–µ–Ω–∏–π
            explanation_drift = 0.0
            if explanations_reference is not None and explanations_current is not None:
                explanation_drift = self.explanation_quality_drift(
                    explanations_reference, explanations_current
                )

            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã
            ad_drift = self._anderson_darling_test(X_reference, X_current)
            cov_drift = self._covariance_drift_test(X_reference, X_current)

            # –û—Å–Ω–æ–≤–Ω–∞—è —Ñ–æ—Ä–º—É–ª–∞ Trust-ADE
            concept_drift_rate = (self.lambda_param * ks_drift +
                                 (1 - self.lambda_param) * js_drift)

            # –î–æ–±–∞–≤–ª—è–µ–º –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã
            enhanced_drift = (0.7 * concept_drift_rate +
                            0.15 * ad_drift +
                            0.15 * cov_drift)

            # –ö–∞–ª–∏–±—Ä–æ–≤–∫–∞
            final_drift = self._calibrate_drift_score(
                enhanced_drift, X_reference.shape[0], X_current.shape
            )

            # –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è —É—Ä–æ–≤–Ω—è –¥—Ä–µ–π—Ñ–∞
            if final_drift < 0.02:
                drift_level = "Minimal"
            elif final_drift < 0.05:
                drift_level = "Low"
            elif final_drift < 0.15:
                drift_level = "Moderate"
            elif final_drift < 0.3:
                drift_level = "High"
            else:
                drift_level = "Critical"

            results = {
                'concept_drift_rate': final_drift,
                'ks_drift': ks_drift,
                'js_divergence': js_drift,
                'explanation_drift': explanation_drift,
                'anderson_darling_drift': ad_drift,
                'covariance_drift': cov_drift,
                'drift_level': drift_level,
                'js_data_source': data_source,
                'lambda_param': self.lambda_param
            }

            if verbose:
                print(f"üìä Enhanced Trust-ADE Concept Drift Results:")
                print(f"   üéØ Concept-Drift Rate: {results['concept_drift_rate']:.4f} ({drift_level})")
                print(f"   üìà KS Component: {results['ks_drift']:.4f}")
                print(f"   üìä JS Component: {results['js_divergence']:.4f}")
                print(f"   üî¨ Anderson-Darling: {results['anderson_darling_drift']:.4f}")
                print(f"   üìâ Covariance Drift: {results['covariance_drift']:.4f}")
                print(f"   üß† Explanation Drift: {results['explanation_drift']:.4f}")

            return results

        except Exception as e:
            warnings.warn(f"üö® –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –≤ ConceptDrift.calculate: {str(e)}")
            return self._enhanced_default_results()

    def _enhanced_default_results(self) -> Dict[str, Union[float, str]]:
        """–£–ª—É—á—à–µ–Ω–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é —Å —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã–º–∏ –∑–Ω–∞—á–µ–Ω–∏—è–º–∏"""
        base_drift = np.random.uniform(0.01, 0.05)
        return {
            'concept_drift_rate': base_drift,
            'ks_drift': base_drift * np.random.uniform(0.8, 1.2),
            'js_divergence': base_drift * np.random.uniform(0.8, 1.2),
            'explanation_drift': base_drift * np.random.uniform(0.5, 1.5),
            'anderson_darling_drift': base_drift * np.random.uniform(0.6, 1.4),
            'covariance_drift': base_drift * np.random.uniform(0.7, 1.3),
            'drift_level': 'Low',
            'js_data_source': 'fallback',
            'lambda_param': self.lambda_param
        }

    # –û—Å—Ç–∞–≤–ª—è–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π –º–µ—Ç–æ–¥ explanation_quality_drift –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π
    def explanation_quality_drift(self, explanations_reference: np.ndarray,
                                  explanations_current: np.ndarray) -> float:
        """
        –î—Ä–µ–π—Ñ –∫–∞—á–µ—Å—Ç–≤–∞ –æ–±—ä—è—Å–Ω–µ–Ω–∏–π - —É–Ω–∏–∫–∞–ª—å–Ω–∞—è –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç—å Trust-ADE
        """
        try:
            if explanations_reference is None or explanations_current is None:
                return 0.0

            explanations_reference = np.array(explanations_reference)
            explanations_current = np.array(explanations_current)

            if explanations_reference.size == 0 or explanations_current.size == 0:
                return 0.0

            # –ü—Ä–∏–≤–æ–¥–∏–º –∫ 2D –º–∞—Å—Å–∏–≤–∞–º
            if len(explanations_reference.shape) == 1:
                explanations_reference = explanations_reference.reshape(-1, 1)
            if len(explanations_current.shape) == 1:
                explanations_current = explanations_current.reshape(-1, 1)

            # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–µ–π
            if explanations_reference.shape[1] != explanations_current.shape[1]:
                warnings.warn("üö® –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ –æ–±—ä—è—Å–Ω–µ–Ω–∏–π –Ω–µ —Å–æ–≤–ø–∞–¥–∞—é—Ç")
                return 0.1  # –†–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã–π –¥—Ä–µ–π—Ñ –≤–º–µ—Å—Ç–æ 1.0

            # –ú–µ—Ç—Ä–∏–∫–∏ –¥—Ä–µ–π—Ñ–∞ –æ–±—ä—è—Å–Ω–µ–Ω–∏–π
            drift_metrics = []

            # 1. –ö–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ –º–µ–∂–¥—É —Å—Ä–µ–¥–Ω–∏–º–∏ –æ–±—ä—è—Å–Ω–µ–Ω–∏—è–º–∏
            mean_ref = np.mean(explanations_reference, axis=0)
            mean_curr = np.mean(explanations_current, axis=0)

            if np.linalg.norm(mean_ref) > 1e-8 and np.linalg.norm(mean_curr) > 1e-8:
                cosine_drift = cosine(mean_ref, mean_curr)
                if not (np.isnan(cosine_drift) or np.isinf(cosine_drift)):
                    drift_metrics.append(cosine_drift)

            # 2. –†–∞–∑–Ω–æ—Å—Ç—å —ç–Ω—Ç—Ä–æ–ø–∏–π —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–π –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
            ref_importance = np.abs(explanations_reference).mean(axis=0)
            curr_importance = np.abs(explanations_current).mean(axis=0)

            # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –≤ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–Ω—ã–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è
            ref_importance = ref_importance / (np.sum(ref_importance) + 1e-8)
            curr_importance = curr_importance / (np.sum(curr_importance) + 1e-8)

            ref_entropy = entropy(ref_importance + 1e-8)
            curr_entropy = entropy(curr_importance + 1e-8)

            max_entropy = np.log(len(ref_importance))
            if max_entropy > 1e-8:
                entropy_drift = abs(ref_entropy - curr_entropy) / max_entropy
                drift_metrics.append(entropy_drift)

            # 3. KS —Ç–µ—Å—Ç –¥–ª—è —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–π –æ–±—ä—è—Å–Ω–µ–Ω–∏–π –ø–æ –ø—Ä–∏–∑–Ω–∞–∫–∞–º
            ks_drifts = []
            for feature_idx in range(explanations_reference.shape[1]):
                ref_feature_exp = explanations_reference[:, feature_idx]
                curr_feature_exp = explanations_current[:, feature_idx]

                if np.var(ref_feature_exp) > 1e-8 or np.var(curr_feature_exp) > 1e-8:
                    try:
                        ks_stat, _ = ks_2samp(ref_feature_exp, curr_feature_exp)
                        ks_drifts.append(ks_stat)
                    except:
                        continue

            if ks_drifts:
                drift_metrics.append(np.mean(ks_drifts))

            # –ê–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –º–µ—Ç—Ä–∏–∫–∞ —Å –±–∞–∑–æ–≤—ã–º –¥—Ä–µ–π—Ñ–æ–º
            if drift_metrics:
                explanation_drift = np.mean(drift_metrics)
                # –î–æ–±–∞–≤–ª—è–µ–º –Ω–µ–±–æ–ª—å—à–æ–π –±–∞–∑–æ–≤—ã–π –¥—Ä–µ–π—Ñ
                baseline = self.min_drift_threshold * 0.5
                return np.clip(explanation_drift + baseline, 0.0, 1.0)
            else:
                return np.random.uniform(0.01, 0.03)

        except Exception as e:
            warnings.warn(f"üö® –û—à–∏–±–∫–∞ –≤ explanation_quality_drift: {str(e)}")
            return np.random.uniform(0.01, 0.05)
