"""
Trust-ADE Explainability Score Module
–†–µ–∞–ª–∏–∑–∞—Ü–∏—è –∫–∞—É–∑–∞–ª—å–Ω–æ–π –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –æ–±—ä—è—Å–Ω–µ–Ω–∏–π —Å–æ–≥–ª–∞—Å–Ω–æ —Å—Ç–∞—Ç—å–µ XAI 2.0
"""

import numpy as np
import warnings
from scipy.spatial.distance import cosine, euclidean
from scipy.stats import entropy, ks_2samp
from sklearn.metrics import mutual_info_score
from typing import Dict, List, Optional, Union, Any


class ExplainabilityScore:
    """
    –ö–∞—É–∑–∞–ª—å–Ω–æ-–æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π Explainability Score –¥–ª—è Trust-ADE –ø—Ä–æ—Ç–æ–∫–æ–ª–∞

    –†–µ–∞–ª–∏–∑—É–µ—Ç —Ñ–æ—Ä–º—É–ª—É: ES = w_c¬∑F_c + w_s¬∑C_s + w_i¬∑S_i + w_h¬∑U_h
    –≥–¥–µ:
    - F_c: –∫–∞—É–∑–∞–ª—å–Ω–∞—è —Ñ–∏–¥–µ–ª–∏—Ç–∏ —Å —ç–∫—Å–ø–µ—Ä—Ç–Ω—ã–º–∏ –≥—Ä–∞—Ñ–∞–º–∏
    - C_s: —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∞—è –∫–æ–≥–µ—Ä–µ–Ω—Ç–Ω–æ—Å—Ç—å —á–µ—Ä–µ–∑ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω—É—é —ç–Ω—Ç—Ä–æ–ø–∏—é
    - S_i: —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏–π –ø—Ä–∏ Œµ-–≤–æ–∑–º—É—â–µ–Ω–∏—è—Ö
    - U_h: —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∞—è –ø–æ–Ω—è—Ç–Ω–æ—Å—Ç—å —á–µ—Ä–µ–∑ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã
    """

    def __init__(self, causal_weight: float = 0.35, coherence_weight: float = 0.25,
                 stability_weight: float = 0.25, human_weight: float = 0.15,
                 alpha: float = 0.5, gamma: float = 1.0, noise_threshold: float = 1e-6):
        """
        Args:
            causal_weight: w_c - –≤–µ—Å –∫–∞—É–∑–∞–ª—å–Ω–æ–π —Ñ–∏–¥–µ–ª–∏—Ç–∏
            coherence_weight: w_s - –≤–µ—Å —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π –∫–æ–≥–µ—Ä–µ–Ω—Ç–Ω–æ—Å—Ç–∏
            stability_weight: w_i - –≤–µ—Å —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏ –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏–π
            human_weight: w_h - –≤–µ—Å —á–µ–ª–æ–≤–µ—á–µ—Å–∫–æ–π –ø–æ–Ω—è—Ç–Ω–æ—Å—Ç–∏
            alpha: –ø–∞—Ä–∞–º–µ—Ç—Ä –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏ –ø–æ–ª–Ω–æ—Ç—ã –∏ —Ç–æ—á–Ω–æ—Å—Ç–∏ –≤ F_c
            gamma: —á—É–≤—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∫ concept drift
            noise_threshold: –ø–æ—Ä–æ–≥ –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ —à—É–º–∞
        """
        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –≤–µ—Å–æ–≤ –¥–æ 1.0
        total_weight = causal_weight + coherence_weight + stability_weight + human_weight

        self.w_c = causal_weight / total_weight
        self.w_s = coherence_weight / total_weight
        self.w_i = stability_weight / total_weight
        self.w_h = human_weight / total_weight

        self.alpha = alpha  # –¥–ª—è —Ñ–æ—Ä–º—É–ª—ã –∫–∞—É–∑–∞–ª—å–Ω–æ–π —Ñ–∏–¥–µ–ª–∏—Ç–∏
        self.gamma = gamma  # –¥–ª—è concept drift
        self.noise_threshold = noise_threshold

        print(f"üß† Trust-ADE Explainability Score initialized:")
        print(f"   Causal Fidelity weight: {self.w_c:.3f}")
        print(f"   Semantic Coherence weight: {self.w_s:.3f}")
        print(f"   Interpretation Stability weight: {self.w_i:.3f}")
        print(f"   Human Comprehensibility weight: {self.w_h:.3f}")

    def causal_fidelity(self, system_edges: set, expert_edges: set,
                       confidence_scores: Optional[Dict] = None,
                       snr_ratio: Optional[float] = None) -> float:
        """
        –ö–∞—É–∑–∞–ª—å–Ω–∞—è —Ñ–∏–¥–µ–ª–∏—Ç–∏ —Å–æ–≥–ª–∞—Å–Ω–æ —Ñ–æ—Ä–º—É–ª–µ –∏–∑ —Å—Ç–∞—Ç—å–∏:
        F_c = |E_sys ‚à© E_exp|/|E_exp| √ó Œ± + |E_sys ‚à© E_exp|/|E_sys| √ó (1-Œ±)

        –° —Ä–æ–±–∞—Å—Ç–Ω–æ–π –º–æ–¥–∏—Ñ–∏–∫–∞—Ü–∏–µ–π –¥–ª—è –∑–∞—à—É–º–ª–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö:
        F_c_robust = F_c √ó (1 - Œ∑¬∑SNR^(-1))

        Args:
            system_edges: –∫–∞—É–∑–∞–ª—å–Ω—ã–µ —Å–≤—è–∑–∏, –≤—ã—è–≤–ª–µ–Ω–Ω—ã–µ —Å–∏—Å—Ç–µ–º–æ–π
            expert_edges: —ç–∫—Å–ø–µ—Ä—Ç–Ω—ã–µ –∫–∞—É–∑–∞–ª—å–Ω—ã–µ —Å–≤—è–∑–∏
            confidence_scores: —É—Ä–æ–≤–µ–Ω—å –∫–æ–Ω—Å–µ–Ω—Å—É—Å–∞ —ç–∫—Å–ø–µ—Ä—Ç–æ–≤
            snr_ratio: –æ—Ç–Ω–æ—à–µ–Ω–∏–µ —Å–∏–≥–Ω–∞–ª/—à—É–º –¥–ª—è —Ä–æ–±–∞—Å—Ç–Ω–æ—Å—Ç–∏

        Returns:
            float: –∫–∞—É–∑–∞–ª—å–Ω–∞—è —Ñ–∏–¥–µ–ª–∏—Ç–∏ [0, 1]
        """
        try:
            if len(expert_edges) == 0 or len(system_edges) == 0:
                return 0.5  # –Ω–µ–π—Ç—Ä–∞–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –ø—Ä–∏ –æ—Ç—Å—É—Ç—Å—Ç–≤–∏–∏ –¥–∞–Ω–Ω—ã—Ö

            # –ü–µ—Ä–µ—Å–µ—á–µ–Ω–∏–µ –∫–∞—É–∑–∞–ª—å–Ω—ã—Ö —Å–≤—è–∑–µ–π
            intersection = system_edges.intersection(expert_edges)

            # –ë–∞–∑–æ–≤–∞—è —Ñ–æ—Ä–º—É–ª–∞ F_c –∏–∑ —Å—Ç–∞—Ç—å–∏
            recall = len(intersection) / len(expert_edges)  # –ø–æ–ª–Ω–æ—Ç–∞
            precision = len(intersection) / len(system_edges)  # —Ç–æ—á–Ω–æ—Å—Ç—å

            base_fidelity = self.alpha * recall + (1 - self.alpha) * precision

            # –†–æ–±–∞—Å—Ç–Ω–∞—è –º–æ–¥–∏—Ñ–∏–∫–∞—Ü–∏—è –¥–ª—è –∫–æ–Ω—Ñ–ª–∏–∫—Ç–Ω—ã—Ö —ç–∫—Å–ø–µ—Ä—Ç–Ω—ã—Ö –º–Ω–µ–Ω–∏–π
            if confidence_scores is not None:
                # –ö–æ–Ω—Å–µ—Ä–≤–∞—Ç–∏–≤–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –¥–ª—è —Å–ø–æ—Ä–Ω—ã—Ö —Å–≤—è–∑–µ–π
                disputed_edges = {edge for edge in expert_edges
                                if confidence_scores.get(edge, 1.0) < 0.7}

                # –ò—Å–∫–ª—é—á–∞–µ–º —Å–ø–æ—Ä–Ω—ã–µ —Å–≤—è–∑–∏ –∏–∑ –±–∞–∑–æ–≤–æ–π –æ—Ü–µ–Ω–∫–∏
                clean_expert = expert_edges - disputed_edges
                clean_intersection = system_edges.intersection(clean_expert)

                if len(clean_expert) > 0:
                    clean_recall = len(clean_intersection) / len(clean_expert)
                    clean_precision = len(clean_intersection) / len(system_edges)
                    base_fidelity = self.alpha * clean_recall + (1 - self.alpha) * clean_precision

            # –ú–æ–¥–∏—Ñ–∏–∫–∞—Ü–∏—è –¥–ª—è –∑–∞—à—É–º–ª–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
            if snr_ratio is not None and snr_ratio > 0:
                eta = 0.1  # –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –≤–ª–∏—è–Ω–∏—è —à—É–º–∞
                noise_penalty = eta / snr_ratio
                robust_fidelity = base_fidelity * (1 - noise_penalty)
                return np.clip(robust_fidelity, 0.0, 1.0)

            return np.clip(base_fidelity, 0.0, 1.0)

        except Exception as e:
            warnings.warn(f"üö® –û—à–∏–±–∫–∞ –≤ causal_fidelity: {str(e)}")
            return 0.5

    def extract_causal_edges_from_explanations(self, explanations: np.ndarray,
                                             feature_names: Optional[List[str]] = None,
                                             threshold: float = 0.1) -> set:
        """
        –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–∞—É–∑–∞–ª—å–Ω—ã—Ö —Å–≤—è–∑–µ–π –∏–∑ –º–∞—Ç—Ä–∏—Ü—ã –æ–±—ä—è—Å–Ω–µ–Ω–∏–π

        Args:
            explanations: –º–∞—Ç—Ä–∏—Ü–∞ –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ [n_samples, n_features]
            feature_names: –∏–º–µ–Ω–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
            threshold: –ø–æ—Ä–æ–≥ –∑–Ω–∞—á–∏–º–æ—Å—Ç–∏ –¥–ª—è –∫–∞—É–∑–∞–ª—å–Ω–æ–π —Å–≤—è–∑–∏

        Returns:
            set: –º–Ω–æ–∂–µ—Å—Ç–≤–æ –∫–∞—É–∑–∞–ª—å–Ω—ã—Ö —Å–≤—è–∑–µ–π (–ø–∞—Ä –ø—Ä–∏–∑–Ω–∞–∫–æ–≤)
        """
        try:
            if explanations is None or len(explanations) == 0:
                return set()

            explanations = np.array(explanations)
            n_samples, n_features = explanations.shape

            # –ö–∞—É–∑–∞–ª—å–Ω—ã–µ —Å–≤—è–∑–∏ —á–µ—Ä–µ–∑ –≤–∑–∞–∏–º–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
            causal_edges = set()

            for i in range(n_features):
                for j in range(i + 1, n_features):
                    # –í–∑–∞–∏–º–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –º–µ–∂–¥—É –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏ i –∏ j
                    try:
                        mi_score = mutual_info_score(
                            explanations[:, i] > threshold,
                            explanations[:, j] > threshold
                        )

                        # –ï—Å–ª–∏ –≤–∑–∞–∏–º–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –≤—ã—à–µ –ø–æ—Ä–æ–≥–∞ - —Å—á–∏—Ç–∞–µ–º –∫–∞—É–∑–∞–ª—å–Ω–æ–π —Å–≤—è–∑—å—é
                        if mi_score > 0.1:
                            if feature_names:
                                edge = (feature_names[i], feature_names[j])
                            else:
                                edge = (i, j)
                            causal_edges.add(edge)

                    except Exception:
                        continue

            return causal_edges

        except Exception as e:
            warnings.warn(f"üö® –û—à–∏–±–∫–∞ –≤ extract_causal_edges: {str(e)}")
            return set()

    def semantic_coherence(self, explanations: np.ndarray) -> float:
        """
        –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∞—è –∫–æ–≥–µ—Ä–µ–Ω—Ç–Ω–æ—Å—Ç—å —Å–æ–≥–ª–∞—Å–Ω–æ —Ñ–æ—Ä–º—É–ª–µ:
        C_s = 1 - H(E)/H_max
        –≥–¥–µ H(E) - —ç–Ω—Ç—Ä–æ–ø–∏—è —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –æ–±—ä—è—Å–Ω–µ–Ω–∏–π

        Args:
            explanations: –º–∞—Ç—Ä–∏—Ü–∞ –æ–±—ä—è—Å–Ω–µ–Ω–∏–π

        Returns:
            float: —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∞—è –∫–æ–≥–µ—Ä–µ–Ω—Ç–Ω–æ—Å—Ç—å [0, 1]
        """
        try:
            if explanations is None or len(explanations) == 0:
                return 0.0

            # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –æ–±—ä—è—Å–Ω–µ–Ω–∏–π –≤ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π
            exp_flat = np.abs(explanations).flatten()
            exp_filtered = exp_flat[exp_flat > self.noise_threshold]

            if len(exp_filtered) <= 1:
                return 1.0  # –º–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –∫–æ–≥–µ—Ä–µ–Ω—Ç–Ω–æ—Å—Ç—å –¥–ª—è –∫–æ–Ω—Å—Ç–∞–Ω—Ç—ã

            # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –≤ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ
            total_mass = np.sum(exp_filtered)
            if total_mass < 1e-12:
                return 0.5

            prob_dist = exp_filtered / total_mass

            # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω–∞—è —ç–Ω—Ç—Ä–æ–ø–∏—è H(E)
            H_E = entropy(prob_dist, base=2)
            H_max = np.log2(len(prob_dist))  # –º–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è —ç–Ω—Ç—Ä–æ–ø–∏—è

            # C_s = 1 - H(E)/H_max (–±–æ–ª—å—à–µ –∫–æ–≥–µ—Ä–µ–Ω—Ç–Ω–æ—Å—Ç–∏ = –º–µ–Ω—å—à–µ —ç–Ω—Ç—Ä–æ–ø–∏–∏)
            coherence = 1.0 - (H_E / H_max) if H_max > 0 else 1.0

            return np.clip(coherence, 0.0, 1.0)

        except Exception as e:
            warnings.warn(f"üö® –û—à–∏–±–∫–∞ –≤ semantic_coherence: {str(e)}")
            return 0.5

    def interpretation_stability(self, model: Any, explainer: Any, X: np.ndarray,
                               perturbation_sizes: List[float] = [0.01, 0.05, 0.1],
                               n_samples: int = 25, distance_metric: str = 'cosine') -> float:
        """
        –°—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏–π —Å–æ–≥–ª–∞—Å–Ω–æ —Ñ–æ—Ä–º—É–ª–µ:
        S_i = 1 - (1/N) ‚àë d(E_i, E_i^Œµ)
        –≥–¥–µ d - –º–µ—Ç—Ä–∏–∫–∞ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏—è –º–µ–∂–¥—É –æ–±—ä—è—Å–Ω–µ–Ω–∏—è–º–∏

        Args:
            model: –º–æ–¥–µ–ª—å –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
            explainer: –æ–±—ä—è—Å–Ω–∏—Ç–µ–ª—å
            X: –≤—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
            perturbation_sizes: —Ä–∞–∑–º–µ—Ä—ã –≤–æ–∑–º—É—â–µ–Ω–∏–π Œµ
            n_samples: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–µ—Å—Ç–æ–≤—ã—Ö –æ–±—Ä–∞–∑—Ü–æ–≤
            distance_metric: –º–µ—Ç—Ä–∏–∫–∞ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏—è ('cosine', 'euclidean', 'manhattan')

        Returns:
            float: —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏–π [0, 1]
        """
        try:
            if X is None or len(X) == 0:
                return 0.0

            X = np.array(X)
            n_test = min(n_samples, len(X))
            all_stabilities = []

            # –¢–µ—Å—Ç–∏—Ä—É–µ–º —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —Ä–∞–∑–º–µ—Ä–æ–≤ –≤–æ–∑–º—É—â–µ–Ω–∏–π
            for eps in perturbation_sizes:
                eps_stabilities = []

                for i in range(n_test):
                    try:
                        # –û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π –æ–±—Ä–∞–∑–µ—Ü
                        x_orig = X[i:i+1]

                        # –í–æ–∑–º—É—â–µ–Ω–Ω—ã–π –æ–±—Ä–∞–∑–µ—Ü
                        noise = np.random.normal(0, eps, x_orig.shape)
                        x_pert = x_orig + noise

                        # –ü–æ–ª—É—á–∞–µ–º –æ–±—ä—è—Å–Ω–µ–Ω–∏—è
                        exp_orig = self._safe_explain(explainer, x_orig)
                        exp_pert = self._safe_explain(explainer, x_pert)

                        if exp_orig is not None and exp_pert is not None:
                            # –í—ã—á–∏—Å–ª—è–µ–º —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ –º–µ–∂–¥—É –æ–±—ä—è—Å–Ω–µ–Ω–∏—è–º–∏
                            distance = self._compute_explanation_distance(
                                exp_orig, exp_pert, distance_metric
                            )

                            # S_i = 1 - distance (–±–æ–ª—å—à–µ —Å—Ö–æ–¥—Å—Ç–≤–∞ = –±–æ–ª—å—à–µ —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏)
                            stability = max(0.0, 1.0 - distance)
                            eps_stabilities.append(stability)

                    except Exception:
                        continue  # –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –ø—Ä–æ–±–ª–µ–º–Ω—ã–µ –æ–±—Ä–∞–∑—Ü—ã

                if eps_stabilities:
                    all_stabilities.extend(eps_stabilities)

            return np.mean(all_stabilities) if all_stabilities else 0.5

        except Exception as e:
            warnings.warn(f"üö® –û—à–∏–±–∫–∞ –≤ interpretation_stability: {str(e)}")
            return 0.5

    def _safe_explain(self, explainer: Any, X: np.ndarray) -> Optional[np.ndarray]:
        """–ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ –ø–æ–ª—É—á–µ–Ω–∏–µ –æ–±—ä—è—Å–Ω–µ–Ω–∏–π —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫"""
        try:
            if hasattr(explainer, 'explain_instance'):
                # LIME-style explainer
                explanations = []
                for sample in X:
                    exp = explainer.explain_instance(sample, explainer.predict_fn)
                    if hasattr(exp, 'as_list'):
                        exp_values = [val for _, val in exp.as_list()]
                        explanations.append(exp_values)
                return np.array(explanations) if explanations else None

            elif hasattr(explainer, 'shap_values'):
                # SHAP explainer
                return explainer.shap_values(X)

            elif hasattr(explainer, '__call__'):
                # Generic callable explainer
                return explainer(X)

            else:
                return None

        except Exception as e:
            warnings.warn(f"üö® –û—à–∏–±–∫–∞ –≤ _safe_explain: {str(e)}")
            return None

    def _compute_explanation_distance(self, exp1: np.ndarray, exp2: np.ndarray,
                                    metric: str = 'cosine') -> float:
        """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏—è –º–µ–∂–¥—É –æ–±—ä—è—Å–Ω–µ–Ω–∏—è–º–∏"""
        try:
            exp1_flat = np.array(exp1).flatten()
            exp2_flat = np.array(exp2).flatten()

            if len(exp1_flat) != len(exp2_flat):
                # –ü—Ä–∏–≤–æ–¥–∏–º –∫ –æ–¥–∏–Ω–∞–∫–æ–≤–æ–π –¥–ª–∏–Ω–µ
                min_len = min(len(exp1_flat), len(exp2_flat))
                exp1_flat = exp1_flat[:min_len]
                exp2_flat = exp2_flat[:min_len]

            if metric == 'cosine':
                return cosine(exp1_flat, exp2_flat)
            elif metric == 'euclidean':
                return euclidean(exp1_flat, exp2_flat) / np.sqrt(len(exp1_flat))
            elif metric == 'manhattan':
                return np.mean(np.abs(exp1_flat - exp2_flat))
            else:
                return cosine(exp1_flat, exp2_flat)  # default

        except Exception:
            return 1.0  # –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ –ø—Ä–∏ –æ—à–∏–±–∫–µ

    def human_comprehensibility(self, explanations: np.ndarray,
                              expert_ratings: Optional[List[float]] = None,
                              complexity_factors: Optional[Dict] = None) -> float:
        """
        –ß–µ–ª–æ–≤–µ—á–µ—Å–∫–∞—è –ø–æ–Ω—è—Ç–Ω–æ—Å—Ç—å —á–µ—Ä–µ–∑ –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ–º—ã–µ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã

        Args:
            explanations: –º–∞—Ç—Ä–∏—Ü–∞ –æ–±—ä—è—Å–Ω–µ–Ω–∏–π
            expert_ratings: —ç–∫—Å–ø–µ—Ä—Ç–Ω—ã–µ –æ—Ü–µ–Ω–∫–∏ –ø–æ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–π —à–∫–∞–ª–µ [0, 1]
            complexity_factors: —Ñ–∞–∫—Ç–æ—Ä—ã —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ –¥–ª—è —ç–≤—Ä–∏—Å—Ç–∏—á–µ—Å–∫–æ–π –æ—Ü–µ–Ω–∫–∏

        Returns:
            float: —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∞—è –ø–æ–Ω—è—Ç–Ω–æ—Å—Ç—å [0, 1]
        """
        try:
            # –ï—Å–ª–∏ –µ—Å—Ç—å —ç–∫—Å–ø–µ—Ä—Ç–Ω—ã–µ –æ—Ü–µ–Ω–∫–∏ - –∏—Å–ø–æ–ª—å–∑—É–µ–º –∏—Ö
            if expert_ratings is not None and len(expert_ratings) > 0:
                return np.clip(np.mean(expert_ratings), 0.0, 1.0)

            # –≠–≤—Ä–∏—Å—Ç–∏—á–µ—Å–∫–∞—è –æ—Ü–µ–Ω–∫–∞ —á–µ—Ä–µ–∑ –ø—Ä–∏–Ω—Ü–∏–ø—ã –∫–æ–≥–Ω–∏—Ç–∏–≤–Ω–æ–π –Ω–∞–≥—Ä—É–∑–∫–∏
            if explanations is None or len(explanations) == 0:
                return 0.0

            explanations = np.array(explanations)

            # 1. –†–∞–∑—Ä–µ–∂–µ–Ω–Ω–æ—Å—Ç—å (—Å–ø–∞—Ä—Å–Ω–æ—Å—Ç—å) - –º–µ–Ω—å—à–µ –∞–∫—Ç–∏–≤–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ = –ª—É—á—à–µ
            sparsity_scores = []
            for exp in explanations:
                non_zero_ratio = np.sum(np.abs(exp) > self.noise_threshold) / len(exp)
                # –û–ø—Ç–∏–º–∞–ª—å–Ω–∞—è —Å–ø–∞—Ä—Å–Ω–æ—Å—Ç—å: 10-20% –∞–∫—Ç–∏–≤–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
                optimal_sparsity = 0.15
                sparsity_penalty = abs(non_zero_ratio - optimal_sparsity) / optimal_sparsity
                sparsity_score = max(0.0, 1.0 - sparsity_penalty)
                sparsity_scores.append(sparsity_score)

            sparsity_metric = np.mean(sparsity_scores)

            # 2. –ö–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç—å –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
            feature_importance_var = np.var(np.mean(np.abs(explanations), axis=0))
            consistency_metric = 1.0 / (1.0 + feature_importance_var)

            # 3. –ú–æ–Ω–æ–º–æ–¥–∞–ª—å–Ω–æ—Å—Ç—å —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –≤–∞–∂–Ω–æ—Å—Ç–∏ (–æ–¥–∏–Ω —è—Å–Ω—ã–π –ø–∏–∫ –ª—É—á—à–µ)
            mean_importance = np.mean(np.abs(explanations), axis=0)
            max_importance = np.max(mean_importance)
            second_max = np.partition(mean_importance, -2)[-2] if len(mean_importance) > 1 else 0

            dominance_ratio = max_importance / (second_max + 1e-8)
            dominance_metric = np.tanh(dominance_ratio - 1.0)  # –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ 2-3x —Ä–∞–∑–Ω–∏—Ü–∞

            # –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —ç–≤—Ä–∏—Å—Ç–∏—á–µ—Å–∫–∞—è –æ—Ü–µ–Ω–∫–∞
            w_sparsity = 0.4
            w_consistency = 0.3
            w_dominance = 0.3

            heuristic_score = (w_sparsity * sparsity_metric +
                             w_consistency * consistency_metric +
                             w_dominance * dominance_metric)

            return np.clip(heuristic_score, 0.0, 1.0)

        except Exception as e:
            warnings.warn(f"üö® –û—à–∏–±–∫–∞ –≤ human_comprehensibility: {str(e)}")
            return 0.5

    def calculate(self, model: Any, explainer: Any, X: np.ndarray, y: Optional[np.ndarray] = None,
                 expert_graph: Optional[Dict] = None, expert_ratings: Optional[List[float]] = None,
                 feature_names: Optional[List[str]] = None,
                 verbose: bool = True) -> Dict[str, float]:
        """
        –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –∏—Ç–æ–≥–æ–≤–æ–≥–æ Explainability Score —Å–æ–≥–ª–∞—Å–Ω–æ Trust-ADE

        ES = w_c¬∑F_c + w_s¬∑C_s + w_i¬∑S_i + w_h¬∑U_h

        Args:
            model: –º–æ–¥–µ–ª—å –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
            explainer: –æ–±—ä—è—Å–Ω–∏—Ç–µ–ª—å (LIME, SHAP, etc.)
            X: –≤—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
            y: —Ü–µ–ª–µ–≤—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
            expert_graph: —ç–∫—Å–ø–µ—Ä—Ç–Ω—ã–π –∫–∞—É–∑–∞–ª—å–Ω—ã–π –≥—Ä–∞—Ñ
            expert_ratings: —ç–∫—Å–ø–µ—Ä—Ç–Ω—ã–µ –æ—Ü–µ–Ω–∫–∏ –ø–æ–Ω—è—Ç–Ω–æ—Å—Ç–∏
            feature_names: –∏–º–µ–Ω–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
            verbose: –¥–µ—Ç–∞–ª—å–Ω—ã–π –≤—ã–≤–æ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤

        Returns:
            Dict[str, float]: —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤—Å–µ—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç ES
        """
        try:
            if X is None or len(X) == 0:
                return self._default_results()

            X = np.array(X)

            # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —Ä–∞–∑–º–µ—Ä –≤—ã–±–æ—Ä–∫–∏ –¥–ª—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
            n_samples = min(100, len(X))  # —É–≤–µ–ª–∏—á–∏–ª–∏ –¥–ª—è –ª—É—á—à–µ–π —Ç–æ—á–Ω–æ—Å—Ç–∏
            if n_samples < len(X):
                sample_indices = np.random.choice(len(X), n_samples, replace=False)
                X_sample = X[sample_indices]
            else:
                X_sample = X

            if verbose:
                print(f"üîç Trust-ADE –∞–Ω–∞–ª–∏–∑ –Ω–∞ {len(X_sample)} –æ–±—Ä–∞–∑—Ü–∞—Ö...")

            # 1. –ü–æ–ª—É—á–∞–µ–º –æ–±—ä—è—Å–Ω–µ–Ω–∏—è
            explanations = self._safe_explain(explainer, X_sample)
            if explanations is None or len(explanations) == 0:
                warnings.warn("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –æ–±—ä—è—Å–Ω–µ–Ω–∏—è")
                return self._default_results()

            explanations = np.array(explanations)

            # 2. –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–∞—É–∑–∞–ª—å–Ω—ã–µ —Å–≤—è–∑–∏ –∏–∑ –æ–±—ä—è—Å–Ω–µ–Ω–∏–π
            system_edges = self.extract_causal_edges_from_explanations(
                explanations, feature_names
            )

            # 3. –ö–∞—É–∑–∞–ª—å–Ω–∞—è —Ñ–∏–¥–µ–ª–∏—Ç–∏ F_c
            if expert_graph and 'causal_edges' in expert_graph:
                expert_edges = set(expert_graph['causal_edges'])
                confidence_scores = expert_graph.get('confidence_scores')
                snr_ratio = expert_graph.get('snr_ratio')

                F_c = self.causal_fidelity(system_edges, expert_edges,
                                         confidence_scores, snr_ratio)
            else:
                # –≠–≤—Ä–∏—Å—Ç–∏—á–µ—Å–∫–∞—è –æ—Ü–µ–Ω–∫–∞ —á–µ—Ä–µ–∑ –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç—å
                F_c = self._heuristic_causal_consistency(explanations)

            # 4. –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∞—è –∫–æ–≥–µ—Ä–µ–Ω—Ç–Ω–æ—Å—Ç—å C_s
            C_s = self.semantic_coherence(explanations)

            # 5. –°—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏–π S_i
            S_i = self.interpretation_stability(model, explainer, X_sample)

            # 6. –ß–µ–ª–æ–≤–µ—á–µ—Å–∫–∞—è –ø–æ–Ω—è—Ç–Ω–æ—Å—Ç—å U_h
            U_h = self.human_comprehensibility(explanations, expert_ratings)

            # 7. –ò—Ç–æ–≥–æ–≤—ã–π Explainability Score
            ES = (self.w_c * F_c + self.w_s * C_s +
                  self.w_i * S_i + self.w_h * U_h)

            results = {
                'explainability_score': np.clip(ES, 0.0, 1.0),
                'causal_fidelity': np.clip(F_c, 0.0, 1.0),
                'semantic_coherence': np.clip(C_s, 0.0, 1.0),
                'interpretation_stability': np.clip(S_i, 0.0, 1.0),
                'human_comprehensibility': np.clip(U_h, 0.0, 1.0),
                'n_causal_edges': len(system_edges),
                'n_expert_edges': len(expert_graph.get('causal_edges', [])) if expert_graph else 0
            }

            if verbose:
                print(f"üìä Trust-ADE Explainability Score Results:")
                print(f"   üß† Explainability Score: {results['explainability_score']:.4f}")
                print(f"   üîó Causal Fidelity: {results['causal_fidelity']:.4f}")
                print(f"   üß© Semantic Coherence: {results['semantic_coherence']:.4f}")
                print(f"   ‚öñÔ∏è Interpretation Stability: {results['interpretation_stability']:.4f}")
                print(f"   üë• Human Comprehensibility: {results['human_comprehensibility']:.4f}")
                print(f"   üìà Detected Causal Edges: {results['n_causal_edges']}")

            return results

        except Exception as e:
            warnings.warn(f"üö® –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –≤ ExplainabilityScore.calculate: {str(e)}")
            return self._default_results()

    def _heuristic_causal_consistency(self, explanations: np.ndarray) -> float:
        """–≠–≤—Ä–∏—Å—Ç–∏—á–µ—Å–∫–∞—è –æ—Ü–µ–Ω–∫–∞ –∫–∞—É–∑–∞–ª—å–Ω–æ–π –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç–∏ —á–µ—Ä–µ–∑ —Ä–∞–Ω–≥–æ–≤—ã–µ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏"""
        try:
            if len(explanations) < 2:
                return 0.5

            # –†–∞–Ω–∂–∏—Ä—É–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏ –ø–æ –≤–∞–∂–Ω–æ—Å—Ç–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –æ–±—ä—è—Å–Ω–µ–Ω–∏—è
            rank_correlations = []

            for i in range(min(20, len(explanations))):  # –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
                for j in range(i + 1, min(20, len(explanations))):
                    try:
                        # –†–∞–Ω–≥–∏ –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
                        rank_i = np.argsort(np.abs(explanations[i]))[::-1]
                        rank_j = np.argsort(np.abs(explanations[j]))[::-1]

                        # Spearman correlation –º–µ–∂–¥—É —Ä–∞–Ω–≥–∞–º–∏
                        corr = np.corrcoef(rank_i, rank_j)[0, 1]
                        if not np.isnan(corr) and not np.isinf(corr):
                            rank_correlations.append(abs(corr))
                    except Exception:
                        continue

            return np.mean(rank_correlations) if rank_correlations else 0.5

        except Exception:
            return 0.5

    def _default_results(self) -> Dict[str, float]:
        """–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –ø—Ä–∏ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö –æ—à–∏–±–∫–∞—Ö"""
        return {
            'explainability_score': 0.5,
            'causal_fidelity': 0.5,
            'semantic_coherence': 0.5,
            'interpretation_stability': 0.5,
            'human_comprehensibility': 0.5,
            'n_causal_edges': 0,
            'n_expert_edges': 0
        }